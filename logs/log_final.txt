Using device: cuda
Episode (1/100) took 7.027 seconds.
  Mean final reward: -0.0277
  Mean return: -0.3262
  Mean final entropy: 0.0189
  Max final entropy: 0.3503
  Pseudo loss: 0.01922
  Solved trajectories: 104 / 256
  Avg steps to disentangle: 1.562
Episode (2/100) took 6.992 seconds.
  Mean final reward: -0.0316
  Mean return: -0.3476
  Mean final entropy: 0.0215
  Max final entropy: 0.3419
  Pseudo loss: 0.00730
  Solved trajectories: 104 / 256
  Avg steps to disentangle: 1.578
Episode (3/100) took 7.003 seconds.
  Mean final reward: -0.0275
  Mean return: -0.3179
  Mean final entropy: 0.0187
  Max final entropy: 0.4112
  Pseudo loss: 0.00588
  Solved trajectories: 111 / 256
  Avg steps to disentangle: 1.625
Episode (4/100) took 6.975 seconds.
  Mean final reward: -0.0317
  Mean return: -0.3496
  Mean final entropy: 0.0216
  Max final entropy: 0.3116
  Pseudo loss: -0.01112
  Solved trajectories: 100 / 256
  Avg steps to disentangle: 1.562
Episode (5/100) took 6.971 seconds.
  Mean final reward: -0.0348
  Mean return: -0.3364
  Mean final entropy: 0.0238
  Max final entropy: 0.3129
  Pseudo loss: -0.02951
  Solved trajectories: 100 / 256
  Avg steps to disentangle: 1.508
Episode (6/100) took 6.996 seconds.
  Mean final reward: -0.0297
  Mean return: -0.3224
  Mean final entropy: 0.0202
  Max final entropy: 0.3392
  Pseudo loss: -0.03904
  Solved trajectories: 121 / 256
  Avg steps to disentangle: 1.703
Episode (7/100) took 6.966 seconds.
  Mean final reward: -0.0409
  Mean return: -0.3419
  Mean final entropy: 0.0280
  Max final entropy: 0.4109
  Pseudo loss: 0.01304
  Solved trajectories: 117 / 256
  Avg steps to disentangle: 1.625
Episode (8/100) took 7.010 seconds.
  Mean final reward: -0.0410
  Mean return: -0.3463
  Mean final entropy: 0.0281
  Max final entropy: 0.5497
  Pseudo loss: -0.03732
  Solved trajectories: 100 / 256
  Avg steps to disentangle: 1.523
Episode (9/100) took 6.979 seconds.
  Mean final reward: -0.0406
  Mean return: -0.3626
  Mean final entropy: 0.0278
  Max final entropy: 0.4869
  Pseudo loss: -0.02057
  Solved trajectories: 104 / 256
  Avg steps to disentangle: 1.551
Episode (10/100) took 6.989 seconds.
  Mean final reward: -0.0265
  Mean return: -0.3039
  Mean final entropy: 0.0180
  Max final entropy: 0.2436
  Pseudo loss: -0.05543
  Solved trajectories: 109 / 256
  Avg steps to disentangle: 1.598
Episode (11/100) took 7.259 seconds.
  Mean final reward: -0.0280
  Mean return: -0.2832
  Mean final entropy: 0.0190
  Max final entropy: 0.5768
  Pseudo loss: -0.05328
  Solved trajectories: 121 / 256
  Avg steps to disentangle: 1.645
Episode (12/100) took 7.229 seconds.
  Mean final reward: -0.0196
  Mean return: -0.2928
  Mean final entropy: 0.0132
  Max final entropy: 0.3646
  Pseudo loss: -0.04164
  Solved trajectories: 133 / 256
  Avg steps to disentangle: 1.719
Episode (13/100) took 7.306 seconds.
  Mean final reward: -0.0176
  Mean return: -0.2653
  Mean final entropy: 0.0117
  Max final entropy: 0.2675
  Pseudo loss: -0.01915
  Solved trajectories: 137 / 256
  Avg steps to disentangle: 1.750
Episode (14/100) took 7.466 seconds.
  Mean final reward: -0.0270
  Mean return: -0.2882
  Mean final entropy: 0.0183
  Max final entropy: 0.2945
  Pseudo loss: -0.00718
  Solved trajectories: 124 / 256
  Avg steps to disentangle: 1.648
Episode (15/100) took 7.239 seconds.
  Mean final reward: -0.0235
  Mean return: -0.2613
  Mean final entropy: 0.0158
  Max final entropy: 0.4496
  Pseudo loss: -0.02776
  Solved trajectories: 138 / 256
  Avg steps to disentangle: 1.773
Episode (16/100) took 7.108 seconds.
  Mean final reward: -0.0204
  Mean return: -0.2974
  Mean final entropy: 0.0137
  Max final entropy: 0.4680
  Pseudo loss: -0.04088
  Solved trajectories: 137 / 256
  Avg steps to disentangle: 1.805
Episode (17/100) took 7.204 seconds.
  Mean final reward: -0.0174
  Mean return: -0.2535
  Mean final entropy: 0.0116
  Max final entropy: 0.2252
  Pseudo loss: -0.02154
  Solved trajectories: 147 / 256
  Avg steps to disentangle: 1.812
Episode (18/100) took 7.063 seconds.
  Mean final reward: -0.0144
  Mean return: -0.2523
  Mean final entropy: 0.0095
  Max final entropy: 0.1772
  Pseudo loss: -0.05666
  Solved trajectories: 129 / 256
  Avg steps to disentangle: 1.680
Episode (19/100) took 7.191 seconds.
  Mean final reward: -0.0177
  Mean return: -0.2540
  Mean final entropy: 0.0118
  Max final entropy: 0.1532
  Pseudo loss: -0.04375
  Solved trajectories: 139 / 256
  Avg steps to disentangle: 1.750
Episode (20/100) took 7.193 seconds.
  Mean final reward: -0.0164
  Mean return: -0.2680
  Mean final entropy: 0.0109
  Max final entropy: 0.3943
  Pseudo loss: 0.00534
  Solved trajectories: 146 / 256
  Avg steps to disentangle: 1.852
Episode (21/100) took 7.219 seconds.
  Mean final reward: -0.0131
  Mean return: -0.2500
  Mean final entropy: 0.0085
  Max final entropy: 0.1860
  Pseudo loss: -0.02608
  Solved trajectories: 157 / 256
  Avg steps to disentangle: 1.871
Episode (22/100) took 7.147 seconds.
  Mean final reward: -0.0166
  Mean return: -0.2398
  Mean final entropy: 0.0110
  Max final entropy: 0.3368
  Pseudo loss: -0.04076
  Solved trajectories: 159 / 256
  Avg steps to disentangle: 1.840
Episode (23/100) took 7.008 seconds.
  Mean final reward: -0.0133
  Mean return: -0.2495
  Mean final entropy: 0.0087
  Max final entropy: 0.1549
  Pseudo loss: -0.02035
  Solved trajectories: 145 / 256
  Avg steps to disentangle: 1.789
Episode (24/100) took 7.005 seconds.
  Mean final reward: -0.0147
  Mean return: -0.2916
  Mean final entropy: 0.0097
  Max final entropy: 0.3389
  Pseudo loss: -0.05183
  Solved trajectories: 161 / 256
  Avg steps to disentangle: 1.922
Episode (25/100) took 7.117 seconds.
  Mean final reward: -0.0123
  Mean return: -0.2495
  Mean final entropy: 0.0080
  Max final entropy: 0.1645
  Pseudo loss: -0.03372
  Solved trajectories: 161 / 256
  Avg steps to disentangle: 1.902
Episode (26/100) took 6.937 seconds.
  Mean final reward: -0.0141
  Mean return: -0.2644
  Mean final entropy: 0.0093
  Max final entropy: 0.1629
  Pseudo loss: -0.02165
  Solved trajectories: 152 / 256
  Avg steps to disentangle: 1.828
Episode (27/100) took 7.027 seconds.
  Mean final reward: -0.0163
  Mean return: -0.2693
  Mean final entropy: 0.0108
  Max final entropy: 0.2350
  Pseudo loss: -0.03961
  Solved trajectories: 151 / 256
  Avg steps to disentangle: 1.867
Episode (28/100) took 6.982 seconds.
  Mean final reward: -0.0113
  Mean return: -0.2327
  Mean final entropy: 0.0073
  Max final entropy: 0.1082
  Pseudo loss: -0.03143
  Solved trajectories: 151 / 256
  Avg steps to disentangle: 1.875
Episode (29/100) took 6.952 seconds.
  Mean final reward: -0.0138
  Mean return: -0.2660
  Mean final entropy: 0.0090
  Max final entropy: 0.1851
  Pseudo loss: -0.01917
  Solved trajectories: 154 / 256
  Avg steps to disentangle: 1.879
Episode (30/100) took 6.935 seconds.
  Mean final reward: -0.0105
  Mean return: -0.2382
  Mean final entropy: 0.0067
  Max final entropy: 0.1813
  Pseudo loss: -0.02454
  Solved trajectories: 168 / 256
  Avg steps to disentangle: 1.934
Episode (31/100) took 6.935 seconds.
  Mean final reward: -0.0111
  Mean return: -0.2478
  Mean final entropy: 0.0071
  Max final entropy: 0.1751
  Pseudo loss: -0.00735
  Solved trajectories: 176 / 256
  Avg steps to disentangle: 1.992
Episode (32/100) took 6.964 seconds.
  Mean final reward: -0.0124
  Mean return: -0.2524
  Mean final entropy: 0.0080
  Max final entropy: 0.1282
  Pseudo loss: 0.01130
  Solved trajectories: 165 / 256
  Avg steps to disentangle: 1.887
Episode (33/100) took 6.955 seconds.
  Mean final reward: -0.0128
  Mean return: -0.2269
  Mean final entropy: 0.0084
  Max final entropy: 0.1421
  Pseudo loss: -0.01024
  Solved trajectories: 151 / 256
  Avg steps to disentangle: 1.859
Episode (34/100) took 6.998 seconds.
  Mean final reward: -0.0116
  Mean return: -0.2546
  Mean final entropy: 0.0075
  Max final entropy: 0.1911
  Pseudo loss: 0.00848
  Solved trajectories: 168 / 256
  Avg steps to disentangle: 2.023
Episode (35/100) took 6.979 seconds.
  Mean final reward: -0.0126
  Mean return: -0.2670
  Mean final entropy: 0.0082
  Max final entropy: 0.3246
  Pseudo loss: -0.01810
  Solved trajectories: 163 / 256
  Avg steps to disentangle: 1.934
Episode (36/100) took 7.003 seconds.
  Mean final reward: -0.0133
  Mean return: -0.2557
  Mean final entropy: 0.0086
  Max final entropy: 0.1863
  Pseudo loss: -0.01181
  Solved trajectories: 172 / 256
  Avg steps to disentangle: 1.957
Episode (37/100) took 6.985 seconds.
  Mean final reward: -0.0113
  Mean return: -0.2631
  Mean final entropy: 0.0073
  Max final entropy: 0.1236
  Pseudo loss: -0.01387
  Solved trajectories: 157 / 256
  Avg steps to disentangle: 1.906
Episode (38/100) took 7.020 seconds.
  Mean final reward: -0.0119
  Mean return: -0.2711
  Mean final entropy: 0.0077
  Max final entropy: 0.2039
  Pseudo loss: 0.00233
  Solved trajectories: 166 / 256
  Avg steps to disentangle: 1.977
Episode (39/100) took 7.002 seconds.
  Mean final reward: -0.0112
  Mean return: -0.2622
  Mean final entropy: 0.0072
  Max final entropy: 0.1874
  Pseudo loss: -0.00171
  Solved trajectories: 166 / 256
  Avg steps to disentangle: 1.953
Episode (40/100) took 7.037 seconds.
  Mean final reward: -0.0122
  Mean return: -0.2612
  Mean final entropy: 0.0079
  Max final entropy: 0.1982
  Pseudo loss: -0.02002
  Solved trajectories: 174 / 256
  Avg steps to disentangle: 1.957
Episode (41/100) took 6.906 seconds.
  Mean final reward: -0.0110
  Mean return: -0.2446
  Mean final entropy: 0.0071
  Max final entropy: 0.1252
  Pseudo loss: -0.01734
  Solved trajectories: 163 / 256
  Avg steps to disentangle: 1.918
Episode (42/100) took 6.962 seconds.
  Mean final reward: -0.0113
  Mean return: -0.2704
  Mean final entropy: 0.0072
  Max final entropy: 0.1981
  Pseudo loss: 0.00151
  Solved trajectories: 181 / 256
  Avg steps to disentangle: 2.020
Episode (43/100) took 6.993 seconds.
  Mean final reward: -0.0130
  Mean return: -0.2621
  Mean final entropy: 0.0084
  Max final entropy: 0.2278
  Pseudo loss: -0.01051
  Solved trajectories: 174 / 256
  Avg steps to disentangle: 1.984
Episode (44/100) took 6.987 seconds.
  Mean final reward: -0.0118
  Mean return: -0.2435
  Mean final entropy: 0.0075
  Max final entropy: 0.2116
  Pseudo loss: -0.00852
  Solved trajectories: 174 / 256
  Avg steps to disentangle: 1.961
Episode (45/100) took 6.954 seconds.
  Mean final reward: -0.0106
  Mean return: -0.2681
  Mean final entropy: 0.0067
  Max final entropy: 0.2373
  Pseudo loss: 0.01447
  Solved trajectories: 178 / 256
  Avg steps to disentangle: 1.980
Episode (46/100) took 6.931 seconds.
  Mean final reward: -0.0085
  Mean return: -0.2096
  Mean final entropy: 0.0053
  Max final entropy: 0.1114
  Pseudo loss: -0.00960
  Solved trajectories: 167 / 256
  Avg steps to disentangle: 1.926
Episode (47/100) took 6.924 seconds.
  Mean final reward: -0.0111
  Mean return: -0.2476
  Mean final entropy: 0.0070
  Max final entropy: 0.2218
  Pseudo loss: -0.00892
  Solved trajectories: 180 / 256
  Avg steps to disentangle: 1.980
Episode (48/100) took 7.012 seconds.
  Mean final reward: -0.0113
  Mean return: -0.2445
  Mean final entropy: 0.0072
  Max final entropy: 0.2056
  Pseudo loss: -0.03868
  Solved trajectories: 188 / 256
  Avg steps to disentangle: 2.047
Episode (49/100) took 6.975 seconds.
  Mean final reward: -0.0097
  Mean return: -0.2691
  Mean final entropy: 0.0061
  Max final entropy: 0.1475
  Pseudo loss: -0.00586
  Solved trajectories: 173 / 256
  Avg steps to disentangle: 1.941
Episode (50/100) took 7.019 seconds.
  Mean final reward: -0.0087
  Mean return: -0.2544
  Mean final entropy: 0.0054
  Max final entropy: 0.1277
  Pseudo loss: 0.00378
  Solved trajectories: 176 / 256
  Avg steps to disentangle: 1.969
Episode (51/100) took 7.216 seconds.
  Mean final reward: -0.0112
  Mean return: -0.2496
  Mean final entropy: 0.0071
  Max final entropy: 0.1613
  Pseudo loss: 0.00117
  Solved trajectories: 183 / 256
  Avg steps to disentangle: 2.035
Episode (52/100) took 7.210 seconds.
  Mean final reward: -0.0129
  Mean return: -0.2543
  Mean final entropy: 0.0084
  Max final entropy: 0.2683
  Pseudo loss: -0.00835
  Solved trajectories: 169 / 256
  Avg steps to disentangle: 1.949
Episode (53/100) took 7.330 seconds.
  Mean final reward: -0.0110
  Mean return: -0.2896
  Mean final entropy: 0.0070
  Max final entropy: 0.2170
  Pseudo loss: -0.01170
  Solved trajectories: 181 / 256
  Avg steps to disentangle: 2.059
Episode (54/100) took 7.170 seconds.
  Mean final reward: -0.0096
  Mean return: -0.2645
  Mean final entropy: 0.0061
  Max final entropy: 0.2124
  Pseudo loss: -0.02956
  Solved trajectories: 185 / 256
  Avg steps to disentangle: 2.062
Episode (55/100) took 7.227 seconds.
  Mean final reward: -0.0108
  Mean return: -0.2068
  Mean final entropy: 0.0069
  Max final entropy: 0.2063
  Pseudo loss: 0.00551
  Solved trajectories: 178 / 256
  Avg steps to disentangle: 1.965
Episode (56/100) took 7.269 seconds.
  Mean final reward: -0.0093
  Mean return: -0.2454
  Mean final entropy: 0.0058
  Max final entropy: 0.2164
  Pseudo loss: -0.01607
  Solved trajectories: 175 / 256
  Avg steps to disentangle: 1.992
Episode (57/100) took 7.204 seconds.
  Mean final reward: -0.0097
  Mean return: -0.2257
  Mean final entropy: 0.0061
  Max final entropy: 0.1691
  Pseudo loss: -0.01034
  Solved trajectories: 181 / 256
  Avg steps to disentangle: 1.984
Episode (58/100) took 7.184 seconds.
  Mean final reward: -0.0091
  Mean return: -0.2536
  Mean final entropy: 0.0057
  Max final entropy: 0.2035
  Pseudo loss: -0.01469
  Solved trajectories: 183 / 256
  Avg steps to disentangle: 2.016
Episode (59/100) took 7.233 seconds.
  Mean final reward: -0.0112
  Mean return: -0.2404
  Mean final entropy: 0.0072
  Max final entropy: 0.1704
  Pseudo loss: -0.02017
  Solved trajectories: 167 / 256
  Avg steps to disentangle: 1.914
Episode (60/100) took 7.225 seconds.
  Mean final reward: -0.0092
  Mean return: -0.2423
  Mean final entropy: 0.0058
  Max final entropy: 0.1348
  Pseudo loss: -0.01231
  Solved trajectories: 167 / 256
  Avg steps to disentangle: 1.938
Episode (61/100) took 7.262 seconds.
  Mean final reward: -0.0074
  Mean return: -0.2050
  Mean final entropy: 0.0045
  Max final entropy: 0.1880
  Pseudo loss: -0.00125
  Solved trajectories: 189 / 256
  Avg steps to disentangle: 2.012
Episode (62/100) took 7.283 seconds.
  Mean final reward: -0.0106
  Mean return: -0.2290
  Mean final entropy: 0.0068
  Max final entropy: 0.1795
  Pseudo loss: -0.00250
  Solved trajectories: 175 / 256
  Avg steps to disentangle: 1.977
Episode (63/100) took 7.564 seconds.
  Mean final reward: -0.0123
  Mean return: -0.2571
  Mean final entropy: 0.0079
  Max final entropy: 0.2204
  Pseudo loss: -0.04328
  Solved trajectories: 183 / 256
  Avg steps to disentangle: 2.051
Episode (64/100) took 7.414 seconds.
  Mean final reward: -0.0096
  Mean return: -0.2477
  Mean final entropy: 0.0060
  Max final entropy: 0.1899
  Pseudo loss: -0.00210
  Solved trajectories: 177 / 256
  Avg steps to disentangle: 1.988
Episode (65/100) took 7.350 seconds.
  Mean final reward: -0.0078
  Mean return: -0.2512
  Mean final entropy: 0.0047
  Max final entropy: 0.1622
  Pseudo loss: -0.00240
  Solved trajectories: 198 / 256
  Avg steps to disentangle: 2.070
Episode (66/100) took 7.330 seconds.
  Mean final reward: -0.0073
  Mean return: -0.2240
  Mean final entropy: 0.0044
  Max final entropy: 0.1186
  Pseudo loss: -0.00820
  Solved trajectories: 184 / 256
  Avg steps to disentangle: 2.051
Episode (67/100) took 7.171 seconds.
  Mean final reward: -0.0092
  Mean return: -0.2432
  Mean final entropy: 0.0057
  Max final entropy: 0.1504
  Pseudo loss: 0.00450
  Solved trajectories: 190 / 256
  Avg steps to disentangle: 2.062
Episode (68/100) took 7.309 seconds.
  Mean final reward: -0.0074
  Mean return: -0.2593
  Mean final entropy: 0.0044
  Max final entropy: 0.1537
  Pseudo loss: -0.00402
  Solved trajectories: 192 / 256
  Avg steps to disentangle: 2.059
Episode (69/100) took 7.342 seconds.
  Mean final reward: -0.0075
  Mean return: -0.2177
  Mean final entropy: 0.0045
  Max final entropy: 0.1171
  Pseudo loss: 0.00061
  Solved trajectories: 188 / 256
  Avg steps to disentangle: 2.027
Episode (70/100) took 7.163 seconds.
  Mean final reward: -0.0095
  Mean return: -0.2319
  Mean final entropy: 0.0060
  Max final entropy: 0.1401
  Pseudo loss: -0.01863
  Solved trajectories: 177 / 256
  Avg steps to disentangle: 1.934
Episode (71/100) took 7.306 seconds.
  Mean final reward: -0.0098
  Mean return: -0.2285
  Mean final entropy: 0.0062
  Max final entropy: 0.1192
  Pseudo loss: 0.00043
  Solved trajectories: 171 / 256
  Avg steps to disentangle: 1.949
Episode (72/100) took 7.221 seconds.
  Mean final reward: -0.0094
  Mean return: -0.2337
  Mean final entropy: 0.0059
  Max final entropy: 0.1606
  Pseudo loss: -0.00021
  Solved trajectories: 178 / 256
  Avg steps to disentangle: 2.047
Episode (73/100) took 7.137 seconds.
  Mean final reward: -0.0097
  Mean return: -0.2384
  Mean final entropy: 0.0060
  Max final entropy: 0.1505
  Pseudo loss: -0.00055
  Solved trajectories: 193 / 256
  Avg steps to disentangle: 2.062
Episode (74/100) took 7.410 seconds.
  Mean final reward: -0.0097
  Mean return: -0.2475
  Mean final entropy: 0.0061
  Max final entropy: 0.3809
  Pseudo loss: -0.03055
  Solved trajectories: 181 / 256
  Avg steps to disentangle: 2.004
Episode (75/100) took 7.250 seconds.
  Mean final reward: -0.0069
  Mean return: -0.2503
  Mean final entropy: 0.0041
  Max final entropy: 0.0700
  Pseudo loss: -0.00697
  Solved trajectories: 183 / 256
  Avg steps to disentangle: 2.020
Episode (76/100) took 7.227 seconds.
  Mean final reward: -0.0086
  Mean return: -0.2485
  Mean final entropy: 0.0054
  Max final entropy: 0.1631
  Pseudo loss: -0.01608
  Solved trajectories: 173 / 256
  Avg steps to disentangle: 1.977
Episode (77/100) took 7.220 seconds.
  Mean final reward: -0.0109
  Mean return: -0.2578
  Mean final entropy: 0.0070
  Max final entropy: 0.1970
  Pseudo loss: -0.00860
  Solved trajectories: 175 / 256
  Avg steps to disentangle: 1.988
Episode (78/100) took 7.081 seconds.
  Mean final reward: -0.0100
  Mean return: -0.2309
  Mean final entropy: 0.0063
  Max final entropy: 0.1540
  Pseudo loss: 0.00079
  Solved trajectories: 176 / 256
  Avg steps to disentangle: 1.996
Episode (79/100) took 7.174 seconds.
  Mean final reward: -0.0068
  Mean return: -0.2648
  Mean final entropy: 0.0041
  Max final entropy: 0.0855
  Pseudo loss: 0.00001
  Solved trajectories: 178 / 256
  Avg steps to disentangle: 2.012
Episode (80/100) took 7.211 seconds.
  Mean final reward: -0.0051
  Mean return: -0.2489
  Mean final entropy: 0.0029
  Max final entropy: 0.0659
  Pseudo loss: -0.00378
  Solved trajectories: 189 / 256
  Avg steps to disentangle: 2.020
Episode (81/100) took 7.074 seconds.
  Mean final reward: -0.0088
  Mean return: -0.2451
  Mean final entropy: 0.0055
  Max final entropy: 0.0949
  Pseudo loss: -0.00111
  Solved trajectories: 169 / 256
  Avg steps to disentangle: 1.992
Episode (82/100) took 7.006 seconds.
  Mean final reward: -0.0090
  Mean return: -0.2479
  Mean final entropy: 0.0056
  Max final entropy: 0.1221
  Pseudo loss: -0.00407
  Solved trajectories: 171 / 256
  Avg steps to disentangle: 1.957
Episode (83/100) took 7.105 seconds.
  Mean final reward: -0.0062
  Mean return: -0.2553
  Mean final entropy: 0.0037
  Max final entropy: 0.1825
  Pseudo loss: 0.00258
  Solved trajectories: 176 / 256
  Avg steps to disentangle: 1.973
Episode (84/100) took 7.227 seconds.
  Mean final reward: -0.0108
  Mean return: -0.2493
  Mean final entropy: 0.0068
  Max final entropy: 0.1342
  Pseudo loss: -0.00973
  Solved trajectories: 185 / 256
  Avg steps to disentangle: 2.059
Episode (85/100) took 7.185 seconds.
  Mean final reward: -0.0065
  Mean return: -0.2037
  Mean final entropy: 0.0039
  Max final entropy: 0.1306
  Pseudo loss: -0.00127
  Solved trajectories: 182 / 256
  Avg steps to disentangle: 1.977
Episode (86/100) took 6.960 seconds.
  Mean final reward: -0.0090
  Mean return: -0.2542
  Mean final entropy: 0.0056
  Max final entropy: 0.1563
  Pseudo loss: 0.00049
  Solved trajectories: 188 / 256
  Avg steps to disentangle: 2.066
Episode (87/100) took 6.897 seconds.
  Mean final reward: -0.0101
  Mean return: -0.2411
  Mean final entropy: 0.0064
  Max final entropy: 0.2424
  Pseudo loss: -0.00005
  Solved trajectories: 169 / 256
  Avg steps to disentangle: 1.902
Episode (88/100) took 6.979 seconds.
  Mean final reward: -0.0104
  Mean return: -0.2489
  Mean final entropy: 0.0066
  Max final entropy: 0.1551
  Pseudo loss: -0.00245
  Solved trajectories: 182 / 256
  Avg steps to disentangle: 1.938
Episode (89/100) took 7.025 seconds.
  Mean final reward: -0.0075
  Mean return: -0.2336
  Mean final entropy: 0.0046
  Max final entropy: 0.1928
  Pseudo loss: 0.00003
  Solved trajectories: 189 / 256
  Avg steps to disentangle: 2.062
Episode (90/100) took 7.003 seconds.
  Mean final reward: -0.0126
  Mean return: -0.2519
  Mean final entropy: 0.0081
  Max final entropy: 0.1940
  Pseudo loss: -0.00104
  Solved trajectories: 181 / 256
  Avg steps to disentangle: 1.996
Episode (91/100) took 6.993 seconds.
  Mean final reward: -0.0090
  Mean return: -0.2466
  Mean final entropy: 0.0056
  Max final entropy: 0.2018
  Pseudo loss: -0.02295
  Solved trajectories: 179 / 256
  Avg steps to disentangle: 1.996
Episode (92/100) took 7.038 seconds.
  Mean final reward: -0.0077
  Mean return: -0.2475
  Mean final entropy: 0.0047
  Max final entropy: 0.2165
  Pseudo loss: -0.00092
  Solved trajectories: 174 / 256
  Avg steps to disentangle: 2.027
Episode (93/100) took 7.042 seconds.
  Mean final reward: -0.0100
  Mean return: -0.2285
  Mean final entropy: 0.0063
  Max final entropy: 0.1728
  Pseudo loss: -0.00011
  Solved trajectories: 174 / 256
  Avg steps to disentangle: 1.957
Episode (94/100) took 7.021 seconds.
  Mean final reward: -0.0098
  Mean return: -0.2409
  Mean final entropy: 0.0062
  Max final entropy: 0.1898
  Pseudo loss: 0.00058
  Solved trajectories: 176 / 256
  Avg steps to disentangle: 2.016
Episode (95/100) took 7.047 seconds.
  Mean final reward: -0.0089
  Mean return: -0.2571
  Mean final entropy: 0.0056
  Max final entropy: 0.1175
  Pseudo loss: 0.00148
  Solved trajectories: 178 / 256
  Avg steps to disentangle: 1.977
Episode (96/100) took 7.044 seconds.
  Mean final reward: -0.0108
  Mean return: -0.2442
  Mean final entropy: 0.0069
  Max final entropy: 0.1432
  Pseudo loss: -0.00179
  Solved trajectories: 169 / 256
  Avg steps to disentangle: 1.945
Episode (97/100) took 7.034 seconds.
  Mean final reward: -0.0098
  Mean return: -0.2417
  Mean final entropy: 0.0062
  Max final entropy: 0.1836
  Pseudo loss: 0.00274
  Solved trajectories: 179 / 256
  Avg steps to disentangle: 2.004
Episode (98/100) took 6.945 seconds.
  Mean final reward: -0.0104
  Mean return: -0.2518
  Mean final entropy: 0.0066
  Max final entropy: 0.1480
  Pseudo loss: 0.00038
  Solved trajectories: 177 / 256
  Avg steps to disentangle: 2.008
Episode (99/100) took 6.885 seconds.
  Mean final reward: -0.0095
  Mean return: -0.2685
  Mean final entropy: 0.0060
  Max final entropy: 0.1528
  Pseudo loss: 0.00206
  Solved trajectories: 177 / 256
  Avg steps to disentangle: 2.051
Episode (100/100) took 7.008 seconds.
  Mean final reward: -0.0079
  Mean return: -0.2382
  Mean final entropy: 0.0049
  Max final entropy: 0.1835
  Pseudo loss: -0.00015
  Solved trajectories: 184 / 256
  Avg steps to disentangle: 2.035
##############################
Testing agent accuracy...
Solved states: 767 / 768 = 99.870%
95%% entropy: -0.0014426951529458165
##############################
