##############################
Training parameters:
  Number of trajectories:   256
  Number of episodes:       1001
  Learning rate:            0.001
  Final learning rate:      0.001
  Weight regularization:    0.0
  Entropy regularization:   0.0
  Grad clipping threshold:  0.0
  Policy hidden dimensions: [256, 256]
  Policy dropout rate:      0.0

Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    weight_decay: 0.0
)

Episode (1/1001) took 0.308 seconds.
  Mean final reward:        -5.5239
  Mean return:              -29.5941
  Mean final entropy:       0.2841
  Max final entropy:        0.4996
  95 percentile entropy:    0.43223
  Pseudo loss:              202.03748
  Total gradient norm:      15.92220
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (10/1001) took 0.387 seconds.
  Mean final reward:        -5.4657
  Mean return:              -29.5224
  Mean final entropy:       0.2753
  Max final entropy:        0.4996
  95 percentile entropy:    0.44218
  Pseudo loss:              191.93437
  Total gradient norm:      15.74215
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (20/1001) took 0.213 seconds.
  Mean final reward:        -5.5251
  Mean return:              -29.6608
  Mean final entropy:       0.2978
  Max final entropy:        0.4996
  95 percentile entropy:    0.49178
  Pseudo loss:              179.54594
  Total gradient norm:      15.90136
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (30/1001) took 0.211 seconds.
  Mean final reward:        -5.4466
  Mean return:              -29.5135
  Mean final entropy:       0.2803
  Max final entropy:        0.4918
  95 percentile entropy:    0.43687
  Pseudo loss:              167.11031
  Total gradient norm:      15.29332
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (40/1001) took 0.211 seconds.
  Mean final reward:        -5.4444
  Mean return:              -29.4389
  Mean final entropy:       0.2726
  Max final entropy:        0.4918
  95 percentile entropy:    0.42947
  Pseudo loss:              161.96268
  Total gradient norm:      13.04471
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (50/1001) took 0.212 seconds.
  Mean final reward:        -5.4457
  Mean return:              -29.3792
  Mean final entropy:       0.2733
  Max final entropy:        0.4918
  95 percentile entropy:    0.37957
  Pseudo loss:              160.13017
  Total gradient norm:      16.55423
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (60/1001) took 0.211 seconds.
  Mean final reward:        -5.4276
  Mean return:              -29.3073
  Mean final entropy:       0.2701
  Max final entropy:        0.4721
  95 percentile entropy:    0.38948
  Pseudo loss:              155.76791
  Total gradient norm:      10.16064
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (70/1001) took 0.212 seconds.
  Mean final reward:        -5.3507
  Mean return:              -29.0887
  Mean final entropy:       0.2513
  Max final entropy:        0.4567
  95 percentile entropy:    0.39884
  Pseudo loss:              153.35202
  Total gradient norm:      24.20902
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (80/1001) took 0.233 seconds.
  Mean final reward:        -5.3558
  Mean return:              -29.1329
  Mean final entropy:       0.2546
  Max final entropy:        0.4354
  95 percentile entropy:    0.39578
  Pseudo loss:              137.41573
  Total gradient norm:      11.26895
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (90/1001) took 0.212 seconds.
  Mean final reward:        -5.2506
  Mean return:              -28.9349
  Mean final entropy:       0.2417
  Max final entropy:        0.4918
  95 percentile entropy:    0.40237
  Pseudo loss:              132.72835
  Total gradient norm:      16.97938
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (100/1001) took 0.212 seconds.
  Mean final reward:        -5.1542
  Mean return:              -28.6659
  Mean final entropy:       0.2154
  Max final entropy:        0.4572
  95 percentile entropy:    0.36287
  Pseudo loss:              139.06323
  Total gradient norm:      25.05543
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (110/1001) took 0.212 seconds.
  Mean final reward:        -5.0202
  Mean return:              -28.3141
  Mean final entropy:       0.1985
  Max final entropy:        0.4882
  95 percentile entropy:    0.37020
  Pseudo loss:              139.83005
  Total gradient norm:      16.05168
  Solved trajectories:      3 / 256
  Avg steps to disentangle: 5.000
Episode (120/1001) took 0.212 seconds.
  Mean final reward:        -4.9370
  Mean return:              -28.0902
  Mean final entropy:       0.1777
  Max final entropy:        0.4918
  95 percentile entropy:    0.36287
  Pseudo loss:              128.62921
  Total gradient norm:      8.10974
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (130/1001) took 0.389 seconds.
  Mean final reward:        -4.9433
  Mean return:              -27.9204
  Mean final entropy:       0.1760
  Max final entropy:        0.4349
  95 percentile entropy:    0.36287
  Pseudo loss:              124.61893
  Total gradient norm:      16.93936
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (140/1001) took 0.389 seconds.
  Mean final reward:        -4.9177
  Mean return:              -27.7700
  Mean final entropy:       0.1719
  Max final entropy:        0.4918
  95 percentile entropy:    0.36345
  Pseudo loss:              127.43534
  Total gradient norm:      14.90558
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (150/1001) took 0.304 seconds.
  Mean final reward:        -4.8614
  Mean return:              -27.4263
  Mean final entropy:       0.1641
  Max final entropy:        0.4349
  95 percentile entropy:    0.34639
  Pseudo loss:              98.54132
  Total gradient norm:      16.51438
  Solved trajectories:      3 / 256
  Avg steps to disentangle: 5.000
Episode (160/1001) took 0.305 seconds.
  Mean final reward:        -4.7523
  Mean return:              -27.0718
  Mean final entropy:       0.1517
  Max final entropy:        0.4349
  95 percentile entropy:    0.34434
  Pseudo loss:              71.12069
  Total gradient norm:      10.01472
  Solved trajectories:      6 / 256
  Avg steps to disentangle: 5.000
Episode (170/1001) took 0.386 seconds.
  Mean final reward:        -4.4232
  Mean return:              -26.3403
  Mean final entropy:       0.1255
  Max final entropy:        0.3661
  95 percentile entropy:    0.30561
  Pseudo loss:              52.10793
  Total gradient norm:      13.35774
  Solved trajectories:      10 / 256
  Avg steps to disentangle: 5.000
Episode (180/1001) took 0.303 seconds.
  Mean final reward:        -3.5654
  Mean return:              -24.6074
  Mean final entropy:       0.0733
  Max final entropy:        0.3629
  95 percentile entropy:    0.25755
  Pseudo loss:              29.33577
  Total gradient norm:      6.28532
  Solved trajectories:      10 / 256
  Avg steps to disentangle: 5.000
Episode (190/1001) took 0.211 seconds.
  Mean final reward:        -3.0208
  Mean return:              -23.2937
  Mean final entropy:       0.0316
  Max final entropy:        0.3005
  95 percentile entropy:    0.14095
  Pseudo loss:              12.16957
  Total gradient norm:      6.85587
  Solved trajectories:      4 / 256
  Avg steps to disentangle: 5.000
Episode (200/1001) took 0.384 seconds.
  Mean final reward:        -2.9178
  Mean return:              -22.9482
  Mean final entropy:       0.0234
  Max final entropy:        0.3443
  95 percentile entropy:    0.01737
  Pseudo loss:              5.95506
  Total gradient norm:      3.26765
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (210/1001) took 0.384 seconds.
  Mean final reward:        -2.8946
  Mean return:              -22.8231
  Mean final entropy:       0.0198
  Max final entropy:        0.2279
  95 percentile entropy:    0.01737
  Pseudo loss:              2.97511
  Total gradient norm:      1.85500
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (220/1001) took 0.384 seconds.
  Mean final reward:        -2.8624
  Mean return:              -22.7983
  Mean final entropy:       0.0209
  Max final entropy:        0.3702
  95 percentile entropy:    0.01737
  Pseudo loss:              4.67486
  Total gradient norm:      1.56176
  Solved trajectories:      3 / 256
  Avg steps to disentangle: 5.000
Episode (230/1001) took 0.384 seconds.
  Mean final reward:        -2.8398
  Mean return:              -22.7482
  Mean final entropy:       0.0177
  Max final entropy:        0.1323
  95 percentile entropy:    0.01737
  Pseudo loss:              3.07344
  Total gradient norm:      1.08241
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (240/1001) took 0.383 seconds.
  Mean final reward:        -2.8185
  Mean return:              -22.7004
  Mean final entropy:       0.0176
  Max final entropy:        0.1429
  95 percentile entropy:    0.01737
  Pseudo loss:              2.06869
  Total gradient norm:      1.52014
  Solved trajectories:      4 / 256
  Avg steps to disentangle: 5.000
Episode (250/1001) took 0.385 seconds.
  Mean final reward:        -2.8622
  Mean return:              -22.7637
  Mean final entropy:       0.0187
  Max final entropy:        0.2576
  95 percentile entropy:    0.01737
  Pseudo loss:              1.23748
  Total gradient norm:      1.32719
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (260/1001) took 0.384 seconds.
  Mean final reward:        -2.8462
  Mean return:              -22.7313
  Mean final entropy:       0.0187
  Max final entropy:        0.1593
  95 percentile entropy:    0.01737
  Pseudo loss:              0.52782
  Total gradient norm:      0.98901
  Solved trajectories:      3 / 256
  Avg steps to disentangle: 5.000
Episode (270/1001) took 0.386 seconds.
  Mean final reward:        -2.8199
  Mean return:              -22.7106
  Mean final entropy:       0.0188
  Max final entropy:        0.1409
  95 percentile entropy:    0.01737
  Pseudo loss:              0.62770
  Total gradient norm:      1.21798
  Solved trajectories:      6 / 256
  Avg steps to disentangle: 5.000
Episode (280/1001) took 0.386 seconds.
  Mean final reward:        -2.8197
  Mean return:              -22.7060
  Mean final entropy:       0.0188
  Max final entropy:        0.1323
  95 percentile entropy:    0.01737
  Pseudo loss:              0.22333
  Total gradient norm:      0.90472
  Solved trajectories:      6 / 256
  Avg steps to disentangle: 5.000
Episode (290/1001) took 0.384 seconds.
  Mean final reward:        -2.1586
  Mean return:              -22.0709
  Mean final entropy:       0.0161
  Max final entropy:        0.2599
  95 percentile entropy:    0.01737
  Pseudo loss:              0.48955
  Total gradient norm:      4.99389
  Solved trajectories:      67 / 256
  Avg steps to disentangle: 5.000
Episode (300/1001) took 0.384 seconds.
  Mean final reward:        -0.5194
  Mean return:              -20.5821
  Mean final entropy:       0.0120
  Max final entropy:        0.3629
  95 percentile entropy:    0.13226
  Pseudo loss:              -1.22121
  Total gradient norm:      5.42530
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 5.000
Episode (310/1001) took 0.382 seconds.
  Mean final reward:        -0.1785
  Mean return:              -20.0509
  Mean final entropy:       0.0021
  Max final entropy:        0.3052
  95 percentile entropy:    0.01737
  Pseudo loss:              -0.05784
  Total gradient norm:      1.80722
  Solved trajectories:      241 / 256
  Avg steps to disentangle: 5.000
Episode (320/1001) took 0.384 seconds.
  Mean final reward:        -0.0748
  Mean return:              -19.9374
  Mean final entropy:       0.0009
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.33771
  Total gradient norm:      0.73738
  Solved trajectories:      250 / 256
  Avg steps to disentangle: 5.000
Episode (330/1001) took 0.383 seconds.
  Mean final reward:        -0.0743
  Mean return:              -19.9547
  Mean final entropy:       0.0017
  Max final entropy:        0.2599
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.18981
  Total gradient norm:      0.59149
  Solved trajectories:      251 / 256
  Avg steps to disentangle: 5.000
Episode (340/1001) took 0.384 seconds.
  Mean final reward:        -0.0329
  Mean return:              -19.9010
  Mean final entropy:       0.0011
  Max final entropy:        0.2599
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01853
  Total gradient norm:      0.46238
  Solved trajectories:      254 / 256
  Avg steps to disentangle: 5.000
Episode (350/1001) took 0.386 seconds.
  Mean final reward:        -0.0414
  Mean return:              -19.9039
  Mean final entropy:       0.0007
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.31432
  Total gradient norm:      0.47856
  Solved trajectories:      253 / 256
  Avg steps to disentangle: 5.000
Episode (360/1001) took 0.384 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.03142
  Total gradient norm:      0.27963
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (370/1001) took 0.384 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.04326
  Total gradient norm:      0.28741
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (380/1001) took 0.386 seconds.
  Mean final reward:        -0.0383
  Mean return:              -19.9010
  Mean final entropy:       0.0004
  Max final entropy:        0.0598
  95 percentile entropy:    0.00000
  Pseudo loss:              0.40818
  Total gradient norm:      0.91820
  Solved trajectories:      253 / 256
  Avg steps to disentangle: 5.000
Episode (390/1001) took 0.303 seconds.
  Mean final reward:        -0.0605
  Mean return:              -19.9353
  Mean final entropy:       0.0012
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.45289
  Total gradient norm:      0.58066
  Solved trajectories:      252 / 256
  Avg steps to disentangle: 5.000
Episode (400/1001) took 0.389 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.05213
  Total gradient norm:      0.28606
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (410/1001) took 0.388 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.02170
  Total gradient norm:      0.20395
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (420/1001) took 0.387 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.02059
  Total gradient norm:      0.19640
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (430/1001) took 0.389 seconds.
  Mean final reward:        -0.0302
  Mean return:              -19.8972
  Mean final entropy:       0.0006
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.20903
  Total gradient norm:      0.41511
  Solved trajectories:      254 / 256
  Avg steps to disentangle: 5.000
Episode (440/1001) took 0.388 seconds.
  Mean final reward:        -0.0335
  Mean return:              -19.8881
  Mean final entropy:       0.0002
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.24617
  Total gradient norm:      0.54963
  Solved trajectories:      253 / 256
  Avg steps to disentangle: 5.000
Episode (450/1001) took 0.379 seconds.
  Mean final reward:        -0.0223
  Mean return:              -19.8769
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.13920
  Total gradient norm:      0.36404
  Solved trajectories:      254 / 256
  Avg steps to disentangle: 5.000
Episode (460/1001) took 0.389 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.06191
  Total gradient norm:      0.25110
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (470/1001) took 0.389 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.06552
  Total gradient norm:      0.23916
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (480/1001) took 0.389 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01332
  Total gradient norm:      0.13009
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (490/1001) took 0.389 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01246
  Total gradient norm:      0.12311
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (500/1001) took 0.388 seconds.
  Mean final reward:        -0.0191
  Mean return:              -19.8816
  Mean final entropy:       0.0005
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.20780
  Total gradient norm:      0.55014
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (510/1001) took 0.388 seconds.
  Mean final reward:        -0.0216
  Mean return:              -19.8915
  Mean final entropy:       0.0010
  Max final entropy:        0.2514
  95 percentile entropy:    0.00000
  Pseudo loss:              0.56321
  Total gradient norm:      0.84509
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (520/1001) took 0.388 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01078
  Total gradient norm:      0.10957
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (530/1001) took 0.304 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01110
  Total gradient norm:      0.11379
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (540/1001) took 0.303 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01073
  Total gradient norm:      0.11027
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (550/1001) took 0.212 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.01068
  Total gradient norm:      0.10988
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (560/1001) took 0.212 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00989
  Total gradient norm:      0.10188
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (570/1001) took 0.211 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00946
  Total gradient norm:      0.09768
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (580/1001) took 0.212 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.07159
  Total gradient norm:      0.23287
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (590/1001) took 0.211 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00945
  Total gradient norm:      0.09857
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (600/1001) took 0.212 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00954
  Total gradient norm:      0.10040
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (610/1001) took 0.212 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00932
  Total gradient norm:      0.09928
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (620/1001) took 0.212 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00848
  Total gradient norm:      0.09030
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (630/1001) took 0.210 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.07210
  Total gradient norm:      0.23293
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (640/1001) took 0.212 seconds.
  Mean final reward:        -0.0223
  Mean return:              -19.8769
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.16337
  Total gradient norm:      0.36987
  Solved trajectories:      254 / 256
  Avg steps to disentangle: 5.000
Episode (650/1001) took 0.215 seconds.
  Mean final reward:        -0.0302
  Mean return:              -19.8972
  Mean final entropy:       0.0006
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.25984
  Total gradient norm:      0.28787
  Solved trajectories:      254 / 256
  Avg steps to disentangle: 5.000
Episode (660/1001) took 0.214 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00813
  Total gradient norm:      0.08664
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (670/1001) took 0.211 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00755
  Total gradient norm:      0.08032
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (680/1001) took 0.213 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00709
  Total gradient norm:      0.07544
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (690/1001) took 0.211 seconds.
  Mean final reward:        -0.0223
  Mean return:              -19.8769
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.17359
  Total gradient norm:      0.42829
  Solved trajectories:      254 / 256
  Avg steps to disentangle: 5.000
Episode (700/1001) took 0.301 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00611
  Total gradient norm:      0.06412
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (710/1001) took 0.301 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00600
  Total gradient norm:      0.06361
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (720/1001) took 0.302 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00658
  Total gradient norm:      0.07132
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (730/1001) took 0.302 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00651
  Total gradient norm:      0.07092
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (740/1001) took 0.302 seconds.
  Mean final reward:        -0.0237
  Mean return:              -19.8964
  Mean final entropy:       0.0017
  Max final entropy:        0.4349
  95 percentile entropy:    0.00000
  Pseudo loss:              0.59549
  Total gradient norm:      0.83564
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (750/1001) took 0.300 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.08637
  Total gradient norm:      0.23066
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (760/1001) took 0.301 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00860
  Total gradient norm:      0.09628
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (770/1001) took 0.227 seconds.
  Mean final reward:        -0.0217
  Mean return:              -19.8899
  Mean final entropy:       0.0010
  Max final entropy:        0.2599
  95 percentile entropy:    0.00000
  Pseudo loss:              0.06424
  Total gradient norm:      0.23758
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (780/1001) took 0.215 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00899
  Total gradient norm:      0.10072
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (790/1001) took 0.221 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.10511
  Total gradient norm:      0.19560
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (800/1001) took 0.307 seconds.
  Mean final reward:        -0.0160
  Mean return:              -19.8770
  Mean final entropy:       0.0002
  Max final entropy:        0.0605
  95 percentile entropy:    0.00000
  Pseudo loss:              0.69015
  Total gradient norm:      1.03903
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (810/1001) took 0.301 seconds.
  Mean final reward:        -0.0191
  Mean return:              -19.8816
  Mean final entropy:       0.0005
  Max final entropy:        0.1323
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.24034
  Total gradient norm:      0.55871
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (820/1001) took 0.303 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00666
  Total gradient norm:      0.07394
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (830/1001) took 0.303 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00597
  Total gradient norm:      0.06575
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (840/1001) took 0.301 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.09153
  Total gradient norm:      0.22237
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (850/1001) took 0.303 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00537
  Total gradient norm:      0.05804
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (860/1001) took 0.304 seconds.
  Mean final reward:        -0.0112
  Mean return:              -19.8658
  Mean final entropy:       0.0001
  Max final entropy:        0.0174
  95 percentile entropy:    0.00000
  Pseudo loss:              -0.09417
  Total gradient norm:      0.21283
  Solved trajectories:      255 / 256
  Avg steps to disentangle: 5.000
Episode (870/1001) took 0.300 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00453
  Total gradient norm:      0.04868
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (880/1001) took 0.245 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00418
  Total gradient norm:      0.04481
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (890/1001) took 0.304 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00387
  Total gradient norm:      0.04125
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (900/1001) took 0.302 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00361
  Total gradient norm:      0.03802
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (910/1001) took 0.304 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00338
  Total gradient norm:      0.03542
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (920/1001) took 0.301 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00318
  Total gradient norm:      0.03291
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (930/1001) took 0.303 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00326
  Total gradient norm:      0.03413
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (940/1001) took 0.302 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00326
  Total gradient norm:      0.03443
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (950/1001) took 0.302 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00330
  Total gradient norm:      0.03505
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (960/1001) took 0.300 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00330
  Total gradient norm:      0.03499
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (970/1001) took 0.292 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00316
  Total gradient norm:      0.03344
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (980/1001) took 0.385 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00325
  Total gradient norm:      0.03479
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (990/1001) took 0.387 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00316
  Total gradient norm:      0.03417
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
Episode (1000/1001) took 0.384 seconds.
  Mean final reward:        0.0000
  Mean return:              -19.8546
  Mean final entropy:       0.0000
  Max final entropy:        0.0000
  95 percentile entropy:    0.00000
  Pseudo loss:              0.00365
  Total gradient norm:      0.04088
  Solved trajectories:      256 / 256
  Avg steps to disentangle: 5.000
