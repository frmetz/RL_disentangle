##############################
Training parameters:
  Number of trajectories:   256
  Number of episodes:       10001
  Learning rate:            0.0001
  Final learning rate:      0.0001
  Weight regularization:    0.0
  Entropy regularization:   0.0
  Grad clipping threshold:  1.0
  Policy hidden dimensions: [4096, 2048, 512]
  Policy dropout rate:      0.0

Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 0.0
)

Episode (1/10001) took 0.998 seconds.
  Mean final reward:        -5.4991
  Mean return:              -29.7091
  Mean final entropy:       0.2935
  Max final entropy:        0.5433
  95 percentile entropy:    0.48285
  Pseudo loss:              199.80316
  Total gradient norm:      66.75943
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (10/10001) took 0.847 seconds.
  Mean final reward:        -5.5869
  Mean return:              -30.0057
  Mean final entropy:       0.3305
  Max final entropy:        0.5689
  95 percentile entropy:    0.51055
  Pseudo loss:              164.93753
  Total gradient norm:      51.77534
  Solved trajectories:      4 / 256
  Avg steps to disentangle: 5.000
Episode (20/10001) took 0.836 seconds.
  Mean final reward:        -5.5717
  Mean return:              -29.8973
  Mean final entropy:       0.3132
  Max final entropy:        0.5716
  95 percentile entropy:    0.50075
  Pseudo loss:              162.66800
  Total gradient norm:      64.98167
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (30/10001) took 0.837 seconds.
  Mean final reward:        -5.4974
  Mean return:              -29.6760
  Mean final entropy:       0.2893
  Max final entropy:        0.6101
  95 percentile entropy:    0.51043
  Pseudo loss:              157.01904
  Total gradient norm:      57.42780
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (40/10001) took 0.830 seconds.
  Mean final reward:        -5.4159
  Mean return:              -29.4860
  Mean final entropy:       0.2840
  Max final entropy:        0.5895
  95 percentile entropy:    0.47992
  Pseudo loss:              133.81288
  Total gradient norm:      62.32571
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (50/10001) took 0.831 seconds.
  Mean final reward:        -5.6402
  Mean return:              -29.9408
  Mean final entropy:       0.3287
  Max final entropy:        0.6274
  95 percentile entropy:    0.53996
  Pseudo loss:              112.28429
  Total gradient norm:      49.49537
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (60/10001) took 0.823 seconds.
  Mean final reward:        -5.6743
  Mean return:              -30.0477
  Mean final entropy:       0.3344
  Max final entropy:        0.5995
  95 percentile entropy:    0.53774
  Pseudo loss:              112.09096
  Total gradient norm:      47.89113
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (70/10001) took 0.833 seconds.
  Mean final reward:        -5.4944
  Mean return:              -29.6945
  Mean final entropy:       0.2980
  Max final entropy:        0.5914
  95 percentile entropy:    0.51886
  Pseudo loss:              115.41371
  Total gradient norm:      42.98986
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (80/10001) took 0.921 seconds.
  Mean final reward:        -5.6191
  Mean return:              -29.9669
  Mean final entropy:       0.3273
  Max final entropy:        0.6344
  95 percentile entropy:    0.52792
  Pseudo loss:              114.42072
  Total gradient norm:      58.22642
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (90/10001) took 0.842 seconds.
  Mean final reward:        -5.5436
  Mean return:              -29.8098
  Mean final entropy:       0.3167
  Max final entropy:        0.6008
  95 percentile entropy:    0.52368
  Pseudo loss:              116.38049
  Total gradient norm:      49.16314
  Solved trajectories:      4 / 256
  Avg steps to disentangle: 5.000
Episode (100/10001) took 1.238 seconds.
  Mean final reward:        -5.4447
  Mean return:              -29.5274
  Mean final entropy:       0.2872
  Max final entropy:        0.5974
  95 percentile entropy:    0.51049
  Pseudo loss:              134.25000
  Total gradient norm:      43.69582
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (110/10001) took 1.420 seconds.
  Mean final reward:        -5.5178
  Mean return:              -29.6832
  Mean final entropy:       0.2937
  Max final entropy:        0.5749
  95 percentile entropy:    0.50264
  Pseudo loss:              142.40855
  Total gradient norm:      50.08666
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (120/10001) took 0.827 seconds.
  Mean final reward:        -5.5690
  Mean return:              -29.8304
  Mean final entropy:       0.3041
  Max final entropy:        0.5753
  95 percentile entropy:    0.46867
  Pseudo loss:              131.76070
  Total gradient norm:      52.33064
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (130/10001) took 0.920 seconds.
  Mean final reward:        -5.5503
  Mean return:              -29.7764
  Mean final entropy:       0.3039
  Max final entropy:        0.5913
  95 percentile entropy:    0.50004
  Pseudo loss:              128.56706
  Total gradient norm:      58.13873
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (140/10001) took 1.188 seconds.
  Mean final reward:        -5.3956
  Mean return:              -29.4985
  Mean final entropy:       0.2727
  Max final entropy:        0.5947
  95 percentile entropy:    0.48005
  Pseudo loss:              119.55795
  Total gradient norm:      48.09302
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (150/10001) took 0.906 seconds.
  Mean final reward:        -5.5067
  Mean return:              -29.5906
  Mean final entropy:       0.2956
  Max final entropy:        0.5913
  95 percentile entropy:    0.48464
  Pseudo loss:              102.78590
  Total gradient norm:      53.27091
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (160/10001) took 0.870 seconds.
  Mean final reward:        -5.4925
  Mean return:              -29.7019
  Mean final entropy:       0.3022
  Max final entropy:        0.5718
  95 percentile entropy:    0.52307
  Pseudo loss:              98.87918
  Total gradient norm:      49.27837
  Solved trajectories:      3 / 256
  Avg steps to disentangle: 5.000
Episode (170/10001) took 0.846 seconds.
  Mean final reward:        -5.7880
  Mean return:              -30.2964
  Mean final entropy:       0.3755
  Max final entropy:        0.6344
  95 percentile entropy:    0.57875
  Pseudo loss:              67.42795
  Total gradient norm:      36.94517
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (180/10001) took 0.871 seconds.
  Mean final reward:        -5.4750
  Mean return:              -29.6560
  Mean final entropy:       0.2987
  Max final entropy:        0.5974
  95 percentile entropy:    0.51995
  Pseudo loss:              77.20976
  Total gradient norm:      47.63934
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (190/10001) took 0.887 seconds.
  Mean final reward:        -5.3315
  Mean return:              -29.3376
  Mean final entropy:       0.2639
  Max final entropy:        0.5552
  95 percentile entropy:    0.47561
  Pseudo loss:              88.17699
  Total gradient norm:      49.85432
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (200/10001) took 0.884 seconds.
  Mean final reward:        -5.2127
  Mean return:              -29.0244
  Mean final entropy:       0.2410
  Max final entropy:        0.5771
  95 percentile entropy:    0.46494
  Pseudo loss:              83.15778
  Total gradient norm:      37.76165
  Solved trajectories:      3 / 256
  Avg steps to disentangle: 5.000
Episode (210/10001) took 0.869 seconds.
  Mean final reward:        -5.2202
  Mean return:              -29.0931
  Mean final entropy:       0.2356
  Max final entropy:        0.5826
  95 percentile entropy:    0.45922
  Pseudo loss:              80.98931
  Total gradient norm:      43.67195
  Solved trajectories:      2 / 256
  Avg steps to disentangle: 5.000
Episode (220/10001) took 0.863 seconds.
  Mean final reward:        -5.3976
  Mean return:              -29.4275
  Mean final entropy:       0.2722
  Max final entropy:        0.6188
  95 percentile entropy:    0.51019
  Pseudo loss:              71.62257
  Total gradient norm:      42.11010
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (230/10001) took 0.858 seconds.
  Mean final reward:        -5.6328
  Mean return:              -29.7877
  Mean final entropy:       0.3219
  Max final entropy:        0.5874
  95 percentile entropy:    0.50754
  Pseudo loss:              63.71522
  Total gradient norm:      36.26809
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (240/10001) took 1.090 seconds.
  Mean final reward:        -5.5733
  Mean return:              -29.7292
  Mean final entropy:       0.3171
  Max final entropy:        0.6321
  95 percentile entropy:    0.57259
  Pseudo loss:              60.55949
  Total gradient norm:      44.73633
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (250/10001) took 0.859 seconds.
  Mean final reward:        -5.3382
  Mean return:              -29.3107
  Mean final entropy:       0.2652
  Max final entropy:        0.6090
  95 percentile entropy:    0.52320
  Pseudo loss:              58.66339
  Total gradient norm:      60.44873
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (260/10001) took 1.193 seconds.
  Mean final reward:        -5.3778
  Mean return:              -29.3513
  Mean final entropy:       0.2721
  Max final entropy:        0.5827
  95 percentile entropy:    0.49027
  Pseudo loss:              60.92310
  Total gradient norm:      49.84027
  Solved trajectories:      1 / 256
  Avg steps to disentangle: 5.000
Episode (270/10001) took 0.841 seconds.
  Mean final reward:        -5.3718
  Mean return:              -29.3760
  Mean final entropy:       0.2641
  Max final entropy:        0.6267
  95 percentile entropy:    0.50016
  Pseudo loss:              55.32609
  Total gradient norm:      39.24677
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (280/10001) took 0.876 seconds.
  Mean final reward:        -5.3837
  Mean return:              -29.3615
  Mean final entropy:       0.2743
  Max final entropy:        0.6045
  95 percentile entropy:    0.49767
  Pseudo loss:              47.89594
  Total gradient norm:      31.74548
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (290/10001) took 0.871 seconds.
  Mean final reward:        -5.5292
  Mean return:              -29.7276
  Mean final entropy:       0.3104
  Max final entropy:        0.6143
  95 percentile entropy:    0.55097
  Pseudo loss:              43.81260
  Total gradient norm:      52.28795
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (300/10001) took 0.864 seconds.
  Mean final reward:        -5.2494
  Mean return:              -29.1402
  Mean final entropy:       0.2420
  Max final entropy:        0.5788
  95 percentile entropy:    0.46815
  Pseudo loss:              51.64169
  Total gradient norm:      44.28754
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (310/10001) took 0.830 seconds.
  Mean final reward:        -5.2311
  Mean return:              -29.0859
  Mean final entropy:       0.2330
  Max final entropy:        0.5314
  95 percentile entropy:    0.47290
  Pseudo loss:              53.00912
  Total gradient norm:      45.52946
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (320/10001) took 0.823 seconds.
  Mean final reward:        -5.4022
  Mean return:              -29.4099
  Mean final entropy:       0.2675
  Max final entropy:        0.5567
  95 percentile entropy:    0.47086
  Pseudo loss:              54.40331
  Total gradient norm:      46.87574
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (330/10001) took 1.053 seconds.
  Mean final reward:        -5.6274
  Mean return:              -29.8595
  Mean final entropy:       0.3239
  Max final entropy:        0.6136
  95 percentile entropy:    0.54207
  Pseudo loss:              47.35345
  Total gradient norm:      65.01431
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (340/10001) took 0.835 seconds.
  Mean final reward:        -5.7675
  Mean return:              -30.1557
  Mean final entropy:       0.3611
  Max final entropy:        0.6220
  95 percentile entropy:    0.56815
  Pseudo loss:              43.22814
  Total gradient norm:      41.52909
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (350/10001) took 0.829 seconds.
  Mean final reward:        -5.9764
  Mean return:              -30.6344
  Mean final entropy:       0.4208
  Max final entropy:        0.6282
  95 percentile entropy:    0.59054
  Pseudo loss:              33.34479
  Total gradient norm:      32.32476
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (360/10001) took 1.171 seconds.
  Mean final reward:        -5.7813
  Mean return:              -30.2523
  Mean final entropy:       0.3725
  Max final entropy:        0.6136
  95 percentile entropy:    0.57523
  Pseudo loss:              28.12821
  Total gradient norm:      34.70338
  Solved trajectories:      0 / 256
  Avg steps to disentangle: nan
Episode (370/10001) took 0.908 seconds.
