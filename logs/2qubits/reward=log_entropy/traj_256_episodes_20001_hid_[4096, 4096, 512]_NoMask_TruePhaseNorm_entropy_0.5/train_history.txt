##############################
Training parameters:
  Number of trajectories:   256
  Number of episodes:       20001
  Learning rate:            0.0001
  Final learning rate:      0.0001
  Weight regularization:    0.0
  Entropy regularization:   0.5
  Grad clipping threshold:  10.0
  Policy hidden dimensions: [4096, 4096, 512]
  Policy dropout rate:      0.0

Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 0.0
)

Episode (1/20001) took 15.115 seconds.
  Mean final reward:        -1.7352
  Mean return:              -8.5893
  Mean final entropy:       0.0257
  Max final entropy:        0.4758
  95 percentile entropy:    0.11783
  Pseudo loss:              12.00769
  Total gradient norm:      15.15449
  Solved trajectories:      97 / 256
  Avg steps to disentangle: 1.508
Episode (100/20001) took 13.512 seconds.
  Mean final reward:        -1.3258
  Mean return:              -7.4562
  Mean final entropy:       0.0157
  Max final entropy:        0.2605
  95 percentile entropy:    0.07412
  Pseudo loss:              5.56989
  Total gradient norm:      8.07044
  Solved trajectories:      127 / 256
  Avg steps to disentangle: 1.695
Episode (200/20001) took 17.918 seconds.
  Mean final reward:        -1.1117
  Mean return:              -7.0461
  Mean final entropy:       0.0132
  Max final entropy:        0.2918
  95 percentile entropy:    0.05994
  Pseudo loss:              4.23937
  Total gradient norm:      7.03288
  Solved trajectories:      140 / 256
  Avg steps to disentangle: 1.777
Episode (300/20001) took 13.673 seconds.
  Mean final reward:        -1.1473
  Mean return:              -6.9594
  Mean final entropy:       0.0116
  Max final entropy:        0.1617
  95 percentile entropy:    0.06093
  Pseudo loss:              4.29434
  Total gradient norm:      7.72895
  Solved trajectories:      138 / 256
  Avg steps to disentangle: 1.680
Episode (400/20001) took 13.921 seconds.
  Mean final reward:        -1.0330
  Mean return:              -6.4731
  Mean final entropy:       0.0108
  Max final entropy:        0.3355
  95 percentile entropy:    0.05694
  Pseudo loss:              3.55425
  Total gradient norm:      8.80763
  Solved trajectories:      133 / 256
  Avg steps to disentangle: 1.633
Episode (500/20001) took 12.996 seconds.
  Mean final reward:        -0.8609
  Mean return:              -6.1421
  Mean final entropy:       0.0076
  Max final entropy:        0.1296
  95 percentile entropy:    0.04667
  Pseudo loss:              2.53223
  Total gradient norm:      6.98352
  Solved trajectories:      151 / 256
  Avg steps to disentangle: 1.758
Episode (600/20001) took 16.782 seconds.
  Mean final reward:        -0.8254
  Mean return:              -5.8377
  Mean final entropy:       0.0067
  Max final entropy:        0.1106
  95 percentile entropy:    0.04035
  Pseudo loss:              1.76443
  Total gradient norm:      6.37930
  Solved trajectories:      150 / 256
  Avg steps to disentangle: 1.711
Episode (700/20001) took 16.069 seconds.
  Mean final reward:        -0.8432
  Mean return:              -6.0465
  Mean final entropy:       0.0064
  Max final entropy:        0.1356
  95 percentile entropy:    0.03187
  Pseudo loss:              3.02285
  Total gradient norm:      7.05034
  Solved trajectories:      145 / 256
  Avg steps to disentangle: 1.762
Episode (800/20001) took 17.638 seconds.
  Mean final reward:        -0.7545
  Mean return:              -5.5981
  Mean final entropy:       0.0058
  Max final entropy:        0.0873
  95 percentile entropy:    0.03976
  Pseudo loss:              1.33645
  Total gradient norm:      8.94409
  Solved trajectories:      161 / 256
  Avg steps to disentangle: 1.797
Episode (900/20001) took 17.066 seconds.
  Mean final reward:        -0.8242
  Mean return:              -5.9822
  Mean final entropy:       0.0070
  Max final entropy:        0.1376
  95 percentile entropy:    0.04040
  Pseudo loss:              2.09994
  Total gradient norm:      7.24244
  Solved trajectories:      151 / 256
  Avg steps to disentangle: 1.750
Episode (1000/20001) took 12.973 seconds.
  Mean final reward:        -0.6884
  Mean return:              -5.5224
  Mean final entropy:       0.0054
  Max final entropy:        0.1019
  95 percentile entropy:    0.03427
  Pseudo loss:              1.35562
  Total gradient norm:      6.41944
  Solved trajectories:      158 / 256
  Avg steps to disentangle: 1.773
Episode (1100/20001) took 13.453 seconds.
  Mean final reward:        -0.7473
  Mean return:              -5.8292
  Mean final entropy:       0.0060
  Max final entropy:        0.1273
  95 percentile entropy:    0.03707
  Pseudo loss:              1.91927
  Total gradient norm:      6.94205
  Solved trajectories:      159 / 256
  Avg steps to disentangle: 1.812
Episode (1200/20001) took 14.929 seconds.
  Mean final reward:        -1.0281
  Mean return:              -6.0773
  Mean final entropy:       0.0092
  Max final entropy:        0.1253
  95 percentile entropy:    0.05194
  Pseudo loss:              2.77381
  Total gradient norm:      8.10054
  Solved trajectories:      139 / 256
  Avg steps to disentangle: 1.660
Episode (1300/20001) took 14.410 seconds.
  Mean final reward:        -0.6048
  Mean return:              -5.2399
  Mean final entropy:       0.0043
  Max final entropy:        0.0951
  95 percentile entropy:    0.02497
  Pseudo loss:              0.96718
  Total gradient norm:      6.26545
  Solved trajectories:      168 / 256
  Avg steps to disentangle: 1.809
Episode (1400/20001) took 14.165 seconds.
  Mean final reward:        -0.6254
  Mean return:              -5.3675
  Mean final entropy:       0.0039
  Max final entropy:        0.0750
  95 percentile entropy:    0.01818
  Pseudo loss:              1.69038
  Total gradient norm:      6.16233
  Solved trajectories:      165 / 256
  Avg steps to disentangle: 1.820
Episode (1500/20001) took 17.934 seconds.
  Mean final reward:        -0.8503
  Mean return:              -5.8180
  Mean final entropy:       0.0065
  Max final entropy:        0.1404
  95 percentile entropy:    0.03401
  Pseudo loss:              2.10159
  Total gradient norm:      7.51768
  Solved trajectories:      148 / 256
  Avg steps to disentangle: 1.809
Episode (1600/20001) took 13.278 seconds.
  Mean final reward:        -0.6487
  Mean return:              -5.2103
  Mean final entropy:       0.0045
  Max final entropy:        0.0769
  95 percentile entropy:    0.02771
  Pseudo loss:              1.26705
  Total gradient norm:      6.57539
  Solved trajectories:      168 / 256
  Avg steps to disentangle: 1.848
Episode (1700/20001) took 15.798 seconds.
  Mean final reward:        -0.7189
  Mean return:              -5.4033
  Mean final entropy:       0.0052
  Max final entropy:        0.1059
  95 percentile entropy:    0.02424
  Pseudo loss:              1.58992
  Total gradient norm:      7.25523
  Solved trajectories:      156 / 256
  Avg steps to disentangle: 1.781
Episode (1800/20001) took 13.390 seconds.
  Mean final reward:        -0.7448
  Mean return:              -5.4796
  Mean final entropy:       0.0047
  Max final entropy:        0.0861
  95 percentile entropy:    0.02389
  Pseudo loss:              1.54416
  Total gradient norm:      6.77648
  Solved trajectories:      152 / 256
  Avg steps to disentangle: 1.754
Episode (1900/20001) took 12.596 seconds.
  Mean final reward:        -0.6451
  Mean return:              -5.1862
  Mean final entropy:       0.0042
  Max final entropy:        0.1049
  95 percentile entropy:    0.01720
  Pseudo loss:              1.12531
  Total gradient norm:      6.57457
  Solved trajectories:      159 / 256
  Avg steps to disentangle: 1.777
Episode (2000/20001) took 12.634 seconds.
  Mean final reward:        -0.7409
  Mean return:              -5.2035
  Mean final entropy:       0.0046
  Max final entropy:        0.0630
  95 percentile entropy:    0.02329
  Pseudo loss:              1.51715
  Total gradient norm:      7.13006
  Solved trajectories:      149 / 256
  Avg steps to disentangle: 1.711
Episode (2100/20001) took 12.634 seconds.
  Mean final reward:        -0.7377
  Mean return:              -5.0986
  Mean final entropy:       0.0051
  Max final entropy:        0.1484
  95 percentile entropy:    0.02481
  Pseudo loss:              0.91324
  Total gradient norm:      6.79265
  Solved trajectories:      160 / 256
  Avg steps to disentangle: 1.777
Episode (2200/20001) took 12.586 seconds.
  Mean final reward:        -0.7762
  Mean return:              -5.4185
  Mean final entropy:       0.0056
  Max final entropy:        0.1061
  95 percentile entropy:    0.03288
  Pseudo loss:              2.29416
  Total gradient norm:      8.89499
  Solved trajectories:      153 / 256
  Avg steps to disentangle: 1.809
Episode (2300/20001) took 12.785 seconds.
  Mean final reward:        -0.7275
  Mean return:              -4.9736
  Mean final entropy:       0.0050
  Max final entropy:        0.0704
  95 percentile entropy:    0.02906
  Pseudo loss:              1.39713
  Total gradient norm:      7.10207
  Solved trajectories:      150 / 256
  Avg steps to disentangle: 1.672
Episode (2400/20001) took 12.545 seconds.
  Mean final reward:        -0.6684
  Mean return:              -4.5393
  Mean final entropy:       0.0048
  Max final entropy:        0.1066
  95 percentile entropy:    0.02328
  Pseudo loss:              0.75753
  Total gradient norm:      7.19781
  Solved trajectories:      164 / 256
  Avg steps to disentangle: 1.723
Episode (2500/20001) took 12.615 seconds.
  Mean final reward:        -0.5815
  Mean return:              -4.6255
  Mean final entropy:       0.0033
  Max final entropy:        0.0662
  95 percentile entropy:    0.01668
  Pseudo loss:              0.75970
  Total gradient norm:      6.37097
  Solved trajectories:      164 / 256
  Avg steps to disentangle: 1.703
Episode (2600/20001) took 12.601 seconds.
  Mean final reward:        -0.6938
  Mean return:              -4.7369
  Mean final entropy:       0.0044
  Max final entropy:        0.0604
  95 percentile entropy:    0.02403
  Pseudo loss:              0.82454
  Total gradient norm:      7.15064
  Solved trajectories:      149 / 256
  Avg steps to disentangle: 1.672
Episode (2700/20001) took 12.677 seconds.
  Mean final reward:        -0.5605
  Mean return:              -4.5188
  Mean final entropy:       0.0034
  Max final entropy:        0.0853
  95 percentile entropy:    0.01453
  Pseudo loss:              0.52539
  Total gradient norm:      6.12008
  Solved trajectories:      172 / 256
  Avg steps to disentangle: 1.832
Episode (2800/20001) took 12.719 seconds.
  Mean final reward:        -0.5117
  Mean return:              -4.3845
  Mean final entropy:       0.0028
  Max final entropy:        0.0507
  95 percentile entropy:    0.01382
  Pseudo loss:              0.27019
  Total gradient norm:      6.00629
  Solved trajectories:      169 / 256
  Avg steps to disentangle: 1.797
Episode (2900/20001) took 12.564 seconds.
  Mean final reward:        -0.6407
  Mean return:              -4.7633
  Mean final entropy:       0.0042
  Max final entropy:        0.0852
  95 percentile entropy:    0.02467
  Pseudo loss:              0.77573
  Total gradient norm:      7.69395
  Solved trajectories:      147 / 256
  Avg steps to disentangle: 1.641
Episode (3000/20001) took 12.563 seconds.
  Mean final reward:        -0.5698
  Mean return:              -4.4145
  Mean final entropy:       0.0039
  Max final entropy:        0.0771
  95 percentile entropy:    0.01967
  Pseudo loss:              0.04577
  Total gradient norm:      7.02221
  Solved trajectories:      171 / 256
  Avg steps to disentangle: 1.777
Episode (3100/20001) took 12.579 seconds.
  Mean final reward:        -0.5809
  Mean return:              -4.5005
  Mean final entropy:       0.0035
  Max final entropy:        0.0697
  95 percentile entropy:    0.01679
  Pseudo loss:              0.42994
  Total gradient norm:      6.93689
  Solved trajectories:      161 / 256
  Avg steps to disentangle: 1.695
Episode (3200/20001) took 12.905 seconds.
  Mean final reward:        -0.5333
  Mean return:              -4.2761
  Mean final entropy:       0.0035
  Max final entropy:        0.0779
  95 percentile entropy:    0.01516
  Pseudo loss:              0.81248
  Total gradient norm:      8.17798
  Solved trajectories:      173 / 256
  Avg steps to disentangle: 1.770
Episode (3300/20001) took 12.504 seconds.
  Mean final reward:        -0.5891
  Mean return:              -4.5744
  Mean final entropy:       0.0034
  Max final entropy:        0.0515
  95 percentile entropy:    0.01666
  Pseudo loss:              0.30908
  Total gradient norm:      6.77502
  Solved trajectories:      163 / 256
  Avg steps to disentangle: 1.777
Episode (3400/20001) took 12.622 seconds.
  Mean final reward:        -0.5605
  Mean return:              -4.3823
  Mean final entropy:       0.0030
  Max final entropy:        0.0666
  95 percentile entropy:    0.01358
  Pseudo loss:              0.58836
  Total gradient norm:      6.67117
  Solved trajectories:      162 / 256
  Avg steps to disentangle: 1.730
Episode (3500/20001) took 12.580 seconds.
  Mean final reward:        -0.5468
  Mean return:              -4.2839
  Mean final entropy:       0.0033
  Max final entropy:        0.0816
  95 percentile entropy:    0.01600
  Pseudo loss:              0.28145
  Total gradient norm:      6.30599
  Solved trajectories:      173 / 256
  Avg steps to disentangle: 1.797
Episode (3600/20001) took 12.616 seconds.
  Mean final reward:        -0.5098
  Mean return:              -4.2044
  Mean final entropy:       0.0027
  Max final entropy:        0.0387
  95 percentile entropy:    0.01656
  Pseudo loss:              -0.07215
  Total gradient norm:      7.41673
  Solved trajectories:      172 / 256
  Avg steps to disentangle: 1.762
Episode (3700/20001) took 12.596 seconds.
  Mean final reward:        -0.5178
  Mean return:              -4.2682
  Mean final entropy:       0.0026
  Max final entropy:        0.0448
  95 percentile entropy:    0.01238
  Pseudo loss:              0.29849
  Total gradient norm:      6.10203
  Solved trajectories:      171 / 256
  Avg steps to disentangle: 1.746
Episode (3800/20001) took 12.684 seconds.
  Mean final reward:        -0.5411
  Mean return:              -4.1679
  Mean final entropy:       0.0027
  Max final entropy:        0.0611
  95 percentile entropy:    0.01338
  Pseudo loss:              0.11513
  Total gradient norm:      6.51342
  Solved trajectories:      165 / 256
  Avg steps to disentangle: 1.758
Episode (3900/20001) took 12.596 seconds.
  Mean final reward:        -0.4436
  Mean return:              -4.0936
  Mean final entropy:       0.0023
  Max final entropy:        0.0571
  95 percentile entropy:    0.01137
  Pseudo loss:              -0.01558
  Total gradient norm:      6.52927
  Solved trajectories:      173 / 256
  Avg steps to disentangle: 1.785
Episode (4000/20001) took 12.619 seconds.
  Mean final reward:        -0.5102
  Mean return:              -4.1361
  Mean final entropy:       0.0025
  Max final entropy:        0.0557
  95 percentile entropy:    0.01214
  Pseudo loss:              0.02992
  Total gradient norm:      6.41630
  Solved trajectories:      172 / 256
  Avg steps to disentangle: 1.738
Episode (4100/20001) took 14.964 seconds.
  Mean final reward:        -0.5239
  Mean return:              -4.2195
  Mean final entropy:       0.0023
  Max final entropy:        0.0293
  95 percentile entropy:    0.01033
  Pseudo loss:              0.06470
  Total gradient norm:      6.01413
  Solved trajectories:      156 / 256
  Avg steps to disentangle: 1.691
Episode (4200/20001) took 13.558 seconds.
  Mean final reward:        -0.5285
  Mean return:              -4.1769
  Mean final entropy:       0.0031
  Max final entropy:        0.0632
  95 percentile entropy:    0.02158
  Pseudo loss:              0.08077
  Total gradient norm:      6.10132
  Solved trajectories:      166 / 256
  Avg steps to disentangle: 1.781
Episode (4300/20001) took 12.563 seconds.
  Mean final reward:        -0.3888
  Mean return:              -3.8841
  Mean final entropy:       0.0020
  Max final entropy:        0.0438
  95 percentile entropy:    0.01267
  Pseudo loss:              -0.35895
  Total gradient norm:      6.27239
  Solved trajectories:      181 / 256
  Avg steps to disentangle: 1.840
Episode (4400/20001) took 12.779 seconds.
  Mean final reward:        -0.4678
  Mean return:              -3.6889
  Mean final entropy:       0.0024
  Max final entropy:        0.0568
  95 percentile entropy:    0.01232
  Pseudo loss:              -0.25215
  Total gradient norm:      6.51257
  Solved trajectories:      174 / 256
  Avg steps to disentangle: 1.664
Episode (4500/20001) took 12.658 seconds.
  Mean final reward:        -0.3845
  Mean return:              -3.6112
  Mean final entropy:       0.0019
  Max final entropy:        0.0571
  95 percentile entropy:    0.00963
  Pseudo loss:              -0.61613
  Total gradient norm:      6.04526
  Solved trajectories:      184 / 256
  Avg steps to disentangle: 1.746
Episode (4600/20001) took 14.441 seconds.
  Mean final reward:        -0.4690
  Mean return:              -4.2609
  Mean final entropy:       0.0020
  Max final entropy:        0.0273
  95 percentile entropy:    0.01057
  Pseudo loss:              -0.02874
  Total gradient norm:      6.41295
  Solved trajectories:      172 / 256
  Avg steps to disentangle: 1.832
Episode (4700/20001) took 14.010 seconds.
  Mean final reward:        -0.3341
  Mean return:              -3.5734
  Mean final entropy:       0.0014
  Max final entropy:        0.0227
  95 percentile entropy:    0.00725
  Pseudo loss:              -0.61990
  Total gradient norm:      5.20621
  Solved trajectories:      183 / 256
  Avg steps to disentangle: 1.773
Episode (4800/20001) took 15.878 seconds.
  Mean final reward:        -0.4042
  Mean return:              -3.7791
  Mean final entropy:       0.0020
  Max final entropy:        0.0733
  95 percentile entropy:    0.00894
  Pseudo loss:              -0.17052
  Total gradient norm:      5.88418
  Solved trajectories:      174 / 256
  Avg steps to disentangle: 1.742
Episode (4900/20001) took 16.635 seconds.
  Mean final reward:        -0.3920
  Mean return:              -3.7196
  Mean final entropy:       0.0017
  Max final entropy:        0.0301
  95 percentile entropy:    0.00828
  Pseudo loss:              -0.28363
  Total gradient norm:      5.98002
  Solved trajectories:      175 / 256
  Avg steps to disentangle: 1.754
Episode (5000/20001) took 14.040 seconds.
  Mean final reward:        -0.4770
  Mean return:              -3.8306
  Mean final entropy:       0.0024
  Max final entropy:        0.0821
  95 percentile entropy:    0.00936
  Pseudo loss:              -0.10314
  Total gradient norm:      6.16972
  Solved trajectories:      169 / 256
  Avg steps to disentangle: 1.715
Episode (5100/20001) took 13.641 seconds.
  Mean final reward:        -0.3604
  Mean return:              -3.5596
  Mean final entropy:       0.0023
  Max final entropy:        0.1007
  95 percentile entropy:    0.00855
  Pseudo loss:              -0.45953
  Total gradient norm:      6.73542
  Solved trajectories:      190 / 256
  Avg steps to disentangle: 1.809
Episode (5200/20001) took 12.902 seconds.
  Mean final reward:        -0.4182
  Mean return:              -3.7756
  Mean final entropy:       0.0022
  Max final entropy:        0.0722
  95 percentile entropy:    0.00998
  Pseudo loss:              -0.28192
  Total gradient norm:      6.27131
  Solved trajectories:      174 / 256
  Avg steps to disentangle: 1.754
Episode (5300/20001) took 12.774 seconds.
  Mean final reward:        -0.3987
  Mean return:              -3.7938
  Mean final entropy:       0.0019
  Max final entropy:        0.0406
  95 percentile entropy:    0.00919
  Pseudo loss:              -0.25694
  Total gradient norm:      6.15297
  Solved trajectories:      180 / 256
  Avg steps to disentangle: 1.781
Episode (5400/20001) took 12.786 seconds.
  Mean final reward:        -0.3670
  Mean return:              -3.7295
  Mean final entropy:       0.0020
  Max final entropy:        0.0549
  95 percentile entropy:    0.00883
  Pseudo loss:              -0.65838
  Total gradient norm:      6.98065
  Solved trajectories:      181 / 256
  Avg steps to disentangle: 1.801
Episode (5500/20001) took 12.704 seconds.
  Mean final reward:        -0.4649
  Mean return:              -3.8545
  Mean final entropy:       0.0026
  Max final entropy:        0.0682
  95 percentile entropy:    0.01171
  Pseudo loss:              -0.25930
  Total gradient norm:      7.20330
  Solved trajectories:      182 / 256
  Avg steps to disentangle: 1.828
Episode (5600/20001) took 12.956 seconds.
  Mean final reward:        -0.3125
  Mean return:              -3.4991
  Mean final entropy:       0.0015
  Max final entropy:        0.0301
  95 percentile entropy:    0.00763
  Pseudo loss:              -0.63948
  Total gradient norm:      6.22157
  Solved trajectories:      194 / 256
  Avg steps to disentangle: 1.848
Episode (5700/20001) took 12.876 seconds.
  Mean final reward:        -0.3035
  Mean return:              -3.4385
  Mean final entropy:       0.0016
  Max final entropy:        0.0407
  95 percentile entropy:    0.00769
  Pseudo loss:              -1.03910
  Total gradient norm:      6.03890
  Solved trajectories:      199 / 256
  Avg steps to disentangle: 1.867
Episode (5800/20001) took 12.863 seconds.
  Mean final reward:        -0.3990
  Mean return:              -3.5407
  Mean final entropy:       0.0020
  Max final entropy:        0.0365
  95 percentile entropy:    0.01197
  Pseudo loss:              -0.54100
  Total gradient norm:      7.25805
  Solved trajectories:      185 / 256
  Avg steps to disentangle: 1.746
Episode (5900/20001) took 12.688 seconds.
  Mean final reward:        -0.3221
  Mean return:              -3.2695
  Mean final entropy:       0.0014
  Max final entropy:        0.0345
  95 percentile entropy:    0.00632
  Pseudo loss:              -0.77413
  Total gradient norm:      5.92515
  Solved trajectories:      182 / 256
  Avg steps to disentangle: 1.746
Episode (6000/20001) took 12.914 seconds.
  Mean final reward:        -0.4264
  Mean return:              -3.4988
  Mean final entropy:       0.0019
  Max final entropy:        0.0395
  95 percentile entropy:    0.00864
  Pseudo loss:              -0.80560
  Total gradient norm:      6.18450
  Solved trajectories:      175 / 256
  Avg steps to disentangle: 1.762
Episode (6100/20001) took 12.749 seconds.
  Mean final reward:        -0.4234
  Mean return:              -3.8457
  Mean final entropy:       0.0022
  Max final entropy:        0.0361
  95 percentile entropy:    0.01171
  Pseudo loss:              -0.16696
  Total gradient norm:      7.19602
  Solved trajectories:      178 / 256
  Avg steps to disentangle: 1.793
Episode (6200/20001) took 12.626 seconds.
  Mean final reward:        -0.3376
  Mean return:              -3.4778
  Mean final entropy:       0.0015
  Max final entropy:        0.0224
  95 percentile entropy:    0.00770
  Pseudo loss:              -0.65882
  Total gradient norm:      7.48487
  Solved trajectories:      185 / 256
  Avg steps to disentangle: 1.816
Episode (6300/20001) took 12.921 seconds.
  Mean final reward:        -0.3564
  Mean return:              -3.4175
  Mean final entropy:       0.0014
  Max final entropy:        0.0155
  95 percentile entropy:    0.00846
  Pseudo loss:              -0.44479
  Total gradient norm:      6.19545
  Solved trajectories:      185 / 256
  Avg steps to disentangle: 1.797
Episode (6400/20001) took 12.904 seconds.
  Mean final reward:        -0.3552
  Mean return:              -3.5206
  Mean final entropy:       0.0014
  Max final entropy:        0.0269
  95 percentile entropy:    0.00719
  Pseudo loss:              -0.35056
  Total gradient norm:      6.53857
  Solved trajectories:      186 / 256
  Avg steps to disentangle: 1.793
Episode (6500/20001) took 12.846 seconds.
  Mean final reward:        -0.3176
  Mean return:              -3.3738
  Mean final entropy:       0.0013
  Max final entropy:        0.0223
  95 percentile entropy:    0.00619
  Pseudo loss:              -0.72047
  Total gradient norm:      6.45225
  Solved trajectories:      190 / 256
  Avg steps to disentangle: 1.836
Episode (6600/20001) took 12.881 seconds.
  Mean final reward:        -0.3353
  Mean return:              -3.5664
  Mean final entropy:       0.0019
  Max final entropy:        0.1071
  95 percentile entropy:    0.00851
  Pseudo loss:              -0.22214
  Total gradient norm:      6.79686
  Solved trajectories:      188 / 256
  Avg steps to disentangle: 1.789
Episode (6700/20001) took 12.878 seconds.
  Mean final reward:        -0.4087
  Mean return:              -3.6550
  Mean final entropy:       0.0019
  Max final entropy:        0.0653
  95 percentile entropy:    0.00924
  Pseudo loss:              -0.21348
  Total gradient norm:      6.45711
  Solved trajectories:      171 / 256
  Avg steps to disentangle: 1.723
Episode (6800/20001) took 12.744 seconds.
  Mean final reward:        -0.3701
  Mean return:              -3.5904
  Mean final entropy:       0.0016
  Max final entropy:        0.0416
  95 percentile entropy:    0.00823
  Pseudo loss:              -0.70654
  Total gradient norm:      7.00685
  Solved trajectories:      181 / 256
  Avg steps to disentangle: 1.742
Episode (6900/20001) took 12.873 seconds.
  Mean final reward:        -0.3230
  Mean return:              -3.5276
  Mean final entropy:       0.0015
  Max final entropy:        0.0288
  95 percentile entropy:    0.00792
  Pseudo loss:              -0.42835
  Total gradient norm:      6.10594
  Solved trajectories:      181 / 256
  Avg steps to disentangle: 1.809
Episode (7000/20001) took 13.252 seconds.
  Mean final reward:        -0.2905
  Mean return:              -3.1668
  Mean final entropy:       0.0012
  Max final entropy:        0.0293
  95 percentile entropy:    0.00497
  Pseudo loss:              -0.67680
  Total gradient norm:      5.69574
  Solved trajectories:      189 / 256
  Avg steps to disentangle: 1.789
Episode (7100/20001) took 12.842 seconds.
  Mean final reward:        -0.3136
  Mean return:              -3.4265
  Mean final entropy:       0.0014
  Max final entropy:        0.0373
  95 percentile entropy:    0.00583
  Pseudo loss:              -0.53419
  Total gradient norm:      5.60569
  Solved trajectories:      181 / 256
  Avg steps to disentangle: 1.773
Episode (7200/20001) took 12.824 seconds.
  Mean final reward:        -0.3438
  Mean return:              -3.4488
  Mean final entropy:       0.0016
  Max final entropy:        0.0318
  95 percentile entropy:    0.00734
  Pseudo loss:              -0.68277
  Total gradient norm:      6.61216
  Solved trajectories:      184 / 256
  Avg steps to disentangle: 1.816
Episode (7300/20001) took 12.920 seconds.
  Mean final reward:        -0.3134
  Mean return:              -3.2419
  Mean final entropy:       0.0014
  Max final entropy:        0.0366
  95 percentile entropy:    0.00768
  Pseudo loss:              -0.57538
  Total gradient norm:      5.70654
  Solved trajectories:      185 / 256
  Avg steps to disentangle: 1.793
Episode (7400/20001) took 12.912 seconds.
  Mean final reward:        -0.2809
  Mean return:              -3.2402
  Mean final entropy:       0.0015
  Max final entropy:        0.0501
  95 percentile entropy:    0.00728
  Pseudo loss:              -0.70102
  Total gradient norm:      6.65736
  Solved trajectories:      195 / 256
  Avg steps to disentangle: 1.832
Episode (7500/20001) took 12.832 seconds.
  Mean final reward:        -0.3038
  Mean return:              -3.2343
  Mean final entropy:       0.0013
  Max final entropy:        0.0341
  95 percentile entropy:    0.00764
  Pseudo loss:              -0.82373
  Total gradient norm:      7.36022
  Solved trajectories:      193 / 256
  Avg steps to disentangle: 1.824
Episode (7600/20001) took 12.962 seconds.
  Mean final reward:        -0.2644
  Mean return:              -3.3236
  Mean final entropy:       0.0012
  Max final entropy:        0.0345
  95 percentile entropy:    0.00496
  Pseudo loss:              -0.61354
  Total gradient norm:      6.78157
  Solved trajectories:      193 / 256
  Avg steps to disentangle: 1.836
Episode (7700/20001) took 12.870 seconds.
  Mean final reward:        -0.1925
  Mean return:              -3.0039
  Mean final entropy:       0.0009
  Max final entropy:        0.0168
  95 percentile entropy:    0.00408
  Pseudo loss:              -1.23218
  Total gradient norm:      5.24129
  Solved trajectories:      202 / 256
  Avg steps to disentangle: 1.844
Episode (7800/20001) took 13.147 seconds.
  Mean final reward:        -0.3239
  Mean return:              -3.5042
  Mean final entropy:       0.0014
  Max final entropy:        0.0214
  95 percentile entropy:    0.00694
  Pseudo loss:              -0.61154
  Total gradient norm:      6.22042
  Solved trajectories:      184 / 256
  Avg steps to disentangle: 1.809
Episode (7900/20001) took 12.804 seconds.
  Mean final reward:        -0.2693
  Mean return:              -3.1216
  Mean final entropy:       0.0012
  Max final entropy:        0.0234
  95 percentile entropy:    0.00454
  Pseudo loss:              -0.96242
  Total gradient norm:      5.91626
  Solved trajectories:      188 / 256
  Avg steps to disentangle: 1.789
Episode (8000/20001) took 13.058 seconds.
  Mean final reward:        -0.3032
  Mean return:              -3.5303
  Mean final entropy:       0.0015
  Max final entropy:        0.0456
  95 percentile entropy:    0.00770
  Pseudo loss:              -0.89177
  Total gradient norm:      7.46352
  Solved trajectories:      191 / 256
  Avg steps to disentangle: 1.883
Episode (8100/20001) took 13.069 seconds.
  Mean final reward:        -0.2560
  Mean return:              -3.1788
  Mean final entropy:       0.0013
  Max final entropy:        0.0815
  95 percentile entropy:    0.00524
  Pseudo loss:              -0.88101
  Total gradient norm:      6.63008
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.867
Episode (8200/20001) took 13.031 seconds.
  Mean final reward:        -0.2695
  Mean return:              -3.1852
  Mean final entropy:       0.0013
  Max final entropy:        0.0623
  95 percentile entropy:    0.00592
  Pseudo loss:              -0.77999
  Total gradient norm:      6.08303
  Solved trajectories:      194 / 256
  Avg steps to disentangle: 1.855
Episode (8300/20001) took 12.874 seconds.
  Mean final reward:        -0.1980
  Mean return:              -3.2949
  Mean final entropy:       0.0009
  Max final entropy:        0.0226
  95 percentile entropy:    0.00403
  Pseudo loss:              -1.26051
  Total gradient norm:      6.32697
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.918
Episode (8400/20001) took 12.716 seconds.
  Mean final reward:        -0.2078
  Mean return:              -2.9376
  Mean final entropy:       0.0008
  Max final entropy:        0.0149
  95 percentile entropy:    0.00406
  Pseudo loss:              -0.83264
  Total gradient norm:      5.73780
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.879
Episode (8500/20001) took 12.934 seconds.
  Mean final reward:        -0.2518
  Mean return:              -3.2584
  Mean final entropy:       0.0011
  Max final entropy:        0.0335
  95 percentile entropy:    0.00529
  Pseudo loss:              -0.68546
  Total gradient norm:      5.67953
  Solved trajectories:      190 / 256
  Avg steps to disentangle: 1.828
Episode (8600/20001) took 12.682 seconds.
  Mean final reward:        -0.3090
  Mean return:              -3.2661
  Mean final entropy:       0.0012
  Max final entropy:        0.0132
  95 percentile entropy:    0.00685
  Pseudo loss:              -0.67944
  Total gradient norm:      7.67848
  Solved trajectories:      182 / 256
  Avg steps to disentangle: 1.777
Episode (8700/20001) took 12.776 seconds.
  Mean final reward:        -0.2295
  Mean return:              -3.0599
  Mean final entropy:       0.0011
  Max final entropy:        0.0545
  95 percentile entropy:    0.00413
  Pseudo loss:              -1.14969
  Total gradient norm:      6.90952
  Solved trajectories:      200 / 256
  Avg steps to disentangle: 1.875
Episode (8800/20001) took 12.702 seconds.
  Mean final reward:        -0.1983
  Mean return:              -3.0408
  Mean final entropy:       0.0008
  Max final entropy:        0.0142
  95 percentile entropy:    0.00407
  Pseudo loss:              -0.78580
  Total gradient norm:      6.26602
  Solved trajectories:      201 / 256
  Avg steps to disentangle: 1.883
Episode (8900/20001) took 12.687 seconds.
  Mean final reward:        -0.2190
  Mean return:              -2.9132
  Mean final entropy:       0.0010
  Max final entropy:        0.0303
  95 percentile entropy:    0.00455
  Pseudo loss:              -0.93866
  Total gradient norm:      6.08621
  Solved trajectories:      197 / 256
  Avg steps to disentangle: 1.832
Episode (9000/20001) took 12.741 seconds.
  Mean final reward:        -0.1931
  Mean return:              -2.9557
  Mean final entropy:       0.0008
  Max final entropy:        0.0232
  95 percentile entropy:    0.00362
  Pseudo loss:              -1.17331
  Total gradient norm:      5.59347
  Solved trajectories:      202 / 256
  Avg steps to disentangle: 1.855
Episode (9100/20001) took 12.675 seconds.
  Mean final reward:        -0.2200
  Mean return:              -3.3118
  Mean final entropy:       0.0010
  Max final entropy:        0.0301
  95 percentile entropy:    0.00522
  Pseudo loss:              -0.84555
  Total gradient norm:      6.54415
  Solved trajectories:      199 / 256
  Avg steps to disentangle: 1.922
Episode (9200/20001) took 12.725 seconds.
  Mean final reward:        -0.2461
  Mean return:              -3.0615
  Mean final entropy:       0.0010
  Max final entropy:        0.0148
  95 percentile entropy:    0.00602
  Pseudo loss:              -1.04939
  Total gradient norm:      6.32043
  Solved trajectories:      197 / 256
  Avg steps to disentangle: 1.848
Episode (9300/20001) took 12.716 seconds.
  Mean final reward:        -0.2304
  Mean return:              -3.1584
  Mean final entropy:       0.0009
  Max final entropy:        0.0129
  95 percentile entropy:    0.00402
  Pseudo loss:              -0.87164
  Total gradient norm:      6.39997
  Solved trajectories:      196 / 256
  Avg steps to disentangle: 1.840
Episode (9400/20001) took 12.662 seconds.
  Mean final reward:        -0.1985
  Mean return:              -3.0122
  Mean final entropy:       0.0009
  Max final entropy:        0.0317
  95 percentile entropy:    0.00422
  Pseudo loss:              -1.15503
  Total gradient norm:      5.96446
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 1.922
Episode (9500/20001) took 12.720 seconds.
  Mean final reward:        -0.2447
  Mean return:              -3.0764
  Mean final entropy:       0.0010
  Max final entropy:        0.0161
  95 percentile entropy:    0.00529
  Pseudo loss:              -0.93179
  Total gradient norm:      6.29566
  Solved trajectories:      190 / 256
  Avg steps to disentangle: 1.816
Episode (9600/20001) took 13.150 seconds.
  Mean final reward:        -0.1995
  Mean return:              -2.7741
  Mean final entropy:       0.0009
  Max final entropy:        0.0234
  95 percentile entropy:    0.00398
  Pseudo loss:              -0.92749
  Total gradient norm:      6.66752
  Solved trajectories:      204 / 256
  Avg steps to disentangle: 1.809
Episode (9700/20001) took 12.651 seconds.
  Mean final reward:        -0.2297
  Mean return:              -2.9725
  Mean final entropy:       0.0009
  Max final entropy:        0.0183
  95 percentile entropy:    0.00438
  Pseudo loss:              -0.94142
  Total gradient norm:      5.45835
  Solved trajectories:      200 / 256
  Avg steps to disentangle: 1.824
Episode (9800/20001) took 12.966 seconds.
  Mean final reward:        -0.3006
  Mean return:              -3.1127
  Mean final entropy:       0.0013
  Max final entropy:        0.0196
  95 percentile entropy:    0.00527
  Pseudo loss:              -0.72206
  Total gradient norm:      6.66300
  Solved trajectories:      187 / 256
  Avg steps to disentangle: 1.801
Episode (9900/20001) took 12.692 seconds.
  Mean final reward:        -0.2258
  Mean return:              -3.0255
  Mean final entropy:       0.0010
  Max final entropy:        0.0324
  95 percentile entropy:    0.00491
  Pseudo loss:              -0.85499
  Total gradient norm:      5.61840
  Solved trajectories:      197 / 256
  Avg steps to disentangle: 1.855
Episode (10000/20001) took 12.739 seconds.
  Mean final reward:        -0.2790
  Mean return:              -3.1132
  Mean final entropy:       0.0013
  Max final entropy:        0.0273
  95 percentile entropy:    0.00525
  Pseudo loss:              -0.78163
  Total gradient norm:      7.27721
  Solved trajectories:      192 / 256
  Avg steps to disentangle: 1.820
Episode (10100/20001) took 12.757 seconds.
  Mean final reward:        -0.2354
  Mean return:              -3.1372
  Mean final entropy:       0.0011
  Max final entropy:        0.0424
  95 percentile entropy:    0.00529
  Pseudo loss:              -0.96576
  Total gradient norm:      6.13374
  Solved trajectories:      198 / 256
  Avg steps to disentangle: 1.902
Episode (10200/20001) took 12.686 seconds.
  Mean final reward:        -0.1852
  Mean return:              -2.7720
  Mean final entropy:       0.0008
  Max final entropy:        0.0312
  95 percentile entropy:    0.00378
  Pseudo loss:              -1.04129
  Total gradient norm:      5.91958
  Solved trajectories:      200 / 256
  Avg steps to disentangle: 1.805
Episode (10300/20001) took 12.709 seconds.
  Mean final reward:        -0.2522
  Mean return:              -3.0227
  Mean final entropy:       0.0012
  Max final entropy:        0.0356
  95 percentile entropy:    0.00540
  Pseudo loss:              -0.66845
  Total gradient norm:      8.01819
  Solved trajectories:      193 / 256
  Avg steps to disentangle: 1.793
Episode (10400/20001) took 12.687 seconds.
  Mean final reward:        -0.2524
  Mean return:              -3.1817
  Mean final entropy:       0.0011
  Max final entropy:        0.0267
  95 percentile entropy:    0.00386
  Pseudo loss:              -0.65809
  Total gradient norm:      6.86077
  Solved trajectories:      192 / 256
  Avg steps to disentangle: 1.836
Episode (10500/20001) took 12.682 seconds.
  Mean final reward:        -0.2026
  Mean return:              -2.9970
  Mean final entropy:       0.0008
  Max final entropy:        0.0124
  95 percentile entropy:    0.00425
  Pseudo loss:              -1.01188
  Total gradient norm:      5.90747
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.867
Episode (10600/20001) took 12.721 seconds.
  Mean final reward:        -0.3532
  Mean return:              -3.1823
  Mean final entropy:       0.0015
  Max final entropy:        0.0203
  95 percentile entropy:    0.00713
  Pseudo loss:              -0.44998
  Total gradient norm:      7.02098
  Solved trajectories:      170 / 256
  Avg steps to disentangle: 1.703
Episode (10700/20001) took 12.783 seconds.
  Mean final reward:        -0.2806
  Mean return:              -3.0396
  Mean final entropy:       0.0012
  Max final entropy:        0.0279
  95 percentile entropy:    0.00526
  Pseudo loss:              -0.88506
  Total gradient norm:      6.18695
  Solved trajectories:      189 / 256
  Avg steps to disentangle: 1.801
Episode (10800/20001) took 12.585 seconds.
  Mean final reward:        -0.2183
  Mean return:              -3.0020
  Mean final entropy:       0.0010
  Max final entropy:        0.0187
  95 percentile entropy:    0.00435
  Pseudo loss:              -0.92526
  Total gradient norm:      5.14698
  Solved trajectories:      194 / 256
  Avg steps to disentangle: 1.832
Episode (10900/20001) took 12.659 seconds.
  Mean final reward:        -0.2167
  Mean return:              -2.9010
  Mean final entropy:       0.0010
  Max final entropy:        0.0173
  95 percentile entropy:    0.00516
  Pseudo loss:              -0.88277
  Total gradient norm:      7.11966
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.797
Episode (11000/20001) took 18.374 seconds.
  Mean final reward:        -0.2325
  Mean return:              -2.7906
  Mean final entropy:       0.0010
  Max final entropy:        0.0191
  95 percentile entropy:    0.00511
  Pseudo loss:              -1.00800
  Total gradient norm:      6.02696
  Solved trajectories:      201 / 256
  Avg steps to disentangle: 1.824
Episode (11100/20001) took 15.135 seconds.
  Mean final reward:        -0.1775
  Mean return:              -2.8282
  Mean final entropy:       0.0008
  Max final entropy:        0.0210
  95 percentile entropy:    0.00403
  Pseudo loss:              -0.94216
  Total gradient norm:      5.83621
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.895
Episode (11200/20001) took 13.755 seconds.
  Mean final reward:        -0.2074
  Mean return:              -2.8488
  Mean final entropy:       0.0008
  Max final entropy:        0.0095
  95 percentile entropy:    0.00430
  Pseudo loss:              -0.97729
  Total gradient norm:      5.60750
  Solved trajectories:      198 / 256
  Avg steps to disentangle: 1.785
Episode (11300/20001) took 14.144 seconds.
  Mean final reward:        -0.2224
  Mean return:              -3.2305
  Mean final entropy:       0.0009
  Max final entropy:        0.0127
  95 percentile entropy:    0.00423
  Pseudo loss:              -0.88704
  Total gradient norm:      6.33389
  Solved trajectories:      200 / 256
  Avg steps to disentangle: 1.906
Episode (11400/20001) took 15.745 seconds.
  Mean final reward:        -0.2924
  Mean return:              -3.0672
  Mean final entropy:       0.0011
  Max final entropy:        0.0103
  95 percentile entropy:    0.00647
  Pseudo loss:              -0.69296
  Total gradient norm:      5.54180
  Solved trajectories:      190 / 256
  Avg steps to disentangle: 1.793
Episode (11500/20001) took 16.838 seconds.
  Mean final reward:        -0.1739
  Mean return:              -3.0545
  Mean final entropy:       0.0008
  Max final entropy:        0.0364
  95 percentile entropy:    0.00354
  Pseudo loss:              -0.76920
  Total gradient norm:      7.19623
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.895
Episode (11600/20001) took 18.491 seconds.
  Mean final reward:        -0.2366
  Mean return:              -3.0142
  Mean final entropy:       0.0010
  Max final entropy:        0.0157
  95 percentile entropy:    0.00403
  Pseudo loss:              -0.81100
  Total gradient norm:      6.41912
  Solved trajectories:      184 / 256
  Avg steps to disentangle: 1.824
Episode (11700/20001) took 15.337 seconds.
  Mean final reward:        -0.1985
  Mean return:              -2.6853
  Mean final entropy:       0.0009
  Max final entropy:        0.0258
  95 percentile entropy:    0.00351
  Pseudo loss:              -1.08033
  Total gradient norm:      6.28253
  Solved trajectories:      211 / 256
  Avg steps to disentangle: 1.852
Episode (11800/20001) took 17.374 seconds.
  Mean final reward:        -0.1916
  Mean return:              -2.7035
  Mean final entropy:       0.0008
  Max final entropy:        0.0148
  95 percentile entropy:    0.00403
  Pseudo loss:              -0.91521
  Total gradient norm:      5.78767
  Solved trajectories:      210 / 256
  Avg steps to disentangle: 1.855
Episode (11900/20001) took 13.350 seconds.
  Mean final reward:        -0.2023
  Mean return:              -2.8947
  Mean final entropy:       0.0009
  Max final entropy:        0.0318
  95 percentile entropy:    0.00349
  Pseudo loss:              -1.14705
  Total gradient norm:      6.18174
  Solved trajectories:      206 / 256
  Avg steps to disentangle: 1.883
Episode (12000/20001) took 16.066 seconds.
  Mean final reward:        -0.2624
  Mean return:              -2.9030
  Mean final entropy:       0.0012
  Max final entropy:        0.0241
  95 percentile entropy:    0.00566
  Pseudo loss:              -0.95664
  Total gradient norm:      6.55528
  Solved trajectories:      196 / 256
  Avg steps to disentangle: 1.812
Episode (12100/20001) took 15.691 seconds.
  Mean final reward:        -0.1816
  Mean return:              -2.8305
  Mean final entropy:       0.0008
  Max final entropy:        0.0165
  95 percentile entropy:    0.00368
  Pseudo loss:              -1.05968
  Total gradient norm:      6.23983
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.840
Episode (12200/20001) took 17.087 seconds.
  Mean final reward:        -0.2081
  Mean return:              -2.8328
  Mean final entropy:       0.0009
  Max final entropy:        0.0125
  95 percentile entropy:    0.00373
  Pseudo loss:              -0.86673
  Total gradient norm:      6.56326
  Solved trajectories:      190 / 256
  Avg steps to disentangle: 1.797
Episode (12300/20001) took 15.018 seconds.
  Mean final reward:        -0.1955
  Mean return:              -2.8651
  Mean final entropy:       0.0009
  Max final entropy:        0.0208
  95 percentile entropy:    0.00321
  Pseudo loss:              -0.70847
  Total gradient norm:      5.61860
  Solved trajectories:      199 / 256
  Avg steps to disentangle: 1.934
Episode (12400/20001) took 15.932 seconds.
  Mean final reward:        -0.2347
  Mean return:              -2.8511
  Mean final entropy:       0.0009
  Max final entropy:        0.0116
  95 percentile entropy:    0.00502
  Pseudo loss:              -0.69932
  Total gradient norm:      6.07035
  Solved trajectories:      202 / 256
  Avg steps to disentangle: 1.871
Episode (12500/20001) took 14.255 seconds.
  Mean final reward:        -0.2600
  Mean return:              -3.0053
  Mean final entropy:       0.0010
  Max final entropy:        0.0231
  95 percentile entropy:    0.00464
  Pseudo loss:              -0.76907
  Total gradient norm:      7.94432
  Solved trajectories:      193 / 256
  Avg steps to disentangle: 1.789
Episode (12600/20001) took 15.201 seconds.
  Mean final reward:        -0.2104
  Mean return:              -2.8578
  Mean final entropy:       0.0009
  Max final entropy:        0.0187
  95 percentile entropy:    0.00427
  Pseudo loss:              -1.02606
  Total gradient norm:      6.03678
  Solved trajectories:      200 / 256
  Avg steps to disentangle: 1.789
Episode (12700/20001) took 16.436 seconds.
  Mean final reward:        -0.2057
  Mean return:              -2.6892
  Mean final entropy:       0.0009
  Max final entropy:        0.0158
  95 percentile entropy:    0.00424
  Pseudo loss:              -0.91751
  Total gradient norm:      5.68905
  Solved trajectories:      201 / 256
  Avg steps to disentangle: 1.809
Episode (12800/20001) took 13.974 seconds.
  Mean final reward:        -0.2287
  Mean return:              -2.8400
  Mean final entropy:       0.0009
  Max final entropy:        0.0137
  95 percentile entropy:    0.00493
  Pseudo loss:              -0.85452
  Total gradient norm:      6.08935
  Solved trajectories:      201 / 256
  Avg steps to disentangle: 1.809
Episode (12900/20001) took 14.678 seconds.
  Mean final reward:        -0.1806
  Mean return:              -2.9673
  Mean final entropy:       0.0008
  Max final entropy:        0.0172
  95 percentile entropy:    0.00349
  Pseudo loss:              -0.81290
  Total gradient norm:      5.75734
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.863
Episode (13000/20001) took 15.921 seconds.
  Mean final reward:        -0.1566
  Mean return:              -2.8462
  Mean final entropy:       0.0007
  Max final entropy:        0.0122
  95 percentile entropy:    0.00308
  Pseudo loss:              -0.71438
  Total gradient norm:      5.57839
  Solved trajectories:      202 / 256
  Avg steps to disentangle: 1.887
Episode (13100/20001) took 15.328 seconds.
  Mean final reward:        -0.2560
  Mean return:              -2.8862
  Mean final entropy:       0.0011
  Max final entropy:        0.0209
  95 percentile entropy:    0.00539
  Pseudo loss:              -0.88444
  Total gradient norm:      6.39171
  Solved trajectories:      199 / 256
  Avg steps to disentangle: 1.781
Episode (13200/20001) took 16.985 seconds.
  Mean final reward:        -0.1154
  Mean return:              -2.7893
  Mean final entropy:       0.0006
  Max final entropy:        0.0144
  95 percentile entropy:    0.00258
  Pseudo loss:              -1.12449
  Total gradient norm:      5.93627
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 1.996
Episode (13300/20001) took 13.561 seconds.
  Mean final reward:        -0.1832
  Mean return:              -2.7071
  Mean final entropy:       0.0009
  Max final entropy:        0.0219
  95 percentile entropy:    0.00431
  Pseudo loss:              -1.09559
  Total gradient norm:      5.88895
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.840
Episode (13400/20001) took 14.260 seconds.
  Mean final reward:        -0.2261
  Mean return:              -2.7932
  Mean final entropy:       0.0010
  Max final entropy:        0.0206
  95 percentile entropy:    0.00469
  Pseudo loss:              -0.96117
  Total gradient norm:      6.08847
  Solved trajectories:      196 / 256
  Avg steps to disentangle: 1.781
Episode (13500/20001) took 16.643 seconds.
  Mean final reward:        -0.1904
  Mean return:              -2.5716
  Mean final entropy:       0.0008
  Max final entropy:        0.0106
  95 percentile entropy:    0.00410
  Pseudo loss:              -0.95573
  Total gradient norm:      6.39449
  Solved trajectories:      205 / 256
  Avg steps to disentangle: 1.766
Episode (13600/20001) took 13.743 seconds.
  Mean final reward:        -0.2032
  Mean return:              -2.8594
  Mean final entropy:       0.0009
  Max final entropy:        0.0212
  95 percentile entropy:    0.00540
  Pseudo loss:              -0.97419
  Total gradient norm:      8.13511
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.875
Episode (13700/20001) took 12.742 seconds.
  Mean final reward:        -0.1800
  Mean return:              -2.7146
  Mean final entropy:       0.0008
  Max final entropy:        0.0215
  95 percentile entropy:    0.00370
  Pseudo loss:              -0.84652
  Total gradient norm:      8.11919
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.793
Episode (13800/20001) took 12.995 seconds.
  Mean final reward:        -0.1885
  Mean return:              -2.7034
  Mean final entropy:       0.0008
  Max final entropy:        0.0148
  95 percentile entropy:    0.00441
  Pseudo loss:              -0.91317
  Total gradient norm:      6.04035
  Solved trajectories:      205 / 256
  Avg steps to disentangle: 1.859
Episode (13900/20001) took 14.292 seconds.
  Mean final reward:        -0.1569
  Mean return:              -2.6539
  Mean final entropy:       0.0007
  Max final entropy:        0.0090
  95 percentile entropy:    0.00319
  Pseudo loss:              -0.90699
  Total gradient norm:      6.34943
  Solved trajectories:      204 / 256
  Avg steps to disentangle: 1.801
Episode (14000/20001) took 13.935 seconds.
  Mean final reward:        -0.1555
  Mean return:              -2.5616
  Mean final entropy:       0.0007
  Max final entropy:        0.0096
  95 percentile entropy:    0.00330
  Pseudo loss:              -0.96980
  Total gradient norm:      6.41049
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 1.836
Episode (14100/20001) took 14.401 seconds.
  Mean final reward:        -0.1631
  Mean return:              -2.7785
  Mean final entropy:       0.0007
  Max final entropy:        0.0092
  95 percentile entropy:    0.00337
  Pseudo loss:              -0.83240
  Total gradient norm:      5.20666
  Solved trajectories:      206 / 256
  Avg steps to disentangle: 1.891
Episode (14200/20001) took 13.853 seconds.
  Mean final reward:        -0.1603
  Mean return:              -2.6678
  Mean final entropy:       0.0007
  Max final entropy:        0.0093
  95 percentile entropy:    0.00303
  Pseudo loss:              -1.18969
  Total gradient norm:      7.29514
  Solved trajectories:      205 / 256
  Avg steps to disentangle: 1.820
Episode (14300/20001) took 16.997 seconds.
  Mean final reward:        -0.1528
  Mean return:              -2.6800
  Mean final entropy:       0.0006
  Max final entropy:        0.0127
  95 percentile entropy:    0.00314
  Pseudo loss:              -0.97613
  Total gradient norm:      7.40549
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.848
Episode (14400/20001) took 14.257 seconds.
  Mean final reward:        -0.1238
  Mean return:              -2.4425
  Mean final entropy:       0.0006
  Max final entropy:        0.0132
  95 percentile entropy:    0.00260
  Pseudo loss:              -0.96172
  Total gradient norm:      6.09801
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 1.902
Episode (14500/20001) took 14.199 seconds.
  Mean final reward:        -0.1882
  Mean return:              -2.6775
  Mean final entropy:       0.0009
  Max final entropy:        0.0204
  95 percentile entropy:    0.00300
  Pseudo loss:              -0.96288
  Total gradient norm:      5.39390
  Solved trajectories:      195 / 256
  Avg steps to disentangle: 1.770
Episode (14600/20001) took 15.974 seconds.
  Mean final reward:        -0.1809
  Mean return:              -2.7647
  Mean final entropy:       0.0008
  Max final entropy:        0.0164
  95 percentile entropy:    0.00402
  Pseudo loss:              -1.11698
  Total gradient norm:      5.84646
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 1.887
Episode (14700/20001) took 20.913 seconds.
  Mean final reward:        -0.1639
  Mean return:              -2.7390
  Mean final entropy:       0.0007
  Max final entropy:        0.0087
  95 percentile entropy:    0.00320
  Pseudo loss:              -0.92645
  Total gradient norm:      6.20843
  Solved trajectories:      201 / 256
  Avg steps to disentangle: 1.816
Episode (14800/20001) took 17.234 seconds.
  Mean final reward:        -0.1529
  Mean return:              -2.6589
  Mean final entropy:       0.0006
  Max final entropy:        0.0079
  95 percentile entropy:    0.00294
  Pseudo loss:              -0.78095
  Total gradient norm:      6.25755
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 1.906
Episode (14900/20001) took 18.072 seconds.
  Mean final reward:        -0.1972
  Mean return:              -2.7119
  Mean final entropy:       0.0009
  Max final entropy:        0.0246
  95 percentile entropy:    0.00496
  Pseudo loss:              -0.88827
  Total gradient norm:      5.84811
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.867
Episode (15000/20001) took 16.001 seconds.
  Mean final reward:        -0.1292
  Mean return:              -2.4405
  Mean final entropy:       0.0006
  Max final entropy:        0.0086
  95 percentile entropy:    0.00308
  Pseudo loss:              -1.02680
  Total gradient norm:      7.45696
  Solved trajectories:      211 / 256
  Avg steps to disentangle: 1.875
Episode (15100/20001) took 15.133 seconds.
  Mean final reward:        -0.1424
  Mean return:              -2.6528
  Mean final entropy:       0.0008
  Max final entropy:        0.0541
  95 percentile entropy:    0.00299
  Pseudo loss:              -0.78496
  Total gradient norm:      5.96075
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 1.855
Episode (15200/20001) took 15.811 seconds.
  Mean final reward:        -0.2226
  Mean return:              -2.8157
  Mean final entropy:       0.0009
  Max final entropy:        0.0247
  95 percentile entropy:    0.00396
  Pseudo loss:              -0.66453
  Total gradient norm:      6.89256
  Solved trajectories:      198 / 256
  Avg steps to disentangle: 1.852
Episode (15300/20001) took 13.555 seconds.
  Mean final reward:        -0.1988
  Mean return:              -2.5086
  Mean final entropy:       0.0008
  Max final entropy:        0.0148
  95 percentile entropy:    0.00420
  Pseudo loss:              -1.15073
  Total gradient norm:      6.08733
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.809
Episode (15400/20001) took 19.417 seconds.
  Mean final reward:        -0.1674
  Mean return:              -2.6697
  Mean final entropy:       0.0007
  Max final entropy:        0.0126
  95 percentile entropy:    0.00348
  Pseudo loss:              -0.75389
  Total gradient norm:      8.78323
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 1.879
Episode (15500/20001) took 15.618 seconds.
  Mean final reward:        -0.1299
  Mean return:              -2.7004
  Mean final entropy:       0.0006
  Max final entropy:        0.0140
  95 percentile entropy:    0.00273
  Pseudo loss:              -0.89911
  Total gradient norm:      6.36132
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 1.879
Episode (15600/20001) took 16.595 seconds.
  Mean final reward:        -0.1635
  Mean return:              -2.5671
  Mean final entropy:       0.0007
  Max final entropy:        0.0136
  95 percentile entropy:    0.00320
  Pseudo loss:              -0.76620
  Total gradient norm:      6.01169
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.805
Episode (15700/20001) took 17.703 seconds.
  Mean final reward:        -0.1835
  Mean return:              -2.6169
  Mean final entropy:       0.0009
  Max final entropy:        0.0271
  95 percentile entropy:    0.00357
  Pseudo loss:              -0.73632
  Total gradient norm:      6.03203
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.820
Episode (15800/20001) took 13.730 seconds.
  Mean final reward:        -0.1596
  Mean return:              -2.5836
  Mean final entropy:       0.0008
  Max final entropy:        0.0161
  95 percentile entropy:    0.00279
  Pseudo loss:              -0.85066
  Total gradient norm:      6.54733
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 1.859
Episode (15900/20001) took 17.144 seconds.
  Mean final reward:        -0.1286
  Mean return:              -2.5600
  Mean final entropy:       0.0005
  Max final entropy:        0.0078
  95 percentile entropy:    0.00295
  Pseudo loss:              -0.91533
  Total gradient norm:      5.67887
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 1.875
Episode (16000/20001) took 16.849 seconds.
  Mean final reward:        -0.1019
  Mean return:              -2.7386
  Mean final entropy:       0.0005
  Max final entropy:        0.0075
  95 percentile entropy:    0.00217
  Pseudo loss:              -1.08089
  Total gradient norm:      6.45584
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 1.914
Episode (16100/20001) took 17.609 seconds.
  Mean final reward:        -0.2019
  Mean return:              -2.7568
  Mean final entropy:       0.0008
  Max final entropy:        0.0101
  95 percentile entropy:    0.00372
  Pseudo loss:              -0.82942
  Total gradient norm:      6.56273
  Solved trajectories:      198 / 256
  Avg steps to disentangle: 1.832
Episode (16200/20001) took 16.681 seconds.
  Mean final reward:        -0.1815
  Mean return:              -2.6914
  Mean final entropy:       0.0008
  Max final entropy:        0.0167
  95 percentile entropy:    0.00317
  Pseudo loss:              -1.11467
  Total gradient norm:      6.13120
  Solved trajectories:      205 / 256
  Avg steps to disentangle: 1.848
Episode (16300/20001) took 18.684 seconds.
  Mean final reward:        -0.1414
  Mean return:              -2.5622
  Mean final entropy:       0.0006
  Max final entropy:        0.0075
  95 percentile entropy:    0.00299
  Pseudo loss:              -0.92521
  Total gradient norm:      6.41734
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 1.895
Episode (16400/20001) took 16.936 seconds.
  Mean final reward:        -0.1263
  Mean return:              -2.4842
  Mean final entropy:       0.0006
  Max final entropy:        0.0108
  95 percentile entropy:    0.00256
  Pseudo loss:              -1.10869
  Total gradient norm:      5.38360
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 1.895
Episode (16500/20001) took 18.250 seconds.
  Mean final reward:        -0.1499
  Mean return:              -2.6473
  Mean final entropy:       0.0006
  Max final entropy:        0.0081
  95 percentile entropy:    0.00312
  Pseudo loss:              -1.08987
  Total gradient norm:      5.92801
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 1.801
Episode (16600/20001) took 12.679 seconds.
  Mean final reward:        -0.1665
  Mean return:              -2.5851
  Mean final entropy:       0.0007
  Max final entropy:        0.0170
  95 percentile entropy:    0.00329
  Pseudo loss:              -1.04736
  Total gradient norm:      6.78301
  Solved trajectories:      210 / 256
  Avg steps to disentangle: 1.855
Episode (16700/20001) took 12.658 seconds.
  Mean final reward:        -0.1558
  Mean return:              -2.4874
  Mean final entropy:       0.0007
  Max final entropy:        0.0095
  95 percentile entropy:    0.00336
  Pseudo loss:              -0.76306
  Total gradient norm:      6.88727
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 1.844
Episode (16800/20001) took 14.429 seconds.
  Mean final reward:        -0.1771
  Mean return:              -2.7278
  Mean final entropy:       0.0008
  Max final entropy:        0.0163
  95 percentile entropy:    0.00364
  Pseudo loss:              -0.88335
  Total gradient norm:      5.73337
  Solved trajectories:      201 / 256
  Avg steps to disentangle: 1.832
Episode (16900/20001) took 14.019 seconds.
  Mean final reward:        -0.1721
  Mean return:              -2.6632
  Mean final entropy:       0.0007
  Max final entropy:        0.0138
  95 percentile entropy:    0.00343
  Pseudo loss:              -0.76816
  Total gradient norm:      7.28450
  Solved trajectories:      199 / 256
  Avg steps to disentangle: 1.832
Episode (17000/20001) took 12.669 seconds.
  Mean final reward:        -0.1562
  Mean return:              -2.5142
  Mean final entropy:       0.0007
  Max final entropy:        0.0145
  95 percentile entropy:    0.00276
  Pseudo loss:              -0.86511
  Total gradient norm:      5.24645
  Solved trajectories:      210 / 256
  Avg steps to disentangle: 1.820
Episode (17100/20001) took 13.518 seconds.
  Mean final reward:        -0.1165
  Mean return:              -2.4920
  Mean final entropy:       0.0005
  Max final entropy:        0.0115
  95 percentile entropy:    0.00249
  Pseudo loss:              -1.04653
  Total gradient norm:      5.50418
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 1.898
Episode (17200/20001) took 12.657 seconds.
  Mean final reward:        -0.1405
  Mean return:              -2.6054
  Mean final entropy:       0.0006
  Max final entropy:        0.0104
  95 percentile entropy:    0.00291
  Pseudo loss:              -1.19665
  Total gradient norm:      7.28056
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 1.891
Episode (17300/20001) took 12.829 seconds.
  Mean final reward:        -0.1546
  Mean return:              -2.5110
  Mean final entropy:       0.0006
  Max final entropy:        0.0103
  95 percentile entropy:    0.00372
  Pseudo loss:              -0.95120
  Total gradient norm:      6.07386
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.758
Episode (17400/20001) took 12.599 seconds.
  Mean final reward:        -0.1902
  Mean return:              -2.5903
  Mean final entropy:       0.0008
  Max final entropy:        0.0186
  95 percentile entropy:    0.00352
  Pseudo loss:              -0.76493
  Total gradient norm:      6.53055
  Solved trajectories:      197 / 256
  Avg steps to disentangle: 1.789
Episode (17500/20001) took 12.717 seconds.
  Mean final reward:        -0.0988
  Mean return:              -2.5347
  Mean final entropy:       0.0005
  Max final entropy:        0.0110
  95 percentile entropy:    0.00253
  Pseudo loss:              -0.96746
  Total gradient norm:      5.88921
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 1.938
Episode (17600/20001) took 13.876 seconds.
  Mean final reward:        -0.1785
  Mean return:              -2.7083
  Mean final entropy:       0.0007
  Max final entropy:        0.0092
  95 percentile entropy:    0.00381
  Pseudo loss:              -0.93512
  Total gradient norm:      5.62319
  Solved trajectories:      196 / 256
  Avg steps to disentangle: 1.781
Episode (17700/20001) took 12.740 seconds.
  Mean final reward:        -0.1337
  Mean return:              -2.6593
  Mean final entropy:       0.0006
  Max final entropy:        0.0156
  95 percentile entropy:    0.00264
  Pseudo loss:              -0.95024
  Total gradient norm:      6.31781
  Solved trajectories:      204 / 256
  Avg steps to disentangle: 1.836
Episode (17800/20001) took 12.629 seconds.
  Mean final reward:        -0.1481
  Mean return:              -2.4838
  Mean final entropy:       0.0006
  Max final entropy:        0.0066
  95 percentile entropy:    0.00342
  Pseudo loss:              -0.95986
  Total gradient norm:      5.07236
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 1.832
Episode (17900/20001) took 13.707 seconds.
  Mean final reward:        -0.1249
  Mean return:              -2.6556
  Mean final entropy:       0.0006
  Max final entropy:        0.0100
  95 percentile entropy:    0.00278
  Pseudo loss:              -0.99251
  Total gradient norm:      6.76145
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 1.875
Episode (18000/20001) took 13.181 seconds.
  Mean final reward:        -0.1098
  Mean return:              -2.3681
  Mean final entropy:       0.0005
  Max final entropy:        0.0108
  95 percentile entropy:    0.00228
  Pseudo loss:              -1.00451
  Total gradient norm:      5.24957
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 1.871
Episode (18100/20001) took 12.682 seconds.
  Mean final reward:        -0.1775
  Mean return:              -2.6854
  Mean final entropy:       0.0007
  Max final entropy:        0.0088
  95 percentile entropy:    0.00355
  Pseudo loss:              -1.11609
  Total gradient norm:      5.33062
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 1.910
Episode (18200/20001) took 12.702 seconds.
  Mean final reward:        -0.1462
  Mean return:              -2.6422
  Mean final entropy:       0.0007
  Max final entropy:        0.0201
  95 percentile entropy:    0.00333
  Pseudo loss:              -1.07125
  Total gradient norm:      6.69608
  Solved trajectories:      210 / 256
  Avg steps to disentangle: 1.891
Episode (18300/20001) took 13.416 seconds.
  Mean final reward:        -0.1714
  Mean return:              -2.7047
  Mean final entropy:       0.0007
  Max final entropy:        0.0102
  95 percentile entropy:    0.00345
  Pseudo loss:              -0.96738
  Total gradient norm:      5.44084
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.848
Episode (18400/20001) took 12.627 seconds.
  Mean final reward:        -0.1759
  Mean return:              -2.6305
  Mean final entropy:       0.0007
  Max final entropy:        0.0109
  95 percentile entropy:    0.00364
  Pseudo loss:              -0.93919
  Total gradient norm:      6.14627
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.801
Episode (18500/20001) took 12.670 seconds.
  Mean final reward:        -0.1559
  Mean return:              -2.5673
  Mean final entropy:       0.0006
  Max final entropy:        0.0100
  95 percentile entropy:    0.00331
  Pseudo loss:              -1.07649
  Total gradient norm:      7.08083
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 1.895
Episode (18600/20001) took 12.560 seconds.
  Mean final reward:        -0.1554
  Mean return:              -2.4905
  Mean final entropy:       0.0007
  Max final entropy:        0.0153
  95 percentile entropy:    0.00365
  Pseudo loss:              -1.03079
  Total gradient norm:      6.34614
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 1.852
Episode (18700/20001) took 12.732 seconds.
  Mean final reward:        -0.1343
  Mean return:              -2.4924
  Mean final entropy:       0.0006
  Max final entropy:        0.0100
  95 percentile entropy:    0.00294
  Pseudo loss:              -1.07876
  Total gradient norm:      6.39711
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 1.891
Episode (18800/20001) took 12.615 seconds.
  Mean final reward:        -0.1659
  Mean return:              -2.5260
  Mean final entropy:       0.0007
  Max final entropy:        0.0165
  95 percentile entropy:    0.00355
  Pseudo loss:              -0.86292
  Total gradient norm:      6.53685
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 1.840
Episode (18900/20001) took 12.655 seconds.
  Mean final reward:        -0.1595
  Mean return:              -2.5198
  Mean final entropy:       0.0007
  Max final entropy:        0.0083
  95 percentile entropy:    0.00405
  Pseudo loss:              -0.75120
  Total gradient norm:      5.51110
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.859
Episode (19000/20001) took 12.644 seconds.
  Mean final reward:        -0.1338
  Mean return:              -2.4483
  Mean final entropy:       0.0006
  Max final entropy:        0.0072
  95 percentile entropy:    0.00302
  Pseudo loss:              -0.64194
  Total gradient norm:      5.17994
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 1.805
Episode (19100/20001) took 12.559 seconds.
  Mean final reward:        -0.1274
  Mean return:              -2.5435
  Mean final entropy:       0.0005
  Max final entropy:        0.0058
  95 percentile entropy:    0.00250
  Pseudo loss:              -0.98963
  Total gradient norm:      5.11339
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 1.840
Episode (19200/20001) took 12.606 seconds.
  Mean final reward:        -0.1416
  Mean return:              -2.4618
  Mean final entropy:       0.0007
  Max final entropy:        0.0250
  95 percentile entropy:    0.00291
  Pseudo loss:              -0.77755
  Total gradient norm:      5.94268
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 1.816
Episode (19300/20001) took 14.137 seconds.
  Mean final reward:        -0.1650
  Mean return:              -2.6055
  Mean final entropy:       0.0007
  Max final entropy:        0.0090
  95 percentile entropy:    0.00330
  Pseudo loss:              -0.65455
  Total gradient norm:      6.61374
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 1.879
Episode (19400/20001) took 14.026 seconds.
  Mean final reward:        -0.1357
  Mean return:              -2.5104
  Mean final entropy:       0.0006
  Max final entropy:        0.0060
  95 percentile entropy:    0.00297
  Pseudo loss:              -0.90932
  Total gradient norm:      6.28255
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 1.859
Episode (19500/20001) took 13.224 seconds.
  Mean final reward:        -0.1390
  Mean return:              -2.4504
  Mean final entropy:       0.0006
  Max final entropy:        0.0099
  95 percentile entropy:    0.00278
  Pseudo loss:              -0.84314
  Total gradient norm:      6.66671
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 1.883
Episode (19600/20001) took 12.710 seconds.
  Mean final reward:        -0.1253
  Mean return:              -2.5111
  Mean final entropy:       0.0006
  Max final entropy:        0.0104
  95 percentile entropy:    0.00250
  Pseudo loss:              -0.68511
  Total gradient norm:      6.09352
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 1.895
Episode (19700/20001) took 13.495 seconds.
  Mean final reward:        -0.1623
  Mean return:              -2.5768
  Mean final entropy:       0.0007
  Max final entropy:        0.0078
  95 percentile entropy:    0.00369
  Pseudo loss:              -0.95266
  Total gradient norm:      5.65425
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 1.918
Episode (19800/20001) took 12.567 seconds.
  Mean final reward:        -0.1497
  Mean return:              -2.5457
  Mean final entropy:       0.0007
  Max final entropy:        0.0096
  95 percentile entropy:    0.00362
  Pseudo loss:              -1.16254
  Total gradient norm:      8.25667
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 1.863
Episode (19900/20001) took 12.702 seconds.
  Mean final reward:        -0.1045
  Mean return:              -2.4007
  Mean final entropy:       0.0005
  Max final entropy:        0.0104
  95 percentile entropy:    0.00272
  Pseudo loss:              -0.98038
  Total gradient norm:      6.67772
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 1.879
Episode (20000/20001) took 12.636 seconds.
  Mean final reward:        -0.1356
  Mean return:              -2.4864
  Mean final entropy:       0.0006
  Max final entropy:        0.0100
  95 percentile entropy:    0.00259
  Pseudo loss:              -1.04176
  Total gradient norm:      8.08162
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 1.832
