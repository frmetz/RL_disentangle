##############################
Training parameters:
  Number of trajectories:   256
  Number of episodes:       5000
  Learning rate:            0.0001
  Weight regularization:    0.0
  Policy hidden dimensions: [2048, 1024, 256]
  Policy dropout rate:      0.0

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 0.0001
    weight_decay: 0.0
)

Episode (10/5000) took 8.299 seconds.
  Mean final reward:        -1.4837
  Mean return:              -8.5199
  Mean final entropy:       0.0195
  Max final entropy:        0.3255
  Pseudo loss:              -0.45246
  Solved trajectories:      111 / 256
  Avg steps to disentangle: 1.625
Episode (20/5000) took 8.308 seconds.
  Mean final reward:        -1.1900
  Mean return:              -7.4448
  Mean final entropy:       0.0178
  Max final entropy:        0.4198
  Pseudo loss:              -1.64964
  Solved trajectories:      134 / 256
  Avg steps to disentangle: 1.719
Episode (30/5000) took 8.352 seconds.
  Mean final reward:        -0.7549
  Mean return:              -6.3411
  Mean final entropy:       0.0096
  Max final entropy:        0.3643
  Pseudo loss:              -0.93340
  Solved trajectories:      168 / 256
  Avg steps to disentangle: 1.898
Episode (40/5000) took 8.251 seconds.
  Mean final reward:        -0.5579
  Mean return:              -5.9003
  Mean final entropy:       0.0065
  Max final entropy:        0.1731
  Pseudo loss:              -0.21704
  Solved trajectories:      193 / 256
  Avg steps to disentangle: 2.059
Episode (50/5000) took 8.353 seconds.
  Mean final reward:        -0.7322
  Mean return:              -6.2987
  Mean final entropy:       0.0077
  Max final entropy:        0.2200
  Pseudo loss:              0.04532
  Solved trajectories:      177 / 256
  Avg steps to disentangle: 1.957
Episode (60/5000) took 8.274 seconds.
  Mean final reward:        -0.7975
  Mean return:              -6.6758
  Mean final entropy:       0.0083
  Max final entropy:        0.2217
  Pseudo loss:              -0.03928
  Solved trajectories:      172 / 256
  Avg steps to disentangle: 1.934
Episode (70/5000) took 8.258 seconds.
  Mean final reward:        -0.5604
  Mean return:              -5.9297
  Mean final entropy:       0.0048
  Max final entropy:        0.2259
  Pseudo loss:              -0.52976
  Solved trajectories:      185 / 256
  Avg steps to disentangle: 1.996
Episode (80/5000) took 8.268 seconds.
  Mean final reward:        -0.6213
  Mean return:              -6.1597
  Mean final entropy:       0.0052
  Max final entropy:        0.1415
  Pseudo loss:              -0.13563
  Solved trajectories:      175 / 256
  Avg steps to disentangle: 1.941
Episode (90/5000) took 8.304 seconds.
  Mean final reward:        -0.7298
  Mean return:              -6.4049
  Mean final entropy:       0.0064
  Max final entropy:        0.1447
  Pseudo loss:              -0.16724
  Solved trajectories:      171 / 256
  Avg steps to disentangle: 1.934
Episode (100/5000) took 8.331 seconds.
  Mean final reward:        -0.6861
  Mean return:              -6.2292
  Mean final entropy:       0.0064
  Max final entropy:        0.1304
  Pseudo loss:              -0.20039
  Solved trajectories:      167 / 256
  Avg steps to disentangle: 1.906
Episode (110/5000) took 8.329 seconds.
  Mean final reward:        -0.6851
  Mean return:              -6.2483
  Mean final entropy:       0.0069
  Max final entropy:        0.1945
  Pseudo loss:              -0.27962
  Solved trajectories:      176 / 256
  Avg steps to disentangle: 1.961
Episode (120/5000) took 8.260 seconds.
  Mean final reward:        -0.4506
  Mean return:              -6.0082
  Mean final entropy:       0.0043
  Max final entropy:        0.1828
  Pseudo loss:              -0.18847
  Solved trajectories:      194 / 256
  Avg steps to disentangle: 2.082
Episode (130/5000) took 8.372 seconds.
  Mean final reward:        -0.7298
  Mean return:              -6.3229
  Mean final entropy:       0.0087
  Max final entropy:        0.1505
  Pseudo loss:              -0.23878
  Solved trajectories:      176 / 256
  Avg steps to disentangle: 1.926
Episode (140/5000) took 8.362 seconds.
  Mean final reward:        -0.6713
  Mean return:              -6.3641
  Mean final entropy:       0.0070
  Max final entropy:        0.1449
  Pseudo loss:              0.00634
  Solved trajectories:      175 / 256
  Avg steps to disentangle: 1.957
Episode (150/5000) took 8.285 seconds.
  Mean final reward:        -0.4132
  Mean return:              -5.5406
  Mean final entropy:       0.0028
  Max final entropy:        0.0779
  Pseudo loss:              -0.14294
  Solved trajectories:      203 / 256
  Avg steps to disentangle: 2.051
Episode (160/5000) took 8.308 seconds.
  Mean final reward:        -0.5497
  Mean return:              -5.7325
  Mean final entropy:       0.0046
  Max final entropy:        0.1209
  Pseudo loss:              -0.26643
  Solved trajectories:      187 / 256
  Avg steps to disentangle: 1.961
Episode (170/5000) took 8.341 seconds.
  Mean final reward:        -0.5429
  Mean return:              -5.9586
  Mean final entropy:       0.0045
  Max final entropy:        0.1562
  Pseudo loss:              -0.68773
  Solved trajectories:      189 / 256
  Avg steps to disentangle: 2.023
Episode (180/5000) took 8.404 seconds.
  Mean final reward:        -0.4740
  Mean return:              -5.5542
  Mean final entropy:       0.0047
  Max final entropy:        0.1239
  Pseudo loss:              -0.47221
  Solved trajectories:      195 / 256
  Avg steps to disentangle: 2.000
Episode (190/5000) took 8.284 seconds.
  Mean final reward:        -0.5714
  Mean return:              -5.9409
  Mean final entropy:       0.0055
  Max final entropy:        0.1799
  Pseudo loss:              -0.57502
  Solved trajectories:      184 / 256
  Avg steps to disentangle: 2.008
Episode (200/5000) took 8.301 seconds.
  Mean final reward:        -0.4512
  Mean return:              -6.0751
  Mean final entropy:       0.0033
  Max final entropy:        0.1141
  Pseudo loss:              -0.51066
  Solved trajectories:      197 / 256
  Avg steps to disentangle: 2.102
Episode (210/5000) took 8.389 seconds.
  Mean final reward:        -0.4703
  Mean return:              -5.8271
  Mean final entropy:       0.0039
  Max final entropy:        0.0880
  Pseudo loss:              -0.76264
  Solved trajectories:      193 / 256
  Avg steps to disentangle: 2.016
Episode (220/5000) took 8.272 seconds.
  Mean final reward:        -0.4116
  Mean return:              -5.8367
  Mean final entropy:       0.0042
  Max final entropy:        0.1674
  Pseudo loss:              -0.61930
  Solved trajectories:      200 / 256
  Avg steps to disentangle: 2.121
Episode (230/5000) took 8.266 seconds.
  Mean final reward:        -0.3902
  Mean return:              -5.5528
  Mean final entropy:       0.0029
  Max final entropy:        0.0648
  Pseudo loss:              -0.52295
  Solved trajectories:      202 / 256
  Avg steps to disentangle: 2.059
Episode (240/5000) took 8.340 seconds.
  Mean final reward:        -0.3503
  Mean return:              -5.4969
  Mean final entropy:       0.0027
  Max final entropy:        0.1278
  Pseudo loss:              -0.52528
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 2.102
Episode (250/5000) took 8.305 seconds.
  Mean final reward:        -0.4238
  Mean return:              -5.7237
  Mean final entropy:       0.0033
  Max final entropy:        0.1542
  Pseudo loss:              -0.65545
  Solved trajectories:      198 / 256
  Avg steps to disentangle: 2.008
Episode (260/5000) took 8.264 seconds.
  Mean final reward:        -0.3982
  Mean return:              -5.6772
  Mean final entropy:       0.0044
  Max final entropy:        0.1781
  Pseudo loss:              -0.59511
  Solved trajectories:      208 / 256
  Avg steps to disentangle: 2.148
Episode (270/5000) took 8.338 seconds.
  Mean final reward:        -0.3038
  Mean return:              -5.3288
  Mean final entropy:       0.0025
  Max final entropy:        0.0889
  Pseudo loss:              -0.33560
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 2.113
Episode (280/5000) took 8.326 seconds.
  Mean final reward:        -0.3564
  Mean return:              -5.5414
  Mean final entropy:       0.0035
  Max final entropy:        0.1833
  Pseudo loss:              -0.52274
  Solved trajectories:      210 / 256
  Avg steps to disentangle: 2.141
Episode (290/5000) took 8.279 seconds.
  Mean final reward:        -0.3351
  Mean return:              -5.5129
  Mean final entropy:       0.0032
  Max final entropy:        0.1785
  Pseudo loss:              -0.64508
  Solved trajectories:      211 / 256
  Avg steps to disentangle: 2.145
Episode (300/5000) took 8.278 seconds.
  Mean final reward:        -0.3463
  Mean return:              -5.5286
  Mean final entropy:       0.0034
  Max final entropy:        0.1564
  Pseudo loss:              -0.58368
  Solved trajectories:      205 / 256
  Avg steps to disentangle: 2.078
Episode (310/5000) took 8.267 seconds.
  Mean final reward:        -0.2929
  Mean return:              -5.3237
  Mean final entropy:       0.0019
  Max final entropy:        0.0612
  Pseudo loss:              -0.45171
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 2.090
Episode (320/5000) took 8.291 seconds.
  Mean final reward:        -0.2902
  Mean return:              -5.3657
  Mean final entropy:       0.0016
  Max final entropy:        0.0617
  Pseudo loss:              -0.50646
  Solved trajectories:      207 / 256
  Avg steps to disentangle: 2.086
Episode (330/5000) took 8.373 seconds.
  Mean final reward:        -0.2366
  Mean return:              -5.1101
  Mean final entropy:       0.0019
  Max final entropy:        0.1136
  Pseudo loss:              -0.35990
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.156
Episode (340/5000) took 8.330 seconds.
  Mean final reward:        -0.2611
  Mean return:              -5.1434
  Mean final entropy:       0.0018
  Max final entropy:        0.0646
  Pseudo loss:              -0.47999
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.055
Episode (350/5000) took 8.283 seconds.
  Mean final reward:        -0.2251
  Mean return:              -5.2238
  Mean final entropy:       0.0023
  Max final entropy:        0.2364
  Pseudo loss:              -0.51565
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.172
Episode (360/5000) took 8.314 seconds.
  Mean final reward:        -0.2662
  Mean return:              -5.3339
  Mean final entropy:       0.0023
  Max final entropy:        0.1429
  Pseudo loss:              -0.48114
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.105
Episode (370/5000) took 8.381 seconds.
  Mean final reward:        -0.3008
  Mean return:              -5.4751
  Mean final entropy:       0.0021
  Max final entropy:        0.1020
  Pseudo loss:              -0.56420
  Solved trajectories:      209 / 256
  Avg steps to disentangle: 2.113
Episode (380/5000) took 8.314 seconds.
  Mean final reward:        -0.3096
  Mean return:              -5.3793
  Mean final entropy:       0.0024
  Max final entropy:        0.1310
  Pseudo loss:              -0.59575
  Solved trajectories:      211 / 256
  Avg steps to disentangle: 2.102
Episode (390/5000) took 8.402 seconds.
  Mean final reward:        -0.2808
  Mean return:              -5.2775
  Mean final entropy:       0.0021
  Max final entropy:        0.0710
  Pseudo loss:              -0.41552
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.113
Episode (400/5000) took 8.386 seconds.
  Mean final reward:        -0.1707
  Mean return:              -4.7083
  Mean final entropy:       0.0010
  Max final entropy:        0.0427
  Pseudo loss:              -0.42159
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.133
Episode (410/5000) took 8.388 seconds.
  Mean final reward:        -0.2124
  Mean return:              -5.2518
  Mean final entropy:       0.0012
  Max final entropy:        0.0580
  Pseudo loss:              -0.28852
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.117
Episode (420/5000) took 8.404 seconds.
  Mean final reward:        -0.3397
  Mean return:              -5.3502
  Mean final entropy:       0.0027
  Max final entropy:        0.0770
  Pseudo loss:              -0.58662
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 2.102
Episode (430/5000) took 8.399 seconds.
  Mean final reward:        -0.2096
  Mean return:              -4.8414
  Mean final entropy:       0.0013
  Max final entropy:        0.0716
  Pseudo loss:              -0.32062
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.121
Episode (440/5000) took 8.310 seconds.
  Mean final reward:        -0.2116
  Mean return:              -5.2840
  Mean final entropy:       0.0015
  Max final entropy:        0.1112
  Pseudo loss:              -0.37224
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.133
Episode (450/5000) took 8.303 seconds.
  Mean final reward:        -0.2414
  Mean return:              -5.0239
  Mean final entropy:       0.0023
  Max final entropy:        0.1553
  Pseudo loss:              -0.25953
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.113
Episode (460/5000) took 8.322 seconds.
  Mean final reward:        -0.1865
  Mean return:              -4.9115
  Mean final entropy:       0.0020
  Max final entropy:        0.1470
  Pseudo loss:              -0.57787
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.195
Episode (470/5000) took 8.337 seconds.
  Mean final reward:        -0.1480
  Mean return:              -5.0495
  Mean final entropy:       0.0012
  Max final entropy:        0.0773
  Pseudo loss:              -0.39166
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.238
Episode (480/5000) took 8.343 seconds.
  Mean final reward:        -0.1716
  Mean return:              -5.1276
  Mean final entropy:       0.0017
  Max final entropy:        0.2240
  Pseudo loss:              -0.40775
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.203
Episode (490/5000) took 8.381 seconds.
  Mean final reward:        -0.2420
  Mean return:              -5.0967
  Mean final entropy:       0.0021
  Max final entropy:        0.0862
  Pseudo loss:              -0.56248
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.109
Episode (500/5000) took 8.293 seconds.
  Mean final reward:        -0.1916
  Mean return:              -5.1187
  Mean final entropy:       0.0009
  Max final entropy:        0.0501
  Pseudo loss:              -0.28949
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.133
Episode (510/5000) took 8.350 seconds.
  Mean final reward:        -0.1389
  Mean return:              -4.8276
  Mean final entropy:       0.0008
  Max final entropy:        0.0643
  Pseudo loss:              -0.23611
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.180
Episode (520/5000) took 8.279 seconds.
  Mean final reward:        -0.2231
  Mean return:              -5.1836
  Mean final entropy:       0.0016
  Max final entropy:        0.0827
  Pseudo loss:              -0.26214
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.148
Episode (530/5000) took 8.327 seconds.
  Mean final reward:        -0.2379
  Mean return:              -5.1571
  Mean final entropy:       0.0013
  Max final entropy:        0.0753
  Pseudo loss:              -0.47576
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.125
Episode (540/5000) took 8.286 seconds.
  Mean final reward:        -0.1510
  Mean return:              -5.0991
  Mean final entropy:       0.0007
  Max final entropy:        0.0434
  Pseudo loss:              -0.14988
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.215
Episode (550/5000) took 8.308 seconds.
  Mean final reward:        -0.2190
  Mean return:              -5.1776
  Mean final entropy:       0.0014
  Max final entropy:        0.0813
  Pseudo loss:              -0.39633
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.105
Episode (560/5000) took 8.332 seconds.
  Mean final reward:        -0.2439
  Mean return:              -5.0867
  Mean final entropy:       0.0013
  Max final entropy:        0.0404
  Pseudo loss:              -0.48665
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.117
Episode (570/5000) took 8.317 seconds.
  Mean final reward:        -0.2078
  Mean return:              -5.1523
  Mean final entropy:       0.0014
  Max final entropy:        0.0765
  Pseudo loss:              -0.64901
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.156
Episode (580/5000) took 8.328 seconds.
  Mean final reward:        -0.1374
  Mean return:              -4.9148
  Mean final entropy:       0.0008
  Max final entropy:        0.0330
  Pseudo loss:              -0.15715
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.219
Episode (590/5000) took 8.338 seconds.
  Mean final reward:        -0.1174
  Mean return:              -4.9000
  Mean final entropy:       0.0007
  Max final entropy:        0.0680
  Pseudo loss:              -0.27424
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.230
Episode (600/5000) took 8.380 seconds.
  Mean final reward:        -0.1808
  Mean return:              -5.0889
  Mean final entropy:       0.0011
  Max final entropy:        0.0746
  Pseudo loss:              -0.32016
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.141
Episode (610/5000) took 8.295 seconds.
  Mean final reward:        -0.1268
  Mean return:              -4.7851
  Mean final entropy:       0.0007
  Max final entropy:        0.0453
  Pseudo loss:              -0.27068
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.199
Episode (620/5000) took 8.423 seconds.
  Mean final reward:        -0.2357
  Mean return:              -5.2401
  Mean final entropy:       0.0015
  Max final entropy:        0.0643
  Pseudo loss:              -0.55334
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.148
Episode (630/5000) took 8.343 seconds.
  Mean final reward:        -0.2082
  Mean return:              -5.1002
  Mean final entropy:       0.0018
  Max final entropy:        0.1248
  Pseudo loss:              -0.31281
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.156
Episode (640/5000) took 8.440 seconds.
  Mean final reward:        -0.1741
  Mean return:              -4.9067
  Mean final entropy:       0.0008
  Max final entropy:        0.0317
  Pseudo loss:              -0.42719
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.121
Episode (650/5000) took 8.477 seconds.
  Mean final reward:        -0.1397
  Mean return:              -5.2067
  Mean final entropy:       0.0007
  Max final entropy:        0.0201
  Pseudo loss:              -0.23094
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.234
Episode (660/5000) took 8.299 seconds.
  Mean final reward:        -0.2110
  Mean return:              -5.1154
  Mean final entropy:       0.0012
  Max final entropy:        0.0776
  Pseudo loss:              -0.34885
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.113
Episode (670/5000) took 8.360 seconds.
  Mean final reward:        -0.1831
  Mean return:              -5.0175
  Mean final entropy:       0.0015
  Max final entropy:        0.1361
  Pseudo loss:              -0.33607
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.188
Episode (680/5000) took 8.434 seconds.
  Mean final reward:        -0.1383
  Mean return:              -4.7903
  Mean final entropy:       0.0008
  Max final entropy:        0.0290
  Pseudo loss:              -0.24361
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.184
Episode (690/5000) took 8.292 seconds.
  Mean final reward:        -0.1950
  Mean return:              -5.1532
  Mean final entropy:       0.0013
  Max final entropy:        0.0713
  Pseudo loss:              -0.48700
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.168
Episode (700/5000) took 8.346 seconds.
  Mean final reward:        -0.2028
  Mean return:              -5.0841
  Mean final entropy:       0.0018
  Max final entropy:        0.2008
  Pseudo loss:              -0.25444
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.148
Episode (710/5000) took 8.289 seconds.
  Mean final reward:        -0.1763
  Mean return:              -5.0535
  Mean final entropy:       0.0008
  Max final entropy:        0.0447
  Pseudo loss:              -0.25653
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.148
Episode (720/5000) took 8.310 seconds.
  Mean final reward:        -0.2088
  Mean return:              -4.9730
  Mean final entropy:       0.0018
  Max final entropy:        0.0745
  Pseudo loss:              -0.42823
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.180
Episode (730/5000) took 8.279 seconds.
  Mean final reward:        -0.1734
  Mean return:              -4.8776
  Mean final entropy:       0.0008
  Max final entropy:        0.0338
  Pseudo loss:              -0.24390
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.094
Episode (740/5000) took 8.337 seconds.
  Mean final reward:        -0.1678
  Mean return:              -5.0564
  Mean final entropy:       0.0011
  Max final entropy:        0.0964
  Pseudo loss:              -0.23456
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.168
Episode (750/5000) took 8.451 seconds.
  Mean final reward:        -0.2016
  Mean return:              -4.9596
  Mean final entropy:       0.0009
  Max final entropy:        0.0193
  Pseudo loss:              -0.27643
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.070
Episode (760/5000) took 8.331 seconds.
  Mean final reward:        -0.1470
  Mean return:              -5.1402
  Mean final entropy:       0.0007
  Max final entropy:        0.0321
  Pseudo loss:              -0.32069
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.168
Episode (770/5000) took 8.378 seconds.
  Mean final reward:        -0.2273
  Mean return:              -5.1544
  Mean final entropy:       0.0015
  Max final entropy:        0.1049
  Pseudo loss:              -0.35881
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.141
Episode (780/5000) took 8.278 seconds.
  Mean final reward:        -0.1626
  Mean return:              -5.1549
  Mean final entropy:       0.0008
  Max final entropy:        0.0301
  Pseudo loss:              -0.25374
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.168
Episode (790/5000) took 8.362 seconds.
  Mean final reward:        -0.1459
  Mean return:              -4.9010
  Mean final entropy:       0.0008
  Max final entropy:        0.0391
  Pseudo loss:              -0.13626
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.238
Episode (800/5000) took 8.386 seconds.
  Mean final reward:        -0.1923
  Mean return:              -4.9763
  Mean final entropy:       0.0011
  Max final entropy:        0.0279
  Pseudo loss:              -0.26907
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.133
Episode (810/5000) took 8.360 seconds.
  Mean final reward:        -0.1774
  Mean return:              -4.7795
  Mean final entropy:       0.0009
  Max final entropy:        0.0375
  Pseudo loss:              -0.20688
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.121
Episode (820/5000) took 8.320 seconds.
  Mean final reward:        -0.1612
  Mean return:              -5.1345
  Mean final entropy:       0.0009
  Max final entropy:        0.0643
  Pseudo loss:              -0.22857
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.180
Episode (830/5000) took 8.310 seconds.
  Mean final reward:        -0.1926
  Mean return:              -5.1935
  Mean final entropy:       0.0009
  Max final entropy:        0.0438
  Pseudo loss:              -0.40131
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.129
Episode (840/5000) took 8.390 seconds.
  Mean final reward:        -0.1774
  Mean return:              -5.1123
  Mean final entropy:       0.0011
  Max final entropy:        0.0467
  Pseudo loss:              -0.23985
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.230
Episode (850/5000) took 8.387 seconds.
  Mean final reward:        -0.2229
  Mean return:              -5.0971
  Mean final entropy:       0.0016
  Max final entropy:        0.1023
  Pseudo loss:              -0.49490
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.184
Episode (860/5000) took 8.433 seconds.
  Mean final reward:        -0.2086
  Mean return:              -5.1718
  Mean final entropy:       0.0011
  Max final entropy:        0.0335
  Pseudo loss:              -0.34338
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.141
Episode (870/5000) took 8.337 seconds.
  Mean final reward:        -0.2003
  Mean return:              -5.0782
  Mean final entropy:       0.0012
  Max final entropy:        0.0578
  Pseudo loss:              -0.23536
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.148
Episode (880/5000) took 8.373 seconds.
  Mean final reward:        -0.2575
  Mean return:              -5.2305
  Mean final entropy:       0.0021
  Max final entropy:        0.1413
  Pseudo loss:              -0.40335
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.133
Episode (890/5000) took 8.313 seconds.
  Mean final reward:        -0.2041
  Mean return:              -5.1005
  Mean final entropy:       0.0017
  Max final entropy:        0.1360
  Pseudo loss:              -0.16919
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.195
Episode (900/5000) took 8.320 seconds.
  Mean final reward:        -0.1988
  Mean return:              -5.1875
  Mean final entropy:       0.0011
  Max final entropy:        0.0634
  Pseudo loss:              -0.27822
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.160
Episode (910/5000) took 8.400 seconds.
  Mean final reward:        -0.1602
  Mean return:              -5.0559
  Mean final entropy:       0.0009
  Max final entropy:        0.0465
  Pseudo loss:              -0.50924
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.180
Episode (920/5000) took 8.223 seconds.
  Mean final reward:        -0.1725
  Mean return:              -5.1080
  Mean final entropy:       0.0011
  Max final entropy:        0.0518
  Pseudo loss:              -0.35460
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.172
Episode (930/5000) took 8.371 seconds.
  Mean final reward:        -0.1083
  Mean return:              -4.7042
  Mean final entropy:       0.0005
  Max final entropy:        0.0133
  Pseudo loss:              -0.36365
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.168
Episode (940/5000) took 8.325 seconds.
  Mean final reward:        -0.2026
  Mean return:              -4.9935
  Mean final entropy:       0.0010
  Max final entropy:        0.0358
  Pseudo loss:              -0.26181
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.133
Episode (950/5000) took 8.275 seconds.
  Mean final reward:        -0.1881
  Mean return:              -5.1858
  Mean final entropy:       0.0009
  Max final entropy:        0.0250
  Pseudo loss:              -0.20911
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.195
Episode (960/5000) took 8.307 seconds.
  Mean final reward:        -0.1960
  Mean return:              -5.1414
  Mean final entropy:       0.0012
  Max final entropy:        0.0415
  Pseudo loss:              -0.15459
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.176
Episode (970/5000) took 8.332 seconds.
  Mean final reward:        -0.2118
  Mean return:              -5.3765
  Mean final entropy:       0.0016
  Max final entropy:        0.1448
  Pseudo loss:              -0.45363
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.184
Episode (980/5000) took 8.367 seconds.
  Mean final reward:        -0.1901
  Mean return:              -4.9542
  Mean final entropy:       0.0010
  Max final entropy:        0.0408
  Pseudo loss:              -0.37066
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.125
Episode (990/5000) took 8.298 seconds.
  Mean final reward:        -0.1589
  Mean return:              -4.9534
  Mean final entropy:       0.0008
  Max final entropy:        0.0507
  Pseudo loss:              -0.25132
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.156
Episode (1000/5000) took 8.349 seconds.
  Mean final reward:        -0.1890
  Mean return:              -5.0976
  Mean final entropy:       0.0015
  Max final entropy:        0.0797
  Pseudo loss:              -0.31905
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.176
Episode (1010/5000) took 8.347 seconds.
  Mean final reward:        -0.1794
  Mean return:              -4.8803
  Mean final entropy:       0.0009
  Max final entropy:        0.0445
  Pseudo loss:              -0.16365
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.125
Episode (1020/5000) took 8.261 seconds.
  Mean final reward:        -0.1313
  Mean return:              -5.0804
  Mean final entropy:       0.0007
  Max final entropy:        0.0203
  Pseudo loss:              -0.21669
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.207
Episode (1030/5000) took 8.309 seconds.
  Mean final reward:        -0.1601
  Mean return:              -5.0403
  Mean final entropy:       0.0010
  Max final entropy:        0.0387
  Pseudo loss:              -0.33674
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.215
Episode (1040/5000) took 8.288 seconds.
  Mean final reward:        -0.1242
  Mean return:              -5.1393
  Mean final entropy:       0.0009
  Max final entropy:        0.0658
  Pseudo loss:              -0.25255
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.289
Episode (1050/5000) took 8.370 seconds.
  Mean final reward:        -0.0917
  Mean return:              -4.7539
  Mean final entropy:       0.0004
  Max final entropy:        0.0143
  Pseudo loss:              -0.11680
  Solved trajectories:      234 / 256
  Avg steps to disentangle: 2.195
Episode (1060/5000) took 8.340 seconds.
  Mean final reward:        -0.2430
  Mean return:              -5.4024
  Mean final entropy:       0.0017
  Max final entropy:        0.1192
  Pseudo loss:              -0.41909
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.137
Episode (1070/5000) took 8.306 seconds.
  Mean final reward:        -0.1018
  Mean return:              -4.7888
  Mean final entropy:       0.0005
  Max final entropy:        0.0305
  Pseudo loss:              -0.19476
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.234
Episode (1080/5000) took 8.385 seconds.
  Mean final reward:        -0.1378
  Mean return:              -5.2147
  Mean final entropy:       0.0009
  Max final entropy:        0.0598
  Pseudo loss:              -0.18413
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.242
Episode (1090/5000) took 8.317 seconds.
  Mean final reward:        -0.1671
  Mean return:              -5.0865
  Mean final entropy:       0.0009
  Max final entropy:        0.0534
  Pseudo loss:              -0.31464
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.199
Episode (1100/5000) took 8.304 seconds.
  Mean final reward:        -0.1165
  Mean return:              -4.7326
  Mean final entropy:       0.0006
  Max final entropy:        0.0290
  Pseudo loss:              -0.16708
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.203
Episode (1110/5000) took 8.356 seconds.
  Mean final reward:        -0.1772
  Mean return:              -5.2857
  Mean final entropy:       0.0008
  Max final entropy:        0.0409
  Pseudo loss:              -0.16805
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.203
Episode (1120/5000) took 8.401 seconds.
  Mean final reward:        -0.1700
  Mean return:              -4.9778
  Mean final entropy:       0.0011
  Max final entropy:        0.0675
  Pseudo loss:              -0.19614
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.164
Episode (1130/5000) took 8.295 seconds.
  Mean final reward:        -0.1565
  Mean return:              -4.9717
  Mean final entropy:       0.0012
  Max final entropy:        0.1380
  Pseudo loss:              -0.28176
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.125
Episode (1140/5000) took 8.324 seconds.
  Mean final reward:        -0.1542
  Mean return:              -4.7944
  Mean final entropy:       0.0007
  Max final entropy:        0.0267
  Pseudo loss:              -0.15878
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.125
Episode (1150/5000) took 8.351 seconds.
  Mean final reward:        -0.1920
  Mean return:              -5.2493
  Mean final entropy:       0.0015
  Max final entropy:        0.0707
  Pseudo loss:              -0.36800
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.242
Episode (1160/5000) took 8.300 seconds.
  Mean final reward:        -0.2027
  Mean return:              -5.1556
  Mean final entropy:       0.0009
  Max final entropy:        0.0281
  Pseudo loss:              -0.17113
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.137
Episode (1170/5000) took 8.346 seconds.
  Mean final reward:        -0.1941
  Mean return:              -5.1297
  Mean final entropy:       0.0017
  Max final entropy:        0.1497
  Pseudo loss:              -0.45421
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.172
Episode (1180/5000) took 8.266 seconds.
  Mean final reward:        -0.1941
  Mean return:              -5.2322
  Mean final entropy:       0.0008
  Max final entropy:        0.0248
  Pseudo loss:              -0.47109
  Solved trajectories:      211 / 256
  Avg steps to disentangle: 2.051
Episode (1190/5000) took 8.328 seconds.
  Mean final reward:        -0.2293
  Mean return:              -5.1696
  Mean final entropy:       0.0013
  Max final entropy:        0.0459
  Pseudo loss:              -0.23441
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.113
Episode (1200/5000) took 8.269 seconds.
  Mean final reward:        -0.1698
  Mean return:              -4.8714
  Mean final entropy:       0.0007
  Max final entropy:        0.0200
  Pseudo loss:              -0.13725
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.164
Episode (1210/5000) took 8.305 seconds.
  Mean final reward:        -0.1567
  Mean return:              -4.9685
  Mean final entropy:       0.0008
  Max final entropy:        0.0301
  Pseudo loss:              -0.30598
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.148
Episode (1220/5000) took 8.323 seconds.
  Mean final reward:        -0.1652
  Mean return:              -5.0616
  Mean final entropy:       0.0008
  Max final entropy:        0.0357
  Pseudo loss:              -0.23755
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.176
Episode (1230/5000) took 8.458 seconds.
  Mean final reward:        -0.2284
  Mean return:              -5.3322
  Mean final entropy:       0.0015
  Max final entropy:        0.1120
  Pseudo loss:              -0.40029
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 2.113
Episode (1240/5000) took 8.360 seconds.
  Mean final reward:        -0.1478
  Mean return:              -4.9736
  Mean final entropy:       0.0012
  Max final entropy:        0.1521
  Pseudo loss:              -0.41445
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.164
Episode (1250/5000) took 8.383 seconds.
  Mean final reward:        -0.2072
  Mean return:              -5.0891
  Mean final entropy:       0.0013
  Max final entropy:        0.0528
  Pseudo loss:              -0.39509
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.086
Episode (1260/5000) took 8.407 seconds.
  Mean final reward:        -0.2067
  Mean return:              -5.0975
  Mean final entropy:       0.0011
  Max final entropy:        0.0268
  Pseudo loss:              -0.26842
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.145
Episode (1270/5000) took 8.385 seconds.
  Mean final reward:        -0.1383
  Mean return:              -4.9372
  Mean final entropy:       0.0006
  Max final entropy:        0.0195
  Pseudo loss:              -0.18469
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.176
Episode (1280/5000) took 8.400 seconds.
  Mean final reward:        -0.1632
  Mean return:              -5.1372
  Mean final entropy:       0.0007
  Max final entropy:        0.0238
  Pseudo loss:              -0.16819
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.148
Episode (1290/5000) took 8.395 seconds.
  Mean final reward:        -0.1180
  Mean return:              -4.7226
  Mean final entropy:       0.0006
  Max final entropy:        0.0239
  Pseudo loss:              -0.18784
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.160
Episode (1300/5000) took 8.380 seconds.
  Mean final reward:        -0.1877
  Mean return:              -5.1514
  Mean final entropy:       0.0015
  Max final entropy:        0.1224
  Pseudo loss:              -0.27501
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.141
Episode (1310/5000) took 8.291 seconds.
  Mean final reward:        -0.1690
  Mean return:              -5.1698
  Mean final entropy:       0.0012
  Max final entropy:        0.0783
  Pseudo loss:              -0.18396
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.195
Episode (1320/5000) took 8.313 seconds.
  Mean final reward:        -0.1526
  Mean return:              -5.1788
  Mean final entropy:       0.0007
  Max final entropy:        0.0384
  Pseudo loss:              -0.11931
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.137
Episode (1330/5000) took 8.444 seconds.
  Mean final reward:        -0.1412
  Mean return:              -4.9910
  Mean final entropy:       0.0007
  Max final entropy:        0.0303
  Pseudo loss:              -0.15542
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.199
Episode (1340/5000) took 8.371 seconds.
  Mean final reward:        -0.1351
  Mean return:              -4.7161
  Mean final entropy:       0.0009
  Max final entropy:        0.0692
  Pseudo loss:              -0.08377
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.211
Episode (1350/5000) took 8.375 seconds.
  Mean final reward:        -0.1444
  Mean return:              -4.9852
  Mean final entropy:       0.0006
  Max final entropy:        0.0165
  Pseudo loss:              -0.07660
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.195
Episode (1360/5000) took 8.351 seconds.
  Mean final reward:        -0.1021
  Mean return:              -4.8877
  Mean final entropy:       0.0004
  Max final entropy:        0.0089
  Pseudo loss:              -0.10771
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.207
Episode (1370/5000) took 8.371 seconds.
  Mean final reward:        -0.1045
  Mean return:              -4.9165
  Mean final entropy:       0.0005
  Max final entropy:        0.0166
  Pseudo loss:              -0.26304
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.207
Episode (1380/5000) took 8.326 seconds.
  Mean final reward:        -0.1900
  Mean return:              -5.2095
  Mean final entropy:       0.0009
  Max final entropy:        0.0312
  Pseudo loss:              -0.16376
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.137
Episode (1390/5000) took 8.348 seconds.
  Mean final reward:        -0.1345
  Mean return:              -5.0859
  Mean final entropy:       0.0006
  Max final entropy:        0.0184
  Pseudo loss:              -0.10554
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.184
Episode (1400/5000) took 8.346 seconds.
  Mean final reward:        -0.1651
  Mean return:              -5.1379
  Mean final entropy:       0.0008
  Max final entropy:        0.0444
  Pseudo loss:              -0.26219
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.219
Episode (1410/5000) took 8.290 seconds.
  Mean final reward:        -0.1762
  Mean return:              -5.1477
  Mean final entropy:       0.0009
  Max final entropy:        0.0380
  Pseudo loss:              -0.13215
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.125
Episode (1420/5000) took 8.338 seconds.
  Mean final reward:        -0.1688
  Mean return:              -5.0664
  Mean final entropy:       0.0008
  Max final entropy:        0.0252
  Pseudo loss:              -0.27636
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.234
Episode (1430/5000) took 8.374 seconds.
  Mean final reward:        -0.2060
  Mean return:              -5.1247
  Mean final entropy:       0.0014
  Max final entropy:        0.1312
  Pseudo loss:              -0.25516
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.152
Episode (1440/5000) took 8.362 seconds.
  Mean final reward:        -0.2058
  Mean return:              -5.2707
  Mean final entropy:       0.0010
  Max final entropy:        0.0252
  Pseudo loss:              -0.41887
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.141
Episode (1450/5000) took 8.260 seconds.
  Mean final reward:        -0.1300
  Mean return:              -4.8823
  Mean final entropy:       0.0007
  Max final entropy:        0.0377
  Pseudo loss:              -0.11338
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.176
Episode (1460/5000) took 8.393 seconds.
  Mean final reward:        -0.2303
  Mean return:              -5.2306
  Mean final entropy:       0.0017
  Max final entropy:        0.1418
  Pseudo loss:              -0.29442
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.172
Episode (1470/5000) took 8.377 seconds.
  Mean final reward:        -0.1540
  Mean return:              -4.9610
  Mean final entropy:       0.0008
  Max final entropy:        0.0333
  Pseudo loss:              -0.20032
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.117
Episode (1480/5000) took 8.384 seconds.
  Mean final reward:        -0.1811
  Mean return:              -5.1339
  Mean final entropy:       0.0011
  Max final entropy:        0.0667
  Pseudo loss:              -0.22384
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.164
Episode (1490/5000) took 8.463 seconds.
  Mean final reward:        -0.1726
  Mean return:              -5.1850
  Mean final entropy:       0.0009
  Max final entropy:        0.0295
  Pseudo loss:              -0.31268
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.164
Episode (1500/5000) took 8.336 seconds.
  Mean final reward:        -0.1496
  Mean return:              -4.8609
  Mean final entropy:       0.0007
  Max final entropy:        0.0262
  Pseudo loss:              -0.17959
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.172
Episode (1510/5000) took 8.393 seconds.
  Mean final reward:        -0.1707
  Mean return:              -5.2649
  Mean final entropy:       0.0008
  Max final entropy:        0.0318
  Pseudo loss:              -0.17305
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.133
Episode (1520/5000) took 8.275 seconds.
  Mean final reward:        -0.1553
  Mean return:              -4.7908
  Mean final entropy:       0.0013
  Max final entropy:        0.0809
  Pseudo loss:              -0.19675
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.152
Episode (1530/5000) took 8.413 seconds.
  Mean final reward:        -0.1795
  Mean return:              -5.0489
  Mean final entropy:       0.0009
  Max final entropy:        0.0251
  Pseudo loss:              -0.21511
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.184
Episode (1540/5000) took 8.302 seconds.
  Mean final reward:        -0.1947
  Mean return:              -5.0382
  Mean final entropy:       0.0010
  Max final entropy:        0.0531
  Pseudo loss:              -0.13368
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.129
Episode (1550/5000) took 8.380 seconds.
  Mean final reward:        -0.2039
  Mean return:              -5.0286
  Mean final entropy:       0.0014
  Max final entropy:        0.0811
  Pseudo loss:              -0.19996
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 2.125
Episode (1560/5000) took 8.394 seconds.
  Mean final reward:        -0.0894
  Mean return:              -4.7687
  Mean final entropy:       0.0004
  Max final entropy:        0.0088
  Pseudo loss:              -0.05742
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.188
Episode (1570/5000) took 8.321 seconds.
  Mean final reward:        -0.2149
  Mean return:              -5.2063
  Mean final entropy:       0.0011
  Max final entropy:        0.0432
  Pseudo loss:              -0.29617
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.117
Episode (1580/5000) took 8.430 seconds.
  Mean final reward:        -0.1604
  Mean return:              -4.9232
  Mean final entropy:       0.0010
  Max final entropy:        0.0698
  Pseudo loss:              -0.25050
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.180
Episode (1590/5000) took 8.338 seconds.
  Mean final reward:        -0.1195
  Mean return:              -4.8177
  Mean final entropy:       0.0007
  Max final entropy:        0.0339
  Pseudo loss:              -0.23647
  Solved trajectories:      234 / 256
  Avg steps to disentangle: 2.234
Episode (1600/5000) took 8.340 seconds.
  Mean final reward:        -0.1780
  Mean return:              -5.0549
  Mean final entropy:       0.0009
  Max final entropy:        0.0433
  Pseudo loss:              -0.20592
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.176
Episode (1610/5000) took 8.289 seconds.
  Mean final reward:        -0.1434
  Mean return:              -4.8968
  Mean final entropy:       0.0007
  Max final entropy:        0.0254
  Pseudo loss:              -0.15550
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.242
Episode (1620/5000) took 8.332 seconds.
  Mean final reward:        -0.1474
  Mean return:              -5.0057
  Mean final entropy:       0.0008
  Max final entropy:        0.0581
  Pseudo loss:              -0.18120
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.176
Episode (1630/5000) took 8.302 seconds.
  Mean final reward:        -0.2007
  Mean return:              -5.2446
  Mean final entropy:       0.0010
  Max final entropy:        0.0387
  Pseudo loss:              -0.25992
  Solved trajectories:      210 / 256
  Avg steps to disentangle: 2.094
Episode (1640/5000) took 8.407 seconds.
  Mean final reward:        -0.1528
  Mean return:              -5.0759
  Mean final entropy:       0.0006
  Max final entropy:        0.0324
  Pseudo loss:              -0.15209
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.145
Episode (1650/5000) took 8.322 seconds.
  Mean final reward:        -0.1948
  Mean return:              -5.2176
  Mean final entropy:       0.0011
  Max final entropy:        0.0406
  Pseudo loss:              -0.12940
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.199
Episode (1660/5000) took 8.379 seconds.
  Mean final reward:        -0.1058
  Mean return:              -4.8626
  Mean final entropy:       0.0005
  Max final entropy:        0.0250
  Pseudo loss:              -0.27457
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.180
Episode (1670/5000) took 8.418 seconds.
  Mean final reward:        -0.1866
  Mean return:              -5.2589
  Mean final entropy:       0.0009
  Max final entropy:        0.0287
  Pseudo loss:              -0.13591
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.188
Episode (1680/5000) took 8.376 seconds.
  Mean final reward:        -0.1466
  Mean return:              -5.0335
  Mean final entropy:       0.0006
  Max final entropy:        0.0149
  Pseudo loss:              -0.22845
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.160
Episode (1690/5000) took 8.359 seconds.
  Mean final reward:        -0.1682
  Mean return:              -5.0153
  Mean final entropy:       0.0007
  Max final entropy:        0.0205
  Pseudo loss:              -0.20567
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.152
Episode (1700/5000) took 8.341 seconds.
  Mean final reward:        -0.1542
  Mean return:              -4.8984
  Mean final entropy:       0.0009
  Max final entropy:        0.0541
  Pseudo loss:              -0.14833
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.230
Episode (1710/5000) took 8.383 seconds.
  Mean final reward:        -0.1176
  Mean return:              -4.8074
  Mean final entropy:       0.0005
  Max final entropy:        0.0156
  Pseudo loss:              -0.13964
  Solved trajectories:      234 / 256
  Avg steps to disentangle: 2.223
Episode (1720/5000) took 8.335 seconds.
  Mean final reward:        -0.1507
  Mean return:              -5.2468
  Mean final entropy:       0.0007
  Max final entropy:        0.0307
  Pseudo loss:              -0.15745
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.215
Episode (1730/5000) took 8.331 seconds.
  Mean final reward:        -0.1482
  Mean return:              -5.1129
  Mean final entropy:       0.0008
  Max final entropy:        0.0307
  Pseudo loss:              -0.19979
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.215
Episode (1740/5000) took 8.474 seconds.
  Mean final reward:        -0.1270
  Mean return:              -4.8671
  Mean final entropy:       0.0006
  Max final entropy:        0.0162
  Pseudo loss:              -0.21437
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.223
Episode (1750/5000) took 8.320 seconds.
  Mean final reward:        -0.1271
  Mean return:              -5.0435
  Mean final entropy:       0.0006
  Max final entropy:        0.0288
  Pseudo loss:              -0.13251
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.242
Episode (1760/5000) took 8.305 seconds.
  Mean final reward:        -0.1561
  Mean return:              -5.2161
  Mean final entropy:       0.0006
  Max final entropy:        0.0141
  Pseudo loss:              -0.12424
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.176
Episode (1770/5000) took 8.339 seconds.
  Mean final reward:        -0.1142
  Mean return:              -4.9496
  Mean final entropy:       0.0006
  Max final entropy:        0.0240
  Pseudo loss:              -0.06019
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.254
Episode (1780/5000) took 8.334 seconds.
  Mean final reward:        -0.1597
  Mean return:              -5.0608
  Mean final entropy:       0.0008
  Max final entropy:        0.0473
  Pseudo loss:              -0.08822
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.148
Episode (1790/5000) took 8.411 seconds.
  Mean final reward:        -0.1507
  Mean return:              -4.7397
  Mean final entropy:       0.0008
  Max final entropy:        0.0444
  Pseudo loss:              -0.19289
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.168
Episode (1800/5000) took 8.312 seconds.
  Mean final reward:        -0.1600
  Mean return:              -5.2190
  Mean final entropy:       0.0007
  Max final entropy:        0.0191
  Pseudo loss:              -0.12335
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.176
Episode (1810/5000) took 8.299 seconds.
  Mean final reward:        -0.1612
  Mean return:              -5.0364
  Mean final entropy:       0.0008
  Max final entropy:        0.0264
  Pseudo loss:              -0.14665
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.160
Episode (1820/5000) took 8.326 seconds.
  Mean final reward:        -0.1387
  Mean return:              -4.8613
  Mean final entropy:       0.0007
  Max final entropy:        0.0262
  Pseudo loss:              -0.21056
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.207
Episode (1830/5000) took 8.346 seconds.
  Mean final reward:        -0.1159
  Mean return:              -4.7701
  Mean final entropy:       0.0005
  Max final entropy:        0.0149
  Pseudo loss:              -0.07188
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.121
Episode (1840/5000) took 8.257 seconds.
  Mean final reward:        -0.1516
  Mean return:              -4.8959
  Mean final entropy:       0.0007
  Max final entropy:        0.0252
  Pseudo loss:              -0.18468
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.176
Episode (1850/5000) took 8.370 seconds.
  Mean final reward:        -0.1344
  Mean return:              -5.0704
  Mean final entropy:       0.0006
  Max final entropy:        0.0130
  Pseudo loss:              -0.12268
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.191
Episode (1860/5000) took 8.403 seconds.
  Mean final reward:        -0.1626
  Mean return:              -5.1263
  Mean final entropy:       0.0008
  Max final entropy:        0.0215
  Pseudo loss:              -0.15568
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.195
Episode (1870/5000) took 8.331 seconds.
  Mean final reward:        -0.1907
  Mean return:              -5.1253
  Mean final entropy:       0.0011
  Max final entropy:        0.0765
  Pseudo loss:              -0.41586
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.191
Episode (1880/5000) took 8.327 seconds.
  Mean final reward:        -0.1233
  Mean return:              -4.9946
  Mean final entropy:       0.0006
  Max final entropy:        0.0152
  Pseudo loss:              -0.13455
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.180
Episode (1890/5000) took 8.404 seconds.
  Mean final reward:        -0.0991
  Mean return:              -4.7542
  Mean final entropy:       0.0004
  Max final entropy:        0.0168
  Pseudo loss:              -0.06485
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.199
Episode (1900/5000) took 8.368 seconds.
  Mean final reward:        -0.1606
  Mean return:              -5.3955
  Mean final entropy:       0.0006
  Max final entropy:        0.0134
  Pseudo loss:              -0.11425
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.234
Episode (1910/5000) took 8.365 seconds.
  Mean final reward:        -0.1221
  Mean return:              -4.9509
  Mean final entropy:       0.0006
  Max final entropy:        0.0318
  Pseudo loss:              -0.07781
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.203
Episode (1920/5000) took 8.368 seconds.
  Mean final reward:        -0.2100
  Mean return:              -5.1996
  Mean final entropy:       0.0013
  Max final entropy:        0.0588
  Pseudo loss:              -0.09826
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.160
Episode (1930/5000) took 8.378 seconds.
  Mean final reward:        -0.1653
  Mean return:              -5.0306
  Mean final entropy:       0.0008
  Max final entropy:        0.0235
  Pseudo loss:              -0.15066
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.129
Episode (1940/5000) took 8.377 seconds.
  Mean final reward:        -0.2056
  Mean return:              -5.2361
  Mean final entropy:       0.0012
  Max final entropy:        0.0501
  Pseudo loss:              -0.20700
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.176
Episode (1950/5000) took 8.308 seconds.
  Mean final reward:        -0.1689
  Mean return:              -5.0340
  Mean final entropy:       0.0010
  Max final entropy:        0.0605
  Pseudo loss:              -0.13042
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.176
Episode (1960/5000) took 8.358 seconds.
  Mean final reward:        -0.1327
  Mean return:              -5.0357
  Mean final entropy:       0.0006
  Max final entropy:        0.0166
  Pseudo loss:              -0.09707
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.184
Episode (1970/5000) took 8.366 seconds.
  Mean final reward:        -0.1423
  Mean return:              -4.9498
  Mean final entropy:       0.0006
  Max final entropy:        0.0128
  Pseudo loss:              -0.06560
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.172
Episode (1980/5000) took 8.298 seconds.
  Mean final reward:        -0.1253
  Mean return:              -4.9895
  Mean final entropy:       0.0006
  Max final entropy:        0.0292
  Pseudo loss:              -0.16190
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.156
Episode (1990/5000) took 8.336 seconds.
  Mean final reward:        -0.2027
  Mean return:              -5.3013
  Mean final entropy:       0.0015
  Max final entropy:        0.0952
  Pseudo loss:              -0.36378
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.234
Episode (2000/5000) took 8.284 seconds.
  Mean final reward:        -0.1676
  Mean return:              -5.0782
  Mean final entropy:       0.0008
  Max final entropy:        0.0210
  Pseudo loss:              -0.12624
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.137
Episode (2010/5000) took 8.327 seconds.
  Mean final reward:        -0.1551
  Mean return:              -5.0818
  Mean final entropy:       0.0006
  Max final entropy:        0.0183
  Pseudo loss:              -0.18063
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.203
Episode (2020/5000) took 8.367 seconds.
  Mean final reward:        -0.1293
  Mean return:              -4.7423
  Mean final entropy:       0.0006
  Max final entropy:        0.0284
  Pseudo loss:              -0.13229
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.188
Episode (2030/5000) took 8.388 seconds.
  Mean final reward:        -0.1392
  Mean return:              -4.8131
  Mean final entropy:       0.0008
  Max final entropy:        0.0634
  Pseudo loss:              -0.05967
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.164
Episode (2040/5000) took 8.509 seconds.
  Mean final reward:        -0.1488
  Mean return:              -4.8306
  Mean final entropy:       0.0008
  Max final entropy:        0.0399
  Pseudo loss:              -0.21109
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.133
Episode (2050/5000) took 8.888 seconds.
  Mean final reward:        -0.1228
  Mean return:              -5.2382
  Mean final entropy:       0.0005
  Max final entropy:        0.0193
  Pseudo loss:              -0.12252
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.266
Episode (2060/5000) took 8.564 seconds.
  Mean final reward:        -0.1325
  Mean return:              -4.9877
  Mean final entropy:       0.0006
  Max final entropy:        0.0223
  Pseudo loss:              -0.22358
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.219
Episode (2070/5000) took 9.859 seconds.
  Mean final reward:        -0.1583
  Mean return:              -5.0926
  Mean final entropy:       0.0007
  Max final entropy:        0.0277
  Pseudo loss:              -0.13917
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.168
Episode (2080/5000) took 8.369 seconds.
  Mean final reward:        -0.1168
  Mean return:              -4.9255
  Mean final entropy:       0.0005
  Max final entropy:        0.0108
  Pseudo loss:              -0.03190
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.125
Episode (2090/5000) took 8.809 seconds.
  Mean final reward:        -0.1494
  Mean return:              -4.6931
  Mean final entropy:       0.0007
  Max final entropy:        0.0183
  Pseudo loss:              -0.13466
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.156
Episode (2100/5000) took 8.979 seconds.
  Mean final reward:        -0.1509
  Mean return:              -5.3660
  Mean final entropy:       0.0007
  Max final entropy:        0.0181
  Pseudo loss:              -0.16986
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.184
Episode (2110/5000) took 8.505 seconds.
  Mean final reward:        -0.1476
  Mean return:              -5.0716
  Mean final entropy:       0.0006
  Max final entropy:        0.0127
  Pseudo loss:              -0.13360
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.160
Episode (2120/5000) took 8.677 seconds.
  Mean final reward:        -0.1839
  Mean return:              -5.1009
  Mean final entropy:       0.0008
  Max final entropy:        0.0331
  Pseudo loss:              -0.08686
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.203
Episode (2130/5000) took 9.876 seconds.
  Mean final reward:        -0.1678
  Mean return:              -5.2052
  Mean final entropy:       0.0008
  Max final entropy:        0.0319
  Pseudo loss:              -0.15323
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.219
Episode (2140/5000) took 9.093 seconds.
  Mean final reward:        -0.2200
  Mean return:              -5.0025
  Mean final entropy:       0.0012
  Max final entropy:        0.0638
  Pseudo loss:              -0.23521
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 2.027
Episode (2150/5000) took 8.392 seconds.
  Mean final reward:        -0.1191
  Mean return:              -5.0445
  Mean final entropy:       0.0006
  Max final entropy:        0.0234
  Pseudo loss:              -0.21197
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.250
Episode (2160/5000) took 8.371 seconds.
  Mean final reward:        -0.1452
  Mean return:              -5.0000
  Mean final entropy:       0.0006
  Max final entropy:        0.0194
  Pseudo loss:              -0.05185
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.227
Episode (2170/5000) took 8.366 seconds.
  Mean final reward:        -0.1180
  Mean return:              -4.8235
  Mean final entropy:       0.0006
  Max final entropy:        0.0269
  Pseudo loss:              -0.12944
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.227
Episode (2180/5000) took 8.463 seconds.
  Mean final reward:        -0.1395
  Mean return:              -5.3705
  Mean final entropy:       0.0006
  Max final entropy:        0.0181
  Pseudo loss:              -0.28159
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.277
Episode (2190/5000) took 8.330 seconds.
  Mean final reward:        -0.1559
  Mean return:              -5.1120
  Mean final entropy:       0.0009
  Max final entropy:        0.0585
  Pseudo loss:              -0.21504
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.180
Episode (2200/5000) took 8.377 seconds.
  Mean final reward:        -0.1690
  Mean return:              -4.8268
  Mean final entropy:       0.0011
  Max final entropy:        0.0772
  Pseudo loss:              -0.23515
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.184
Episode (2210/5000) took 8.351 seconds.
  Mean final reward:        -0.1301
  Mean return:              -4.9059
  Mean final entropy:       0.0008
  Max final entropy:        0.0849
  Pseudo loss:              -0.10432
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.223
Episode (2220/5000) took 8.276 seconds.
  Mean final reward:        -0.1639
  Mean return:              -5.2922
  Mean final entropy:       0.0011
  Max final entropy:        0.0857
  Pseudo loss:              -0.37881
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.188
Episode (2230/5000) took 8.477 seconds.
  Mean final reward:        -0.1360
  Mean return:              -4.9132
  Mean final entropy:       0.0010
  Max final entropy:        0.1158
  Pseudo loss:              -0.16345
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.184
Episode (2240/5000) took 8.361 seconds.
  Mean final reward:        -0.1906
  Mean return:              -5.2670
  Mean final entropy:       0.0008
  Max final entropy:        0.0206
  Pseudo loss:              -0.26754
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.168
Episode (2250/5000) took 8.291 seconds.
  Mean final reward:        -0.1154
  Mean return:              -4.8834
  Mean final entropy:       0.0005
  Max final entropy:        0.0248
  Pseudo loss:              -0.09747
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.137
Episode (2260/5000) took 8.346 seconds.
  Mean final reward:        -0.1917
  Mean return:              -4.8874
  Mean final entropy:       0.0009
  Max final entropy:        0.0313
  Pseudo loss:              -0.38699
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.086
Episode (2270/5000) took 8.346 seconds.
  Mean final reward:        -0.0703
  Mean return:              -4.6362
  Mean final entropy:       0.0003
  Max final entropy:        0.0106
  Pseudo loss:              -0.00463
  Solved trajectories:      242 / 256
  Avg steps to disentangle: 2.289
Episode (2280/5000) took 8.278 seconds.
  Mean final reward:        -0.1296
  Mean return:              -4.9377
  Mean final entropy:       0.0010
  Max final entropy:        0.1328
  Pseudo loss:              -0.12448
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.180
Episode (2290/5000) took 8.350 seconds.
  Mean final reward:        -0.1641
  Mean return:              -5.1744
  Mean final entropy:       0.0008
  Max final entropy:        0.0281
  Pseudo loss:              -0.21906
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.133
Episode (2300/5000) took 8.313 seconds.
  Mean final reward:        -0.1657
  Mean return:              -5.1022
  Mean final entropy:       0.0008
  Max final entropy:        0.0269
  Pseudo loss:              -0.13852
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.211
Episode (2310/5000) took 8.339 seconds.
  Mean final reward:        -0.1267
  Mean return:              -5.0309
  Mean final entropy:       0.0006
  Max final entropy:        0.0224
  Pseudo loss:              -0.12448
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.211
Episode (2320/5000) took 8.312 seconds.
  Mean final reward:        -0.1338
  Mean return:              -4.8809
  Mean final entropy:       0.0007
  Max final entropy:        0.0378
  Pseudo loss:              -0.00982
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.145
Episode (2330/5000) took 8.532 seconds.
  Mean final reward:        -0.1496
  Mean return:              -4.9560
  Mean final entropy:       0.0009
  Max final entropy:        0.0594
  Pseudo loss:              -0.17971
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.184
Episode (2340/5000) took 8.364 seconds.
  Mean final reward:        -0.1163
  Mean return:              -5.0497
  Mean final entropy:       0.0006
  Max final entropy:        0.0195
  Pseudo loss:              -0.15723
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.242
Episode (2350/5000) took 8.339 seconds.
  Mean final reward:        -0.1570
  Mean return:              -5.1179
  Mean final entropy:       0.0007
  Max final entropy:        0.0285
  Pseudo loss:              -0.23256
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.148
Episode (2360/5000) took 8.396 seconds.
  Mean final reward:        -0.1976
  Mean return:              -5.1969
  Mean final entropy:       0.0009
  Max final entropy:        0.0261
  Pseudo loss:              -0.08222
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.109
Episode (2370/5000) took 8.363 seconds.
  Mean final reward:        -0.2263
  Mean return:              -5.2285
  Mean final entropy:       0.0015
  Max final entropy:        0.1151
  Pseudo loss:              -0.19332
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.109
Episode (2380/5000) took 8.363 seconds.
  Mean final reward:        -0.1772
  Mean return:              -5.1017
  Mean final entropy:       0.0011
  Max final entropy:        0.0523
  Pseudo loss:              -0.26167
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.168
Episode (2390/5000) took 8.323 seconds.
  Mean final reward:        -0.1483
  Mean return:              -5.0357
  Mean final entropy:       0.0008
  Max final entropy:        0.0423
  Pseudo loss:              -0.22600
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.176
Episode (2400/5000) took 8.241 seconds.
  Mean final reward:        -0.1576
  Mean return:              -5.0517
  Mean final entropy:       0.0008
  Max final entropy:        0.0392
  Pseudo loss:              -0.17877
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.168
Episode (2410/5000) took 8.342 seconds.
  Mean final reward:        -0.1193
  Mean return:              -4.7615
  Mean final entropy:       0.0005
  Max final entropy:        0.0148
  Pseudo loss:              -0.08951
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.164
Episode (2420/5000) took 8.389 seconds.
  Mean final reward:        -0.1771
  Mean return:              -4.8078
  Mean final entropy:       0.0010
  Max final entropy:        0.0338
  Pseudo loss:              -0.13343
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.121
Episode (2430/5000) took 8.328 seconds.
  Mean final reward:        -0.1754
  Mean return:              -4.9277
  Mean final entropy:       0.0012
  Max final entropy:        0.0510
  Pseudo loss:              -0.12994
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.129
Episode (2440/5000) took 8.340 seconds.
  Mean final reward:        -0.1253
  Mean return:              -4.9811
  Mean final entropy:       0.0006
  Max final entropy:        0.0216
  Pseudo loss:              -0.09580
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.176
Episode (2450/5000) took 8.391 seconds.
  Mean final reward:        -0.1891
  Mean return:              -5.1138
  Mean final entropy:       0.0009
  Max final entropy:        0.0263
  Pseudo loss:              -0.22302
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.090
Episode (2460/5000) took 8.345 seconds.
  Mean final reward:        -0.1953
  Mean return:              -5.1869
  Mean final entropy:       0.0011
  Max final entropy:        0.0560
  Pseudo loss:              -0.12078
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.129
Episode (2470/5000) took 8.267 seconds.
  Mean final reward:        -0.1313
  Mean return:              -4.8251
  Mean final entropy:       0.0005
  Max final entropy:        0.0166
  Pseudo loss:              -0.15688
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.109
Episode (2480/5000) took 8.324 seconds.
  Mean final reward:        -0.1441
  Mean return:              -5.2005
  Mean final entropy:       0.0007
  Max final entropy:        0.0248
  Pseudo loss:              -0.21901
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.230
Episode (2490/5000) took 8.397 seconds.
  Mean final reward:        -0.1093
  Mean return:              -4.8518
  Mean final entropy:       0.0005
  Max final entropy:        0.0151
  Pseudo loss:              -0.10131
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.195
Episode (2500/5000) took 8.291 seconds.
  Mean final reward:        -0.1697
  Mean return:              -5.2116
  Mean final entropy:       0.0009
  Max final entropy:        0.0287
  Pseudo loss:              -0.26612
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.211
Episode (2510/5000) took 8.315 seconds.
  Mean final reward:        -0.1291
  Mean return:              -4.6225
  Mean final entropy:       0.0005
  Max final entropy:        0.0193
  Pseudo loss:              -0.09368
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.125
Episode (2520/5000) took 8.329 seconds.
  Mean final reward:        -0.2262
  Mean return:              -5.1024
  Mean final entropy:       0.0011
  Max final entropy:        0.0473
  Pseudo loss:              -0.22457
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.066
Episode (2530/5000) took 8.318 seconds.
  Mean final reward:        -0.1667
  Mean return:              -5.1755
  Mean final entropy:       0.0008
  Max final entropy:        0.0288
  Pseudo loss:              -0.10436
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.160
Episode (2540/5000) took 8.380 seconds.
  Mean final reward:        -0.1150
  Mean return:              -4.6610
  Mean final entropy:       0.0007
  Max final entropy:        0.0657
  Pseudo loss:              -0.20322
  Solved trajectories:      239 / 256
  Avg steps to disentangle: 2.227
Episode (2550/5000) took 8.301 seconds.
  Mean final reward:        -0.0799
  Mean return:              -4.7383
  Mean final entropy:       0.0004
  Max final entropy:        0.0303
  Pseudo loss:              -0.09538
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.246
Episode (2560/5000) took 8.351 seconds.
  Mean final reward:        -0.1666
  Mean return:              -4.9526
  Mean final entropy:       0.0008
  Max final entropy:        0.0384
  Pseudo loss:              -0.11914
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.219
Episode (2570/5000) took 8.340 seconds.
  Mean final reward:        -0.1697
  Mean return:              -4.9974
  Mean final entropy:       0.0012
  Max final entropy:        0.0788
  Pseudo loss:              -0.14575
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.164
Episode (2580/5000) took 8.386 seconds.
  Mean final reward:        -0.2314
  Mean return:              -5.0666
  Mean final entropy:       0.0010
  Max final entropy:        0.0280
  Pseudo loss:              -0.24675
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.090
Episode (2590/5000) took 8.294 seconds.
  Mean final reward:        -0.1844
  Mean return:              -4.9489
  Mean final entropy:       0.0008
  Max final entropy:        0.0329
  Pseudo loss:              -0.22790
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.117
Episode (2600/5000) took 8.377 seconds.
  Mean final reward:        -0.1720
  Mean return:              -5.5145
  Mean final entropy:       0.0012
  Max final entropy:        0.1021
  Pseudo loss:              -0.16279
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.152
Episode (2610/5000) took 8.323 seconds.
  Mean final reward:        -0.1422
  Mean return:              -5.0222
  Mean final entropy:       0.0007
  Max final entropy:        0.0352
  Pseudo loss:              -0.13121
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.168
Episode (2620/5000) took 8.397 seconds.
  Mean final reward:        -0.1815
  Mean return:              -5.3096
  Mean final entropy:       0.0010
  Max final entropy:        0.0355
  Pseudo loss:              -0.23084
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.148
Episode (2630/5000) took 8.362 seconds.
  Mean final reward:        -0.1462
  Mean return:              -4.8817
  Mean final entropy:       0.0008
  Max final entropy:        0.0446
  Pseudo loss:              -0.20219
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.180
Episode (2640/5000) took 8.297 seconds.
  Mean final reward:        -0.0995
  Mean return:              -4.7953
  Mean final entropy:       0.0005
  Max final entropy:        0.0263
  Pseudo loss:              -0.05244
  Solved trajectories:      238 / 256
  Avg steps to disentangle: 2.262
Episode (2650/5000) took 8.333 seconds.
  Mean final reward:        -0.1003
  Mean return:              -4.6598
  Mean final entropy:       0.0004
  Max final entropy:        0.0111
  Pseudo loss:              -0.06699
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.184
Episode (2660/5000) took 8.326 seconds.
  Mean final reward:        -0.1978
  Mean return:              -5.1503
  Mean final entropy:       0.0010
  Max final entropy:        0.0307
  Pseudo loss:              -0.17367
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.105
Episode (2670/5000) took 8.300 seconds.
  Mean final reward:        -0.1156
  Mean return:              -5.1291
  Mean final entropy:       0.0005
  Max final entropy:        0.0209
  Pseudo loss:              -0.15983
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.148
Episode (2680/5000) took 8.317 seconds.
  Mean final reward:        -0.1437
  Mean return:              -4.8439
  Mean final entropy:       0.0008
  Max final entropy:        0.0467
  Pseudo loss:              -0.25776
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.266
Episode (2690/5000) took 8.270 seconds.
  Mean final reward:        -0.1401
  Mean return:              -5.0943
  Mean final entropy:       0.0008
  Max final entropy:        0.0529
  Pseudo loss:              -0.19201
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.223
Episode (2700/5000) took 8.327 seconds.
  Mean final reward:        -0.1000
  Mean return:              -4.7867
  Mean final entropy:       0.0004
  Max final entropy:        0.0160
  Pseudo loss:              -0.09793
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.246
Episode (2710/5000) took 8.414 seconds.
  Mean final reward:        -0.0987
  Mean return:              -4.8910
  Mean final entropy:       0.0004
  Max final entropy:        0.0224
  Pseudo loss:              -0.08168
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.168
Episode (2720/5000) took 8.307 seconds.
  Mean final reward:        -0.1658
  Mean return:              -5.0442
  Mean final entropy:       0.0006
  Max final entropy:        0.0128
  Pseudo loss:              -0.12559
  Solved trajectories:      212 / 256
  Avg steps to disentangle: 2.051
Episode (2730/5000) took 8.327 seconds.
  Mean final reward:        -0.1227
  Mean return:              -4.9885
  Mean final entropy:       0.0005
  Max final entropy:        0.0134
  Pseudo loss:              -0.11781
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.188
Episode (2740/5000) took 8.314 seconds.
  Mean final reward:        -0.1519
  Mean return:              -5.0184
  Mean final entropy:       0.0010
  Max final entropy:        0.0792
  Pseudo loss:              -0.11248
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.203
Episode (2750/5000) took 8.360 seconds.
  Mean final reward:        -0.1816
  Mean return:              -5.1605
  Mean final entropy:       0.0010
  Max final entropy:        0.0583
  Pseudo loss:              -0.17342
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.141
Episode (2760/5000) took 8.404 seconds.
  Mean final reward:        -0.1810
  Mean return:              -5.2147
  Mean final entropy:       0.0008
  Max final entropy:        0.0213
  Pseudo loss:              -0.15202
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 2.145
Episode (2770/5000) took 8.413 seconds.
  Mean final reward:        -0.1564
  Mean return:              -4.6539
  Mean final entropy:       0.0007
  Max final entropy:        0.0142
  Pseudo loss:              -0.06487
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.102
Episode (2780/5000) took 8.366 seconds.
  Mean final reward:        -0.1094
  Mean return:              -5.0766
  Mean final entropy:       0.0005
  Max final entropy:        0.0167
  Pseudo loss:              -0.20640
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.215
Episode (2790/5000) took 8.357 seconds.
  Mean final reward:        -0.1449
  Mean return:              -4.8665
  Mean final entropy:       0.0008
  Max final entropy:        0.0339
  Pseudo loss:              -0.41549
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.188
Episode (2800/5000) took 8.327 seconds.
  Mean final reward:        -0.1126
  Mean return:              -4.9426
  Mean final entropy:       0.0007
  Max final entropy:        0.0470
  Pseudo loss:              -0.20452
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.180
Episode (2810/5000) took 8.340 seconds.
  Mean final reward:        -0.1384
  Mean return:              -4.8482
  Mean final entropy:       0.0006
  Max final entropy:        0.0214
  Pseudo loss:              -0.14419
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.195
Episode (2820/5000) took 8.326 seconds.
  Mean final reward:        -0.1197
  Mean return:              -5.2217
  Mean final entropy:       0.0005
  Max final entropy:        0.0197
  Pseudo loss:              -0.08305
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.219
Episode (2830/5000) took 8.390 seconds.
  Mean final reward:        -0.1762
  Mean return:              -5.0038
  Mean final entropy:       0.0008
  Max final entropy:        0.0285
  Pseudo loss:              -0.05553
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.129
Episode (2840/5000) took 8.330 seconds.
  Mean final reward:        -0.1320
  Mean return:              -5.0642
  Mean final entropy:       0.0005
  Max final entropy:        0.0094
  Pseudo loss:              -0.08452
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.211
Episode (2850/5000) took 8.366 seconds.
  Mean final reward:        -0.2336
  Mean return:              -5.1239
  Mean final entropy:       0.0015
  Max final entropy:        0.0736
  Pseudo loss:              -0.38904
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.109
Episode (2860/5000) took 8.345 seconds.
  Mean final reward:        -0.1771
  Mean return:              -4.9926
  Mean final entropy:       0.0009
  Max final entropy:        0.0355
  Pseudo loss:              -0.19991
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.199
Episode (2870/5000) took 8.416 seconds.
  Mean final reward:        -0.1493
  Mean return:              -5.0048
  Mean final entropy:       0.0008
  Max final entropy:        0.0244
  Pseudo loss:              -0.09852
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.250
Episode (2880/5000) took 8.351 seconds.
  Mean final reward:        -0.1760
  Mean return:              -5.3406
  Mean final entropy:       0.0009
  Max final entropy:        0.0483
  Pseudo loss:              -0.14615
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.230
Episode (2890/5000) took 8.326 seconds.
  Mean final reward:        -0.1555
  Mean return:              -4.9151
  Mean final entropy:       0.0007
  Max final entropy:        0.0269
  Pseudo loss:              -0.15551
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.184
Episode (2900/5000) took 8.380 seconds.
  Mean final reward:        -0.1844
  Mean return:              -5.2684
  Mean final entropy:       0.0011
  Max final entropy:        0.1077
  Pseudo loss:              -0.21985
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.164
Episode (2910/5000) took 8.327 seconds.
  Mean final reward:        -0.1272
  Mean return:              -4.8845
  Mean final entropy:       0.0006
  Max final entropy:        0.0242
  Pseudo loss:              -0.20886
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.113
Episode (2920/5000) took 8.424 seconds.
  Mean final reward:        -0.2055
  Mean return:              -5.2245
  Mean final entropy:       0.0016
  Max final entropy:        0.1799
  Pseudo loss:              -0.43614
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.117
Episode (2930/5000) took 8.412 seconds.
  Mean final reward:        -0.1319
  Mean return:              -5.0770
  Mean final entropy:       0.0006
  Max final entropy:        0.0193
  Pseudo loss:              -0.30557
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.176
Episode (2940/5000) took 8.371 seconds.
  Mean final reward:        -0.1426
  Mean return:              -4.7929
  Mean final entropy:       0.0007
  Max final entropy:        0.0224
  Pseudo loss:              -0.21529
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.148
Episode (2950/5000) took 8.321 seconds.
  Mean final reward:        -0.1117
  Mean return:              -5.0744
  Mean final entropy:       0.0006
  Max final entropy:        0.0350
  Pseudo loss:              -0.14785
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.273
Episode (2960/5000) took 8.421 seconds.
  Mean final reward:        -0.1424
  Mean return:              -5.0769
  Mean final entropy:       0.0007
  Max final entropy:        0.0424
  Pseudo loss:              -0.16094
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.242
Episode (2970/5000) took 8.399 seconds.
  Mean final reward:        -0.1425
  Mean return:              -4.8938
  Mean final entropy:       0.0006
  Max final entropy:        0.0173
  Pseudo loss:              -0.06673
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.180
Episode (2980/5000) took 8.376 seconds.
  Mean final reward:        -0.1565
  Mean return:              -5.0652
  Mean final entropy:       0.0009
  Max final entropy:        0.0399
  Pseudo loss:              -0.26645
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.219
Episode (2990/5000) took 8.526 seconds.
  Mean final reward:        -0.1269
  Mean return:              -4.6858
  Mean final entropy:       0.0007
  Max final entropy:        0.0319
  Pseudo loss:              -0.06277
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.203
Episode (3000/5000) took 8.387 seconds.
  Mean final reward:        -0.1852
  Mean return:              -5.0674
  Mean final entropy:       0.0010
  Max final entropy:        0.0350
  Pseudo loss:              -0.22775
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.180
Episode (3010/5000) took 8.451 seconds.
  Mean final reward:        -0.1199
  Mean return:              -4.7889
  Mean final entropy:       0.0005
  Max final entropy:        0.0261
  Pseudo loss:              -0.11275
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.133
Episode (3020/5000) took 8.438 seconds.
  Mean final reward:        -0.0538
  Mean return:              -4.5884
  Mean final entropy:       0.0003
  Max final entropy:        0.0058
  Pseudo loss:              -0.08451
  Solved trajectories:      237 / 256
  Avg steps to disentangle: 2.230
Episode (3030/5000) took 8.304 seconds.
  Mean final reward:        -0.2092
  Mean return:              -5.2447
  Mean final entropy:       0.0013
  Max final entropy:        0.0569
  Pseudo loss:              -0.14562
  Solved trajectories:      217 / 256
  Avg steps to disentangle: 2.125
Episode (3040/5000) took 8.399 seconds.
  Mean final reward:        -0.1871
  Mean return:              -5.2901
  Mean final entropy:       0.0012
  Max final entropy:        0.0636
  Pseudo loss:              -0.30005
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.176
Episode (3050/5000) took 8.374 seconds.
  Mean final reward:        -0.1580
  Mean return:              -5.2063
  Mean final entropy:       0.0009
  Max final entropy:        0.0457
  Pseudo loss:              -0.21370
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.148
Episode (3060/5000) took 8.394 seconds.
  Mean final reward:        -0.1255
  Mean return:              -4.9002
  Mean final entropy:       0.0005
  Max final entropy:        0.0110
  Pseudo loss:              -0.20214
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.137
Episode (3070/5000) took 8.376 seconds.
  Mean final reward:        -0.1396
  Mean return:              -4.8126
  Mean final entropy:       0.0007
  Max final entropy:        0.0248
  Pseudo loss:              -0.21731
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.195
Episode (3080/5000) took 8.393 seconds.
  Mean final reward:        -0.1201
  Mean return:              -4.8530
  Mean final entropy:       0.0005
  Max final entropy:        0.0146
  Pseudo loss:              -0.06390
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.156
Episode (3090/5000) took 8.383 seconds.
  Mean final reward:        -0.0807
  Mean return:              -4.6773
  Mean final entropy:       0.0004
  Max final entropy:        0.0168
  Pseudo loss:              -0.12045
  Solved trajectories:      238 / 256
  Avg steps to disentangle: 2.242
Episode (3100/5000) took 8.324 seconds.
  Mean final reward:        -0.1353
  Mean return:              -4.8481
  Mean final entropy:       0.0006
  Max final entropy:        0.0297
  Pseudo loss:              -0.12181
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.148
Episode (3110/5000) took 8.389 seconds.
  Mean final reward:        -0.1032
  Mean return:              -4.9603
  Mean final entropy:       0.0005
  Max final entropy:        0.0208
  Pseudo loss:              -0.19175
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.254
Episode (3120/5000) took 8.435 seconds.
  Mean final reward:        -0.1598
  Mean return:              -5.0629
  Mean final entropy:       0.0007
  Max final entropy:        0.0267
  Pseudo loss:              -0.12301
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.203
Episode (3130/5000) took 8.395 seconds.
  Mean final reward:        -0.1567
  Mean return:              -5.2487
  Mean final entropy:       0.0007
  Max final entropy:        0.0236
  Pseudo loss:              -0.10590
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.145
Episode (3140/5000) took 8.342 seconds.
  Mean final reward:        -0.2120
  Mean return:              -5.2551
  Mean final entropy:       0.0013
  Max final entropy:        0.0672
  Pseudo loss:              -0.22246
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.168
Episode (3150/5000) took 8.323 seconds.
  Mean final reward:        -0.1416
  Mean return:              -5.0125
  Mean final entropy:       0.0006
  Max final entropy:        0.0124
  Pseudo loss:              -0.09518
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.156
Episode (3160/5000) took 8.353 seconds.
  Mean final reward:        -0.1512
  Mean return:              -5.1632
  Mean final entropy:       0.0006
  Max final entropy:        0.0154
  Pseudo loss:              -0.19782
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.145
Episode (3170/5000) took 8.407 seconds.
  Mean final reward:        -0.2018
  Mean return:              -5.2206
  Mean final entropy:       0.0010
  Max final entropy:        0.0472
  Pseudo loss:              -0.14000
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.188
Episode (3180/5000) took 8.354 seconds.
  Mean final reward:        -0.1413
  Mean return:              -5.1748
  Mean final entropy:       0.0007
  Max final entropy:        0.0290
  Pseudo loss:              -0.09646
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.234
Episode (3190/5000) took 8.400 seconds.
  Mean final reward:        -0.0923
  Mean return:              -4.8342
  Mean final entropy:       0.0004
  Max final entropy:        0.0208
  Pseudo loss:              -0.11570
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.242
Episode (3200/5000) took 8.398 seconds.
  Mean final reward:        -0.1918
  Mean return:              -5.0018
  Mean final entropy:       0.0010
  Max final entropy:        0.0739
  Pseudo loss:              -0.15273
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.117
Episode (3210/5000) took 8.357 seconds.
  Mean final reward:        -0.1594
  Mean return:              -5.1619
  Mean final entropy:       0.0008
  Max final entropy:        0.0403
  Pseudo loss:              -0.24381
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.219
Episode (3220/5000) took 8.311 seconds.
  Mean final reward:        -0.0996
  Mean return:              -4.9632
  Mean final entropy:       0.0005
  Max final entropy:        0.0181
  Pseudo loss:              -0.03130
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.273
Episode (3230/5000) took 8.308 seconds.
  Mean final reward:        -0.1917
  Mean return:              -5.0653
  Mean final entropy:       0.0011
  Max final entropy:        0.0515
  Pseudo loss:              -0.32123
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.156
Episode (3240/5000) took 8.269 seconds.
  Mean final reward:        -0.1381
  Mean return:              -5.1336
  Mean final entropy:       0.0007
  Max final entropy:        0.0232
  Pseudo loss:              -0.14931
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.219
Episode (3250/5000) took 8.303 seconds.
  Mean final reward:        -0.2200
  Mean return:              -5.2753
  Mean final entropy:       0.0011
  Max final entropy:        0.0350
  Pseudo loss:              -0.23342
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.152
Episode (3260/5000) took 8.419 seconds.
  Mean final reward:        -0.1088
  Mean return:              -4.6671
  Mean final entropy:       0.0005
  Max final entropy:        0.0215
  Pseudo loss:              -0.20194
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.164
Episode (3270/5000) took 8.334 seconds.
  Mean final reward:        -0.1065
  Mean return:              -4.8860
  Mean final entropy:       0.0006
  Max final entropy:        0.0478
  Pseudo loss:              -0.11838
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.195
Episode (3280/5000) took 8.366 seconds.
  Mean final reward:        -0.1223
  Mean return:              -4.5651
  Mean final entropy:       0.0006
  Max final entropy:        0.0225
  Pseudo loss:              -0.10462
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.121
Episode (3290/5000) took 8.335 seconds.
  Mean final reward:        -0.1122
  Mean return:              -4.9023
  Mean final entropy:       0.0006
  Max final entropy:        0.0228
  Pseudo loss:              -0.07862
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.207
Episode (3300/5000) took 8.374 seconds.
  Mean final reward:        -0.1099
  Mean return:              -4.7639
  Mean final entropy:       0.0006
  Max final entropy:        0.0229
  Pseudo loss:              -0.15808
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.199
Episode (3310/5000) took 8.357 seconds.
  Mean final reward:        -0.1201
  Mean return:              -4.8151
  Mean final entropy:       0.0005
  Max final entropy:        0.0223
  Pseudo loss:              -0.07122
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.234
Episode (3320/5000) took 8.322 seconds.
  Mean final reward:        -0.1300
  Mean return:              -4.8743
  Mean final entropy:       0.0006
  Max final entropy:        0.0180
  Pseudo loss:              -0.09545
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.168
Episode (3330/5000) took 8.332 seconds.
  Mean final reward:        -0.1675
  Mean return:              -5.0205
  Mean final entropy:       0.0011
  Max final entropy:        0.1163
  Pseudo loss:              -0.18052
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.145
Episode (3340/5000) took 8.417 seconds.
  Mean final reward:        -0.0751
  Mean return:              -4.6581
  Mean final entropy:       0.0005
  Max final entropy:        0.0514
  Pseudo loss:              -0.05564
  Solved trajectories:      239 / 256
  Avg steps to disentangle: 2.230
Episode (3350/5000) took 8.375 seconds.
  Mean final reward:        -0.1888
  Mean return:              -5.0792
  Mean final entropy:       0.0009
  Max final entropy:        0.0342
  Pseudo loss:              -0.32804
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.180
Episode (3360/5000) took 8.368 seconds.
  Mean final reward:        -0.1323
  Mean return:              -4.9731
  Mean final entropy:       0.0007
  Max final entropy:        0.0548
  Pseudo loss:              -0.30994
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.168
Episode (3370/5000) took 8.361 seconds.
  Mean final reward:        -0.0657
  Mean return:              -4.5359
  Mean final entropy:       0.0003
  Max final entropy:        0.0104
  Pseudo loss:              -0.05182
  Solved trajectories:      236 / 256
  Avg steps to disentangle: 2.191
Episode (3380/5000) took 8.386 seconds.
  Mean final reward:        -0.1470
  Mean return:              -5.2961
  Mean final entropy:       0.0007
  Max final entropy:        0.0236
  Pseudo loss:              -0.16440
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.242
Episode (3390/5000) took 8.324 seconds.
  Mean final reward:        -0.1323
  Mean return:              -4.7752
  Mean final entropy:       0.0006
  Max final entropy:        0.0267
  Pseudo loss:              -0.14228
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.133
Episode (3400/5000) took 8.347 seconds.
  Mean final reward:        -0.1893
  Mean return:              -5.1063
  Mean final entropy:       0.0008
  Max final entropy:        0.0175
  Pseudo loss:              -0.23983
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 2.133
Episode (3410/5000) took 8.348 seconds.
  Mean final reward:        -0.0837
  Mean return:              -4.4760
  Mean final entropy:       0.0005
  Max final entropy:        0.0483
  Pseudo loss:              -0.32019
  Solved trajectories:      236 / 256
  Avg steps to disentangle: 2.191
Episode (3420/5000) took 8.380 seconds.
  Mean final reward:        -0.0843
  Mean return:              -4.6945
  Mean final entropy:       0.0004
  Max final entropy:        0.0115
  Pseudo loss:              -0.11169
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.168
Episode (3430/5000) took 8.355 seconds.
  Mean final reward:        -0.1319
  Mean return:              -4.8014
  Mean final entropy:       0.0009
  Max final entropy:        0.0800
  Pseudo loss:              -0.32824
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.188
Episode (3440/5000) took 8.308 seconds.
  Mean final reward:        -0.1536
  Mean return:              -5.1533
  Mean final entropy:       0.0007
  Max final entropy:        0.0228
  Pseudo loss:              -0.14581
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.094
Episode (3450/5000) took 8.393 seconds.
  Mean final reward:        -0.1665
  Mean return:              -5.2019
  Mean final entropy:       0.0009
  Max final entropy:        0.0656
  Pseudo loss:              -0.17072
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.191
Episode (3460/5000) took 8.386 seconds.
  Mean final reward:        -0.2018
  Mean return:              -5.0958
  Mean final entropy:       0.0009
  Max final entropy:        0.0260
  Pseudo loss:              -0.20922
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.160
Episode (3470/5000) took 8.353 seconds.
  Mean final reward:        -0.1290
  Mean return:              -4.8342
  Mean final entropy:       0.0006
  Max final entropy:        0.0262
  Pseudo loss:              -0.08049
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.172
Episode (3480/5000) took 8.296 seconds.
  Mean final reward:        -0.1612
  Mean return:              -5.1286
  Mean final entropy:       0.0007
  Max final entropy:        0.0160
  Pseudo loss:              -0.03283
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.137
Episode (3490/5000) took 8.422 seconds.
  Mean final reward:        -0.1504
  Mean return:              -4.5745
  Mean final entropy:       0.0008
  Max final entropy:        0.0441
  Pseudo loss:              -0.14299
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.102
Episode (3500/5000) took 8.385 seconds.
  Mean final reward:        -0.2345
  Mean return:              -5.3405
  Mean final entropy:       0.0011
  Max final entropy:        0.0278
  Pseudo loss:              -0.18646
  Solved trajectories:      213 / 256
  Avg steps to disentangle: 2.121
Episode (3510/5000) took 8.359 seconds.
  Mean final reward:        -0.1505
  Mean return:              -5.2047
  Mean final entropy:       0.0008
  Max final entropy:        0.0241
  Pseudo loss:              -0.10648
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.254
Episode (3520/5000) took 8.314 seconds.
  Mean final reward:        -0.1555
  Mean return:              -5.1698
  Mean final entropy:       0.0008
  Max final entropy:        0.0535
  Pseudo loss:              -0.23806
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.223
Episode (3530/5000) took 8.430 seconds.
  Mean final reward:        -0.1335
  Mean return:              -4.6647
  Mean final entropy:       0.0006
  Max final entropy:        0.0164
  Pseudo loss:              -0.10051
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.121
Episode (3540/5000) took 8.381 seconds.
  Mean final reward:        -0.1492
  Mean return:              -5.1045
  Mean final entropy:       0.0007
  Max final entropy:        0.0199
  Pseudo loss:              -0.11597
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.223
Episode (3550/5000) took 8.337 seconds.
  Mean final reward:        -0.1188
  Mean return:              -4.8488
  Mean final entropy:       0.0006
  Max final entropy:        0.0201
  Pseudo loss:              -0.13856
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.184
Episode (3560/5000) took 8.434 seconds.
  Mean final reward:        -0.1659
  Mean return:              -4.9707
  Mean final entropy:       0.0008
  Max final entropy:        0.0323
  Pseudo loss:              -0.12475
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.172
Episode (3570/5000) took 8.289 seconds.
  Mean final reward:        -0.1278
  Mean return:              -4.7021
  Mean final entropy:       0.0005
  Max final entropy:        0.0106
  Pseudo loss:              -0.10828
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.160
Episode (3580/5000) took 8.381 seconds.
  Mean final reward:        -0.1685
  Mean return:              -5.1729
  Mean final entropy:       0.0008
  Max final entropy:        0.0182
  Pseudo loss:              -0.18877
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.121
Episode (3590/5000) took 8.372 seconds.
  Mean final reward:        -0.1300
  Mean return:              -4.9384
  Mean final entropy:       0.0005
  Max final entropy:        0.0179
  Pseudo loss:              -0.14577
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.207
Episode (3600/5000) took 8.331 seconds.
  Mean final reward:        -0.1542
  Mean return:              -4.9530
  Mean final entropy:       0.0008
  Max final entropy:        0.0404
  Pseudo loss:              -0.18859
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.188
Episode (3610/5000) took 8.378 seconds.
  Mean final reward:        -0.1384
  Mean return:              -4.9360
  Mean final entropy:       0.0008
  Max final entropy:        0.0436
  Pseudo loss:              -0.13052
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.219
Episode (3620/5000) took 8.383 seconds.
  Mean final reward:        -0.1565
  Mean return:              -5.1370
  Mean final entropy:       0.0014
  Max final entropy:        0.2054
  Pseudo loss:              -0.28691
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.156
Episode (3630/5000) took 8.356 seconds.
  Mean final reward:        -0.1419
  Mean return:              -4.9648
  Mean final entropy:       0.0006
  Max final entropy:        0.0156
  Pseudo loss:              -0.18188
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.141
Episode (3640/5000) took 8.361 seconds.
  Mean final reward:        -0.0885
  Mean return:              -4.7923
  Mean final entropy:       0.0006
  Max final entropy:        0.0665
  Pseudo loss:              -0.22947
  Solved trajectories:      236 / 256
  Avg steps to disentangle: 2.199
Episode (3650/5000) took 8.368 seconds.
  Mean final reward:        -0.1317
  Mean return:              -4.9511
  Mean final entropy:       0.0005
  Max final entropy:        0.0190
  Pseudo loss:              -0.04225
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.199
Episode (3660/5000) took 8.411 seconds.
  Mean final reward:        -0.1183
  Mean return:              -4.7925
  Mean final entropy:       0.0007
  Max final entropy:        0.0546
  Pseudo loss:              -0.14266
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.191
Episode (3670/5000) took 8.295 seconds.
  Mean final reward:        -0.1644
  Mean return:              -4.9866
  Mean final entropy:       0.0007
  Max final entropy:        0.0293
  Pseudo loss:              -0.12959
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.172
Episode (3680/5000) took 8.331 seconds.
  Mean final reward:        -0.1171
  Mean return:              -4.7299
  Mean final entropy:       0.0005
  Max final entropy:        0.0131
  Pseudo loss:              -0.07275
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.168
Episode (3690/5000) took 8.369 seconds.
  Mean final reward:        -0.1325
  Mean return:              -4.9640
  Mean final entropy:       0.0007
  Max final entropy:        0.0309
  Pseudo loss:              -0.28445
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.176
Episode (3700/5000) took 8.331 seconds.
  Mean final reward:        -0.1474
  Mean return:              -5.1520
  Mean final entropy:       0.0006
  Max final entropy:        0.0141
  Pseudo loss:              -0.21258
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.164
Episode (3710/5000) took 8.367 seconds.
  Mean final reward:        -0.1852
  Mean return:              -5.1094
  Mean final entropy:       0.0008
  Max final entropy:        0.0359
  Pseudo loss:              -0.06869
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.133
Episode (3720/5000) took 8.350 seconds.
  Mean final reward:        -0.1781
  Mean return:              -5.1672
  Mean final entropy:       0.0007
  Max final entropy:        0.0138
  Pseudo loss:              -0.11295
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.145
Episode (3730/5000) took 8.374 seconds.
  Mean final reward:        -0.1172
  Mean return:              -4.8872
  Mean final entropy:       0.0005
  Max final entropy:        0.0095
  Pseudo loss:              -0.10387
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.168
Episode (3740/5000) took 8.286 seconds.
  Mean final reward:        -0.1198
  Mean return:              -5.1352
  Mean final entropy:       0.0008
  Max final entropy:        0.0457
  Pseudo loss:              -0.22497
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.242
Episode (3750/5000) took 8.335 seconds.
  Mean final reward:        -0.1750
  Mean return:              -5.2550
  Mean final entropy:       0.0009
  Max final entropy:        0.0298
  Pseudo loss:              -0.05252
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.227
Episode (3760/5000) took 8.326 seconds.
  Mean final reward:        -0.1584
  Mean return:              -5.0517
  Mean final entropy:       0.0007
  Max final entropy:        0.0398
  Pseudo loss:              -0.11913
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.195
Episode (3770/5000) took 8.304 seconds.
  Mean final reward:        -0.1009
  Mean return:              -4.8560
  Mean final entropy:       0.0005
  Max final entropy:        0.0208
  Pseudo loss:              -0.09513
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.242
Episode (3780/5000) took 8.343 seconds.
  Mean final reward:        -0.1371
  Mean return:              -5.1946
  Mean final entropy:       0.0008
  Max final entropy:        0.0488
  Pseudo loss:              -0.21605
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.203
Episode (3790/5000) took 8.397 seconds.
  Mean final reward:        -0.1894
  Mean return:              -4.9930
  Mean final entropy:       0.0009
  Max final entropy:        0.0258
  Pseudo loss:              -0.19104
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.148
Episode (3800/5000) took 8.426 seconds.
  Mean final reward:        -0.1265
  Mean return:              -4.8898
  Mean final entropy:       0.0006
  Max final entropy:        0.0220
  Pseudo loss:              -0.09435
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.199
Episode (3810/5000) took 8.519 seconds.
  Mean final reward:        -0.1641
  Mean return:              -5.0460
  Mean final entropy:       0.0007
  Max final entropy:        0.0195
  Pseudo loss:              -0.13565
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.211
Episode (3820/5000) took 8.385 seconds.
  Mean final reward:        -0.1529
  Mean return:              -5.1781
  Mean final entropy:       0.0008
  Max final entropy:        0.0400
  Pseudo loss:              -0.23216
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.223
Episode (3830/5000) took 8.399 seconds.
  Mean final reward:        -0.2079
  Mean return:              -5.2331
  Mean final entropy:       0.0011
  Max final entropy:        0.0512
  Pseudo loss:              -0.14716
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.168
Episode (3840/5000) took 8.297 seconds.
  Mean final reward:        -0.1603
  Mean return:              -4.7926
  Mean final entropy:       0.0008
  Max final entropy:        0.0206
  Pseudo loss:              -0.18823
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.094
Episode (3850/5000) took 8.431 seconds.
  Mean final reward:        -0.1884
  Mean return:              -5.2006
  Mean final entropy:       0.0010
  Max final entropy:        0.0385
  Pseudo loss:              -0.43261
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.246
Episode (3860/5000) took 8.404 seconds.
  Mean final reward:        -0.1844
  Mean return:              -5.2590
  Mean final entropy:       0.0010
  Max final entropy:        0.0359
  Pseudo loss:              -0.19036
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.168
Episode (3870/5000) took 8.354 seconds.
  Mean final reward:        -0.0871
  Mean return:              -4.9269
  Mean final entropy:       0.0004
  Max final entropy:        0.0150
  Pseudo loss:              -0.06958
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.203
Episode (3880/5000) took 8.302 seconds.
  Mean final reward:        -0.1479
  Mean return:              -5.0686
  Mean final entropy:       0.0006
  Max final entropy:        0.0207
  Pseudo loss:              -0.13208
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.094
Episode (3890/5000) took 8.376 seconds.
  Mean final reward:        -0.1653
  Mean return:              -5.1009
  Mean final entropy:       0.0007
  Max final entropy:        0.0180
  Pseudo loss:              -0.11652
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.184
Episode (3900/5000) took 8.358 seconds.
  Mean final reward:        -0.1487
  Mean return:              -4.8401
  Mean final entropy:       0.0009
  Max final entropy:        0.0566
  Pseudo loss:              -0.25735
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.137
Episode (3910/5000) took 8.383 seconds.
  Mean final reward:        -0.1703
  Mean return:              -4.9572
  Mean final entropy:       0.0008
  Max final entropy:        0.0282
  Pseudo loss:              -0.19129
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.113
Episode (3920/5000) took 8.312 seconds.
  Mean final reward:        -0.1101
  Mean return:              -4.5199
  Mean final entropy:       0.0005
  Max final entropy:        0.0187
  Pseudo loss:              -0.07281
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.172
Episode (3930/5000) took 8.390 seconds.
  Mean final reward:        -0.1282
  Mean return:              -5.2579
  Mean final entropy:       0.0006
  Max final entropy:        0.0167
  Pseudo loss:              -0.09621
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.258
Episode (3940/5000) took 8.347 seconds.
  Mean final reward:        -0.1103
  Mean return:              -4.7248
  Mean final entropy:       0.0006
  Max final entropy:        0.0281
  Pseudo loss:              -0.03555
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.188
Episode (3950/5000) took 8.307 seconds.
  Mean final reward:        -0.1371
  Mean return:              -4.9935
  Mean final entropy:       0.0008
  Max final entropy:        0.0573
  Pseudo loss:              -0.11904
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.184
Episode (3960/5000) took 8.385 seconds.
  Mean final reward:        -0.1119
  Mean return:              -4.8695
  Mean final entropy:       0.0004
  Max final entropy:        0.0097
  Pseudo loss:              -0.12540
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.215
Episode (3970/5000) took 8.369 seconds.
  Mean final reward:        -0.0819
  Mean return:              -4.7057
  Mean final entropy:       0.0004
  Max final entropy:        0.0093
  Pseudo loss:              -0.08436
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.191
Episode (3980/5000) took 8.271 seconds.
  Mean final reward:        -0.0556
  Mean return:              -4.7123
  Mean final entropy:       0.0003
  Max final entropy:        0.0130
  Pseudo loss:              -0.07063
  Solved trajectories:      243 / 256
  Avg steps to disentangle: 2.281
Episode (3990/5000) took 8.342 seconds.
  Mean final reward:        -0.1322
  Mean return:              -4.9539
  Mean final entropy:       0.0006
  Max final entropy:        0.0224
  Pseudo loss:              -0.21680
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.191
Episode (4000/5000) took 8.357 seconds.
  Mean final reward:        -0.1308
  Mean return:              -4.7526
  Mean final entropy:       0.0007
  Max final entropy:        0.0338
  Pseudo loss:              -0.15343
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.203
Episode (4010/5000) took 8.404 seconds.
  Mean final reward:        -0.1810
  Mean return:              -5.0567
  Mean final entropy:       0.0010
  Max final entropy:        0.0356
  Pseudo loss:              -0.10415
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.152
Episode (4020/5000) took 8.323 seconds.
  Mean final reward:        -0.1049
  Mean return:              -4.9460
  Mean final entropy:       0.0006
  Max final entropy:        0.0486
  Pseudo loss:              -0.22899
  Solved trajectories:      234 / 256
  Avg steps to disentangle: 2.246
Episode (4030/5000) took 8.413 seconds.
  Mean final reward:        -0.1257
  Mean return:              -5.0462
  Mean final entropy:       0.0008
  Max final entropy:        0.0699
  Pseudo loss:              -0.17812
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.227
Episode (4040/5000) took 8.331 seconds.
  Mean final reward:        -0.1649
  Mean return:              -5.3341
  Mean final entropy:       0.0009
  Max final entropy:        0.0322
  Pseudo loss:              -0.20017
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.203
Episode (4050/5000) took 8.359 seconds.
  Mean final reward:        -0.1044
  Mean return:              -4.8071
  Mean final entropy:       0.0005
  Max final entropy:        0.0190
  Pseudo loss:              -0.04865
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.250
Episode (4060/5000) took 8.314 seconds.
  Mean final reward:        -0.2572
  Mean return:              -5.3880
  Mean final entropy:       0.0014
  Max final entropy:        0.0485
  Pseudo loss:              -0.47743
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.133
Episode (4070/5000) took 8.329 seconds.
  Mean final reward:        -0.0958
  Mean return:              -4.8281
  Mean final entropy:       0.0004
  Max final entropy:        0.0180
  Pseudo loss:              -0.09891
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.242
Episode (4080/5000) took 8.309 seconds.
  Mean final reward:        -0.2036
  Mean return:              -5.3715
  Mean final entropy:       0.0010
  Max final entropy:        0.0312
  Pseudo loss:              -0.24214
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.164
Episode (4090/5000) took 8.300 seconds.
  Mean final reward:        -0.1552
  Mean return:              -5.3230
  Mean final entropy:       0.0007
  Max final entropy:        0.0140
  Pseudo loss:              -0.03774
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.184
Episode (4100/5000) took 8.329 seconds.
  Mean final reward:        -0.1269
  Mean return:              -5.0846
  Mean final entropy:       0.0007
  Max final entropy:        0.0596
  Pseudo loss:              -0.13254
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.195
Episode (4110/5000) took 8.376 seconds.
  Mean final reward:        -0.1045
  Mean return:              -4.7884
  Mean final entropy:       0.0005
  Max final entropy:        0.0308
  Pseudo loss:              -0.06230
  Solved trajectories:      235 / 256
  Avg steps to disentangle: 2.250
Episode (4120/5000) took 8.290 seconds.
  Mean final reward:        -0.2126
  Mean return:              -5.2247
  Mean final entropy:       0.0013
  Max final entropy:        0.0567
  Pseudo loss:              -0.37874
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.184
Episode (4130/5000) took 8.326 seconds.
  Mean final reward:        -0.1820
  Mean return:              -5.1563
  Mean final entropy:       0.0009
  Max final entropy:        0.0252
  Pseudo loss:              -0.19954
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.156
Episode (4140/5000) took 8.359 seconds.
  Mean final reward:        -0.1729
  Mean return:              -5.1189
  Mean final entropy:       0.0009
  Max final entropy:        0.0327
  Pseudo loss:              -0.17518
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.160
Episode (4150/5000) took 8.397 seconds.
  Mean final reward:        -0.1347
  Mean return:              -5.1104
  Mean final entropy:       0.0007
  Max final entropy:        0.0335
  Pseudo loss:              -0.19328
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.254
Episode (4160/5000) took 8.400 seconds.
  Mean final reward:        -0.1421
  Mean return:              -4.7728
  Mean final entropy:       0.0006
  Max final entropy:        0.0208
  Pseudo loss:              -0.14374
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.168
Episode (4170/5000) took 8.366 seconds.
  Mean final reward:        -0.2037
  Mean return:              -5.5187
  Mean final entropy:       0.0010
  Max final entropy:        0.0497
  Pseudo loss:              -0.19979
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.199
Episode (4180/5000) took 8.315 seconds.
  Mean final reward:        -0.1336
  Mean return:              -4.9278
  Mean final entropy:       0.0006
  Max final entropy:        0.0208
  Pseudo loss:              -0.07416
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.195
Episode (4190/5000) took 8.759 seconds.
  Mean final reward:        -0.1903
  Mean return:              -5.2307
  Mean final entropy:       0.0011
  Max final entropy:        0.0585
  Pseudo loss:              -0.13755
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.188
Episode (4200/5000) took 8.395 seconds.
  Mean final reward:        -0.1702
  Mean return:              -4.9871
  Mean final entropy:       0.0009
  Max final entropy:        0.0300
  Pseudo loss:              -0.27759
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.230
Episode (4210/5000) took 8.370 seconds.
  Mean final reward:        -0.1482
  Mean return:              -4.8572
  Mean final entropy:       0.0006
  Max final entropy:        0.0157
  Pseudo loss:              -0.02844
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.152
Episode (4220/5000) took 8.305 seconds.
  Mean final reward:        -0.1349
  Mean return:              -5.2481
  Mean final entropy:       0.0006
  Max final entropy:        0.0343
  Pseudo loss:              -0.09300
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.215
Episode (4230/5000) took 8.444 seconds.
  Mean final reward:        -0.1609
  Mean return:              -5.0240
  Mean final entropy:       0.0006
  Max final entropy:        0.0104
  Pseudo loss:              -0.05929
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.117
Episode (4240/5000) took 8.365 seconds.
  Mean final reward:        -0.2014
  Mean return:              -5.1399
  Mean final entropy:       0.0010
  Max final entropy:        0.0376
  Pseudo loss:              -0.29777
  Solved trajectories:      216 / 256
  Avg steps to disentangle: 2.129
Episode (4250/5000) took 8.378 seconds.
  Mean final reward:        -0.1471
  Mean return:              -5.1348
  Mean final entropy:       0.0011
  Max final entropy:        0.1139
  Pseudo loss:              -0.29579
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.188
Episode (4260/5000) took 8.433 seconds.
  Mean final reward:        -0.1819
  Mean return:              -5.1755
  Mean final entropy:       0.0010
  Max final entropy:        0.0628
  Pseudo loss:              -0.13545
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.141
Episode (4270/5000) took 8.401 seconds.
  Mean final reward:        -0.0967
  Mean return:              -4.7842
  Mean final entropy:       0.0005
  Max final entropy:        0.0354
  Pseudo loss:              -0.09492
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.215
Episode (4280/5000) took 8.298 seconds.
  Mean final reward:        -0.1177
  Mean return:              -4.7908
  Mean final entropy:       0.0006
  Max final entropy:        0.0237
  Pseudo loss:              -0.19967
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.219
Episode (4290/5000) took 8.355 seconds.
  Mean final reward:        -0.1125
  Mean return:              -4.7417
  Mean final entropy:       0.0006
  Max final entropy:        0.0340
  Pseudo loss:              -0.14368
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.188
Episode (4300/5000) took 8.316 seconds.
  Mean final reward:        -0.1066
  Mean return:              -4.7333
  Mean final entropy:       0.0007
  Max final entropy:        0.0673
  Pseudo loss:              -0.10876
  Solved trajectories:      234 / 256
  Avg steps to disentangle: 2.152
Episode (4310/5000) took 8.387 seconds.
  Mean final reward:        -0.1825
  Mean return:              -5.1952
  Mean final entropy:       0.0008
  Max final entropy:        0.0296
  Pseudo loss:              -0.20747
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.145
Episode (4320/5000) took 8.411 seconds.
  Mean final reward:        -0.1474
  Mean return:              -4.8437
  Mean final entropy:       0.0007
  Max final entropy:        0.0267
  Pseudo loss:              -0.11716
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.199
Episode (4330/5000) took 8.358 seconds.
  Mean final reward:        -0.1425
  Mean return:              -4.8757
  Mean final entropy:       0.0007
  Max final entropy:        0.0308
  Pseudo loss:              -0.24510
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.188
Episode (4340/5000) took 8.372 seconds.
  Mean final reward:        -0.1426
  Mean return:              -5.1322
  Mean final entropy:       0.0006
  Max final entropy:        0.0230
  Pseudo loss:              -0.15828
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.188
Episode (4350/5000) took 8.311 seconds.
  Mean final reward:        -0.1798
  Mean return:              -5.1799
  Mean final entropy:       0.0008
  Max final entropy:        0.0265
  Pseudo loss:              -0.13330
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.199
Episode (4360/5000) took 8.398 seconds.
  Mean final reward:        -0.0975
  Mean return:              -4.6619
  Mean final entropy:       0.0004
  Max final entropy:        0.0097
  Pseudo loss:              -0.06905
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.199
Episode (4370/5000) took 8.411 seconds.
  Mean final reward:        -0.1604
  Mean return:              -4.8856
  Mean final entropy:       0.0011
  Max final entropy:        0.0545
  Pseudo loss:              -0.22159
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.184
Episode (4380/5000) took 8.371 seconds.
  Mean final reward:        -0.1092
  Mean return:              -4.8272
  Mean final entropy:       0.0006
  Max final entropy:        0.0458
  Pseudo loss:              -0.10601
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.160
Episode (4390/5000) took 8.333 seconds.
  Mean final reward:        -0.1028
  Mean return:              -4.8150
  Mean final entropy:       0.0004
  Max final entropy:        0.0116
  Pseudo loss:              -0.08874
  Solved trajectories:      234 / 256
  Avg steps to disentangle: 2.199
Episode (4400/5000) took 8.303 seconds.
  Mean final reward:        -0.1409
  Mean return:              -4.8993
  Mean final entropy:       0.0006
  Max final entropy:        0.0172
  Pseudo loss:              -0.04968
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.102
Episode (4410/5000) took 8.342 seconds.
  Mean final reward:        -0.1344
  Mean return:              -5.0289
  Mean final entropy:       0.0005
  Max final entropy:        0.0176
  Pseudo loss:              -0.13392
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.188
Episode (4420/5000) took 8.374 seconds.
  Mean final reward:        -0.1181
  Mean return:              -4.5459
  Mean final entropy:       0.0006
  Max final entropy:        0.0302
  Pseudo loss:              -0.17680
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.145
Episode (4430/5000) took 8.345 seconds.
  Mean final reward:        -0.1368
  Mean return:              -4.8942
  Mean final entropy:       0.0006
  Max final entropy:        0.0213
  Pseudo loss:              -0.09168
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.199
Episode (4440/5000) took 8.317 seconds.
  Mean final reward:        -0.1377
  Mean return:              -4.9114
  Mean final entropy:       0.0006
  Max final entropy:        0.0162
  Pseudo loss:              -0.13135
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.160
Episode (4450/5000) took 8.282 seconds.
  Mean final reward:        -0.1324
  Mean return:              -5.0070
  Mean final entropy:       0.0006
  Max final entropy:        0.0167
  Pseudo loss:              -0.11549
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.152
Episode (4460/5000) took 8.330 seconds.
  Mean final reward:        -0.1350
  Mean return:              -5.1308
  Mean final entropy:       0.0006
  Max final entropy:        0.0162
  Pseudo loss:              -0.09880
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.277
Episode (4470/5000) took 8.309 seconds.
  Mean final reward:        -0.1540
  Mean return:              -4.9583
  Mean final entropy:       0.0007
  Max final entropy:        0.0199
  Pseudo loss:              -0.11667
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.125
Episode (4480/5000) took 8.416 seconds.
  Mean final reward:        -0.1667
  Mean return:              -5.2462
  Mean final entropy:       0.0007
  Max final entropy:        0.0191
  Pseudo loss:              -0.09344
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.160
Episode (4490/5000) took 8.383 seconds.
  Mean final reward:        -0.2115
  Mean return:              -5.2524
  Mean final entropy:       0.0011
  Max final entropy:        0.0325
  Pseudo loss:              -0.20470
  Solved trajectories:      220 / 256
  Avg steps to disentangle: 2.164
Episode (4500/5000) took 8.452 seconds.
  Mean final reward:        -0.0706
  Mean return:              -4.7712
  Mean final entropy:       0.0003
  Max final entropy:        0.0073
  Pseudo loss:              -0.11546
  Solved trajectories:      233 / 256
  Avg steps to disentangle: 2.227
Episode (4510/5000) took 8.309 seconds.
  Mean final reward:        -0.1593
  Mean return:              -5.0406
  Mean final entropy:       0.0007
  Max final entropy:        0.0154
  Pseudo loss:              -0.15309
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.137
Episode (4520/5000) took 8.354 seconds.
  Mean final reward:        -0.1115
  Mean return:              -4.7915
  Mean final entropy:       0.0005
  Max final entropy:        0.0194
  Pseudo loss:              -0.09540
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.180
Episode (4530/5000) took 8.362 seconds.
  Mean final reward:        -0.1507
  Mean return:              -5.1859
  Mean final entropy:       0.0007
  Max final entropy:        0.0280
  Pseudo loss:              -0.18696
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.191
Episode (4540/5000) took 8.361 seconds.
  Mean final reward:        -0.1681
  Mean return:              -4.9052
  Mean final entropy:       0.0009
  Max final entropy:        0.0307
  Pseudo loss:              -0.42199
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.121
Episode (4550/5000) took 8.346 seconds.
  Mean final reward:        -0.1522
  Mean return:              -4.8112
  Mean final entropy:       0.0007
  Max final entropy:        0.0225
  Pseudo loss:              -0.10502
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.109
Episode (4560/5000) took 8.388 seconds.
  Mean final reward:        -0.1445
  Mean return:              -5.1575
  Mean final entropy:       0.0007
  Max final entropy:        0.0203
  Pseudo loss:              -0.08541
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.141
Episode (4570/5000) took 8.321 seconds.
  Mean final reward:        -0.2095
  Mean return:              -5.3398
  Mean final entropy:       0.0009
  Max final entropy:        0.0256
  Pseudo loss:              -0.26038
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.172
Episode (4580/5000) took 8.367 seconds.
  Mean final reward:        -0.1273
  Mean return:              -4.9896
  Mean final entropy:       0.0006
  Max final entropy:        0.0173
  Pseudo loss:              -0.15741
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.215
Episode (4590/5000) took 8.386 seconds.
  Mean final reward:        -0.1200
  Mean return:              -4.7805
  Mean final entropy:       0.0006
  Max final entropy:        0.0257
  Pseudo loss:              -0.14484
  Solved trajectories:      237 / 256
  Avg steps to disentangle: 2.223
Episode (4600/5000) took 8.342 seconds.
  Mean final reward:        -0.1370
  Mean return:              -5.1274
  Mean final entropy:       0.0006
  Max final entropy:        0.0166
  Pseudo loss:              -0.06651
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.254
Episode (4610/5000) took 8.431 seconds.
  Mean final reward:        -0.1131
  Mean return:              -4.9877
  Mean final entropy:       0.0005
  Max final entropy:        0.0144
  Pseudo loss:              -0.04633
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.219
Episode (4620/5000) took 8.231 seconds.
  Mean final reward:        -0.1797
  Mean return:              -5.2517
  Mean final entropy:       0.0011
  Max final entropy:        0.0665
  Pseudo loss:              -0.22920
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.219
Episode (4630/5000) took 8.393 seconds.
  Mean final reward:        -0.1708
  Mean return:              -5.2093
  Mean final entropy:       0.0009
  Max final entropy:        0.0251
  Pseudo loss:              -0.14984
  Solved trajectories:      218 / 256
  Avg steps to disentangle: 2.215
Episode (4640/5000) took 8.359 seconds.
  Mean final reward:        -0.1254
  Mean return:              -5.0057
  Mean final entropy:       0.0005
  Max final entropy:        0.0189
  Pseudo loss:              -0.15097
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.148
Episode (4650/5000) took 8.312 seconds.
  Mean final reward:        -0.1516
  Mean return:              -5.0021
  Mean final entropy:       0.0009
  Max final entropy:        0.0543
  Pseudo loss:              -0.20672
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.207
Episode (4660/5000) took 8.313 seconds.
  Mean final reward:        -0.1495
  Mean return:              -5.0212
  Mean final entropy:       0.0007
  Max final entropy:        0.0244
  Pseudo loss:              -0.16396
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.199
Episode (4670/5000) took 8.346 seconds.
  Mean final reward:        -0.1867
  Mean return:              -5.0263
  Mean final entropy:       0.0009
  Max final entropy:        0.0287
  Pseudo loss:              -0.30916
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.156
Episode (4680/5000) took 8.439 seconds.
  Mean final reward:        -0.0777
  Mean return:              -4.9469
  Mean final entropy:       0.0004
  Max final entropy:        0.0182
  Pseudo loss:              -0.01042
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.227
Episode (4690/5000) took 8.354 seconds.
  Mean final reward:        -0.0783
  Mean return:              -4.8216
  Mean final entropy:       0.0003
  Max final entropy:        0.0097
  Pseudo loss:              -0.06257
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.172
Episode (4700/5000) took 8.360 seconds.
  Mean final reward:        -0.1503
  Mean return:              -4.9570
  Mean final entropy:       0.0007
  Max final entropy:        0.0244
  Pseudo loss:              -0.19233
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.141
Episode (4710/5000) took 8.455 seconds.
  Mean final reward:        -0.1139
  Mean return:              -4.9415
  Mean final entropy:       0.0009
  Max final entropy:        0.1268
  Pseudo loss:              -0.28446
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.176
Episode (4720/5000) took 8.397 seconds.
  Mean final reward:        -0.1478
  Mean return:              -4.9074
  Mean final entropy:       0.0008
  Max final entropy:        0.0476
  Pseudo loss:              -0.15161
  Solved trajectories:      223 / 256
  Avg steps to disentangle: 2.094
Episode (4730/5000) took 8.369 seconds.
  Mean final reward:        -0.0847
  Mean return:              -4.8715
  Mean final entropy:       0.0003
  Max final entropy:        0.0103
  Pseudo loss:              -0.05584
  Solved trajectories:      236 / 256
  Avg steps to disentangle: 2.270
Episode (4740/5000) took 8.327 seconds.
  Mean final reward:        -0.1295
  Mean return:              -4.9178
  Mean final entropy:       0.0007
  Max final entropy:        0.0240
  Pseudo loss:              -0.18139
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.207
Episode (4750/5000) took 8.350 seconds.
  Mean final reward:        -0.1224
  Mean return:              -4.9603
  Mean final entropy:       0.0006
  Max final entropy:        0.0152
  Pseudo loss:              -0.21682
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.211
Episode (4760/5000) took 8.412 seconds.
  Mean final reward:        -0.1096
  Mean return:              -4.9029
  Mean final entropy:       0.0005
  Max final entropy:        0.0204
  Pseudo loss:              -0.12433
  Solved trajectories:      230 / 256
  Avg steps to disentangle: 2.227
Episode (4770/5000) took 8.336 seconds.
  Mean final reward:        -0.1605
  Mean return:              -4.9478
  Mean final entropy:       0.0008
  Max final entropy:        0.0349
  Pseudo loss:              -0.13255
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.180
Episode (4780/5000) took 8.385 seconds.
  Mean final reward:        -0.1909
  Mean return:              -5.1584
  Mean final entropy:       0.0009
  Max final entropy:        0.0233
  Pseudo loss:              -0.16473
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.188
Episode (4790/5000) took 8.324 seconds.
  Mean final reward:        -0.1640
  Mean return:              -5.1405
  Mean final entropy:       0.0008
  Max final entropy:        0.0293
  Pseudo loss:              -0.08592
  Solved trajectories:      219 / 256
  Avg steps to disentangle: 2.141
Episode (4800/5000) took 8.331 seconds.
  Mean final reward:        -0.1719
  Mean return:              -5.1923
  Mean final entropy:       0.0010
  Max final entropy:        0.0502
  Pseudo loss:              -0.27969
  Solved trajectories:      224 / 256
  Avg steps to disentangle: 2.211
Episode (4810/5000) took 8.371 seconds.
  Mean final reward:        -0.1111
  Mean return:              -5.1550
  Mean final entropy:       0.0005
  Max final entropy:        0.0214
  Pseudo loss:              -0.09632
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.180
Episode (4820/5000) took 8.389 seconds.
  Mean final reward:        -0.1431
  Mean return:              -4.9185
  Mean final entropy:       0.0006
  Max final entropy:        0.0192
  Pseudo loss:              -0.18503
  Solved trajectories:      229 / 256
  Avg steps to disentangle: 2.160
Episode (4830/5000) took 8.314 seconds.
  Mean final reward:        -0.1312
  Mean return:              -5.0285
  Mean final entropy:       0.0006
  Max final entropy:        0.0319
  Pseudo loss:              -0.14711
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.184
Episode (4840/5000) took 8.308 seconds.
  Mean final reward:        -0.1069
  Mean return:              -4.9715
  Mean final entropy:       0.0005
  Max final entropy:        0.0198
  Pseudo loss:              -0.12217
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.172
Episode (4850/5000) took 8.419 seconds.
  Mean final reward:        -0.1530
  Mean return:              -4.9826
  Mean final entropy:       0.0008
  Max final entropy:        0.0372
  Pseudo loss:              -0.10596
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.180
Episode (4860/5000) took 8.353 seconds.
  Mean final reward:        -0.0909
  Mean return:              -4.7953
  Mean final entropy:       0.0005
  Max final entropy:        0.0425
  Pseudo loss:              -0.09958
  Solved trajectories:      236 / 256
  Avg steps to disentangle: 2.234
Episode (4870/5000) took 8.409 seconds.
  Mean final reward:        -0.1096
  Mean return:              -4.8448
  Mean final entropy:       0.0005
  Max final entropy:        0.0134
  Pseudo loss:              -0.10581
  Solved trajectories:      227 / 256
  Avg steps to disentangle: 2.172
Episode (4880/5000) took 8.341 seconds.
  Mean final reward:        -0.1090
  Mean return:              -4.7872
  Mean final entropy:       0.0005
  Max final entropy:        0.0277
  Pseudo loss:              -0.12915
  Solved trajectories:      232 / 256
  Avg steps to disentangle: 2.164
Episode (4890/5000) took 8.330 seconds.
  Mean final reward:        -0.2233
  Mean return:              -5.3878
  Mean final entropy:       0.0012
  Max final entropy:        0.0559
  Pseudo loss:              -0.20373
  Solved trajectories:      214 / 256
  Avg steps to disentangle: 2.129
Episode (4900/5000) took 8.394 seconds.
  Mean final reward:        -0.1640
  Mean return:              -5.1484
  Mean final entropy:       0.0007
  Max final entropy:        0.0185
  Pseudo loss:              -0.14103
  Solved trajectories:      222 / 256
  Avg steps to disentangle: 2.199
Episode (4910/5000) took 8.400 seconds.
  Mean final reward:        -0.1233
  Mean return:              -4.9256
  Mean final entropy:       0.0006
  Max final entropy:        0.0175
  Pseudo loss:              -0.17462
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.172
Episode (4920/5000) took 8.499 seconds.
  Mean final reward:        -0.1426
  Mean return:              -5.1540
  Mean final entropy:       0.0006
  Max final entropy:        0.0334
  Pseudo loss:              -0.23591
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.223
Episode (4930/5000) took 8.348 seconds.
  Mean final reward:        -0.1291
  Mean return:              -4.8581
  Mean final entropy:       0.0007
  Max final entropy:        0.0280
  Pseudo loss:              -0.18392
  Solved trajectories:      231 / 256
  Avg steps to disentangle: 2.176
Episode (4940/5000) took 8.308 seconds.
  Mean final reward:        -0.1495
  Mean return:              -5.1206
  Mean final entropy:       0.0008
  Max final entropy:        0.0511
  Pseudo loss:              -0.24685
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.211
Episode (4950/5000) took 8.344 seconds.
  Mean final reward:        -0.1743
  Mean return:              -5.0712
  Mean final entropy:       0.0009
  Max final entropy:        0.0520
  Pseudo loss:              -0.19626
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.148
Episode (4960/5000) took 8.278 seconds.
  Mean final reward:        -0.1478
  Mean return:              -5.0771
  Mean final entropy:       0.0006
  Max final entropy:        0.0179
  Pseudo loss:              -0.07413
  Solved trajectories:      226 / 256
  Avg steps to disentangle: 2.148
Episode (4970/5000) took 8.329 seconds.
  Mean final reward:        -0.0977
  Mean return:              -4.9491
  Mean final entropy:       0.0004
  Max final entropy:        0.0132
  Pseudo loss:              -0.13027
  Solved trajectories:      228 / 256
  Avg steps to disentangle: 2.207
Episode (4980/5000) took 8.321 seconds.
  Mean final reward:        -0.1311
  Mean return:              -4.9183
  Mean final entropy:       0.0006
  Max final entropy:        0.0129
  Pseudo loss:              -0.14073
  Solved trajectories:      225 / 256
  Avg steps to disentangle: 2.184
Episode (4990/5000) took 8.385 seconds.
  Mean final reward:        -0.1780
  Mean return:              -5.0563
  Mean final entropy:       0.0008
  Max final entropy:        0.0428
  Pseudo loss:              -0.10622
  Solved trajectories:      215 / 256
  Avg steps to disentangle: 2.117
Episode (5000/5000) took 8.306 seconds.
  Mean final reward:        -0.1617
  Mean return:              -5.2235
  Mean final entropy:       0.0007
  Max final entropy:        0.0160
  Pseudo loss:              -0.25715
  Solved trajectories:      221 / 256
  Avg steps to disentangle: 2.223
