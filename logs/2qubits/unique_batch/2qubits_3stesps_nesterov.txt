
step: 1
seed: 12345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.283 seconds.
  Mean final reward: -1.8611
  Mean return: -7.0057
  Mean final entropy: 0.0230
  Max final entropy: 0.0626
  Pseudo loss: -2.40941
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.5881
  Mean return: -4.4332
  Mean final entropy: 0.0034
  Max final entropy: 0.0197
  Pseudo loss: -1.91695
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.275 seconds.
  Mean final reward: -1.5787
  Mean return: -5.9477
  Mean final entropy: 0.0191
  Max final entropy: 0.0731
  Pseudo loss: 0.06949
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.277 seconds.
  Mean final reward: -0.5201
  Mean return: -4.8325
  Mean final entropy: 0.0081
  Max final entropy: 0.0641
  Pseudo loss: -1.73106
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.286 seconds.
  Mean final reward: -0.5130
  Mean return: -4.6128
  Mean final entropy: 0.0020
  Max final entropy: 0.0102
  Pseudo loss: -2.01173
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.286 seconds.
  Mean final reward: -0.0505
  Mean return: -2.8093
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: -0.89443
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4520
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.86747
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6389
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.09270
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0656
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.42830
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5273
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.00781
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.51143456 0.6105597  0.30092776 0.32688206 0.07879784 0.3808744
 0.26923123 0.39384168]
  Mean final entropy: 0.000057
  Max final entropy: 0.000301
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.0372347e-06 6.9580594e-08 5.7984352e-08 7.3475647e-08 7.9598922e-06
 1.4276135e-06 1.3447481e-06 3.0745858e-07]
  Max Min Entropy: 7.959892172948457e-06
  Best actions: [array([8, 7, 4, 8, 1, 0, 2, 4]), array([6, 8, 7, 4, 4, 2, 1, 5]), array([1, 0, 8, 1, 7, 0, 4, 8])]

####################################


step: 2
seed: 12329
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.272 seconds.
  Mean final reward: -2.2666
  Mean return: -9.9507
  Mean final entropy: 0.0736
  Max final entropy: 0.3709
  Pseudo loss: -1.36153
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.281 seconds.
  Mean final reward: -2.1277
  Mean return: -9.5299
  Mean final entropy: 0.0428
  Max final entropy: 0.1864
  Pseudo loss: -2.42412
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.281 seconds.
  Mean final reward: -0.9839
  Mean return: -7.1371
  Mean final entropy: 0.0071
  Max final entropy: 0.0394
  Pseudo loss: -2.89030
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.278 seconds.
  Mean final reward: -0.4236
  Mean return: -6.2193
  Mean final entropy: 0.0037
  Max final entropy: 0.0273
  Pseudo loss: -2.85813
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.0854
  Mean return: -4.9053
  Mean final entropy: 0.0005
  Max final entropy: 0.0014
  Pseudo loss: -0.29233
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.282 seconds.
  Mean final reward: -0.0452
  Mean return: -5.2267
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -1.17806
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6326
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.13132
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.282 seconds.
  Mean final reward: -0.0100
  Mean return: -4.8802
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -0.26034
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3534
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.07106
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.279 seconds.
  Mean final reward: -0.0713
  Mean return: -5.4210
  Mean final entropy: 0.0004
  Max final entropy: 0.0018
  Pseudo loss: -1.73712
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.40172344 0.3687933  0.46383095 0.41811505 0.39810646 0.6593156
 0.38180375 0.43475404]
  Mean final entropy: 0.000236
  Max final entropy: 0.000993
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.8468792e-08 1.9968661e-07 7.2458838e-06 6.9636734e-08 1.8012284e-05
 9.1379469e-08 5.4523011e-04 9.2754682e-08]
  Max Min Entropy: 0.0005452301120385528
  Best actions: [array([2, 3, 0, 5, 6, 8, 6, 4]), array([8, 4, 1, 6, 3, 7, 8, 5]), array([2, 0, 5, 8, 2, 3, 6, 6])]

####################################


step: 3
seed: 12285
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.286 seconds.
  Mean final reward: -2.3702
  Mean return: -9.4645
  Mean final entropy: 0.0283
  Max final entropy: 0.0991
  Pseudo loss: -1.84489
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.274 seconds.
  Mean final reward: -0.6801
  Mean return: -5.4548
  Mean final entropy: 0.0073
  Max final entropy: 0.0525
  Pseudo loss: -1.78845
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.273 seconds.
  Mean final reward: -0.3327
  Mean return: -3.6004
  Mean final entropy: 0.0012
  Max final entropy: 0.0052
  Pseudo loss: 0.58174
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.276 seconds.
  Mean final reward: -1.5621
  Mean return: -6.3594
  Mean final entropy: 0.0152
  Max final entropy: 0.0544
  Pseudo loss: -2.10331
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (50/100) took 0.275 seconds.
  Mean final reward: -0.6165
  Mean return: -4.1077
  Mean final entropy: 0.0028
  Max final entropy: 0.0144
  Pseudo loss: -0.28699
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.278 seconds.
  Mean final reward: -1.0761
  Mean return: -4.7124
  Mean final entropy: 0.0089
  Max final entropy: 0.0544
  Pseudo loss: -2.37754
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (70/100) took 0.273 seconds.
  Mean final reward: -0.7930
  Mean return: -5.1606
  Mean final entropy: 0.0047
  Max final entropy: 0.0238
  Pseudo loss: -5.69099
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.272 seconds.
  Mean final reward: -0.7470
  Mean return: -5.5688
  Mean final entropy: 0.0052
  Max final entropy: 0.0341
  Pseudo loss: -3.60324
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.277 seconds.
  Mean final reward: -0.1639
  Mean return: -3.3086
  Mean final entropy: 0.0007
  Max final entropy: 0.0022
  Pseudo loss: -0.09731
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.278 seconds.
  Mean final reward: -0.1002
  Mean return: -2.6682
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.02066
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.23314145 0.6694178  0.39011353 0.29549757 0.23976189 0.04213208
 0.3161075  0.14502367]
  Mean final entropy: 0.000098
  Max final entropy: 0.000455
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 7.7440893e-07 -4.8748827e-09  8.9898986e-09  8.0433253e-08
  1.7686833e-07  3.9962936e-07  8.5267544e-08  3.0857434e-05]
  Max Min Entropy: 3.08574344671797e-05
  Best actions: [array([8, 8, 6, 8, 3, 5, 2, 1]), array([7, 7, 0, 2, 0, 4, 1, 2]), array([0, 1, 1, 3, 7, 6, 4, 1])]

####################################


step: 4
seed: 12219
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.290 seconds.
  Mean final reward: -0.9026
  Mean return: -7.1432
  Mean final entropy: 0.0147
  Max final entropy: 0.1031
  Pseudo loss: -1.29766
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.280 seconds.
  Mean final reward: -0.2293
  Mean return: -5.6168
  Mean final entropy: 0.0008
  Max final entropy: 0.0030
  Pseudo loss: -1.26096
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.286 seconds.
  Mean final reward: -0.5520
  Mean return: -4.9930
  Mean final entropy: 0.0056
  Max final entropy: 0.0416
  Pseudo loss: -5.79904
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.271 seconds.
  Mean final reward: -0.4065
  Mean return: -4.8801
  Mean final entropy: 0.0020
  Max final entropy: 0.0126
  Pseudo loss: -6.56372
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: -0.3101
  Mean return: -3.4385
  Mean final entropy: 0.0012
  Max final entropy: 0.0077
  Pseudo loss: -0.84068
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8091
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.03357
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8091
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.02531
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8091
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.01784
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.285 seconds.
  Mean final reward: -0.2546
  Mean return: -3.3195
  Mean final entropy: 0.0010
  Max final entropy: 0.0077
  Pseudo loss: -2.30829
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8091
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.01692
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.2725601  0.4368319  0.6344536  0.42357188 0.39017168 0.41888055
 0.22632013 0.3949499 ]
  Mean final entropy: 0.000058
  Max final entropy: 0.000437
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.6145274e-06  1.5770624e-07 -3.9807134e-08  1.4873096e-07
  4.0908282e-07  1.2131910e-06  1.7216545e-06  1.5689332e-06]
  Max Min Entropy: 1.7216544847542536e-06
  Best actions: [array([4, 3, 4, 1, 3, 4, 3, 1]), array([3, 8, 7, 7, 0, 7, 4, 7]), array([5, 2, 0, 1, 4, 4, 3, 1])]

####################################


step: 5
seed: 12137
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.278 seconds.
  Mean final reward: -1.1019
  Mean return: -5.3360
  Mean final entropy: 0.0052
  Max final entropy: 0.0167
  Pseudo loss: 0.63927
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.269 seconds.
  Mean final reward: -0.7110
  Mean return: -5.8920
  Mean final entropy: 0.0046
  Max final entropy: 0.0207
  Pseudo loss: 0.28737
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.276 seconds.
  Mean final reward: -1.3834
  Mean return: -6.6679
  Mean final entropy: 0.0182
  Max final entropy: 0.1151
  Pseudo loss: -0.76421
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.267 seconds.
  Mean final reward: -0.5262
  Mean return: -4.8407
  Mean final entropy: 0.0086
  Max final entropy: 0.0674
  Pseudo loss: -0.92813
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.263 seconds.
  Mean final reward: -0.4607
  Mean return: -5.2142
  Mean final entropy: 0.0022
  Max final entropy: 0.0132
  Pseudo loss: -0.73418
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.1057
  Mean return: -3.9828
  Mean final entropy: 0.0005
  Max final entropy: 0.0020
  Pseudo loss: -0.21691
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.266 seconds.
  Mean final reward: -0.0884
  Mean return: -3.9487
  Mean final entropy: 0.0004
  Max final entropy: 0.0020
  Pseudo loss: -0.22635
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.275 seconds.
  Mean final reward: -0.0884
  Mean return: -3.4588
  Mean final entropy: 0.0004
  Max final entropy: 0.0020
  Pseudo loss: 0.04350
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.270 seconds.
  Mean final reward: -0.6095
  Mean return: -5.0224
  Mean final entropy: 0.0030
  Max final entropy: 0.0177
  Pseudo loss: -0.25015
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.267 seconds.
  Mean final reward: -0.1284
  Mean return: -3.8182
  Mean final entropy: 0.0006
  Max final entropy: 0.0020
  Pseudo loss: -0.09651
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.46185315 0.3445333  0.11998563 0.4662109  0.10785784 0.15743475
 0.04587724 0.39183986]
  Mean final entropy: 0.000419
  Max final entropy: 0.002028
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 8.4537186e-07  4.0597624e-07  4.0730019e-06 -2.9830229e-08
  6.0945342e-07  6.9195732e-05 -1.0443016e-09  6.9263726e-05]
  Max Min Entropy: 6.9263725890778e-05
  Best actions: [array([2, 8, 2, 7, 7, 3, 4, 0]), array([5, 3, 4, 8, 0, 7, 5, 4]), array([2, 1, 2, 3, 5, 3, 7, 0])]

####################################


step: 6
seed: 12045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.279 seconds.
  Mean final reward: -1.6416
  Mean return: -7.1151
  Mean final entropy: 0.0212
  Max final entropy: 0.1148
  Pseudo loss: -1.56854
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.263 seconds.
  Mean final reward: -1.1191
  Mean return: -5.1802
  Mean final entropy: 0.0080
  Max final entropy: 0.0364
  Pseudo loss: -1.24517
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.279 seconds.
  Mean final reward: -1.2829
  Mean return: -5.9294
  Mean final entropy: 0.0151
  Max final entropy: 0.0893
  Pseudo loss: -3.30160
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.266 seconds.
  Mean final reward: -0.8173
  Mean return: -4.1331
  Mean final entropy: 0.0417
  Max final entropy: 0.3280
  Pseudo loss: 1.07186
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.270 seconds.
  Mean final reward: -0.2378
  Mean return: -2.9968
  Mean final entropy: 0.0010
  Max final entropy: 0.0067
  Pseudo loss: 0.37316
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.272 seconds.
  Mean final reward: -0.6353
  Mean return: -3.6913
  Mean final entropy: 0.0150
  Max final entropy: 0.1177
  Pseudo loss: -3.22276
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.265 seconds.
  Mean final reward: -0.4526
  Mean return: -3.5316
  Mean final entropy: 0.0038
  Max final entropy: 0.0273
  Pseudo loss: -4.19260
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6853
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.48515
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.274 seconds.
  Mean final reward: -0.1055
  Mean return: -2.0756
  Mean final entropy: 0.0006
  Max final entropy: 0.0017
  Pseudo loss: -0.04065
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (100/100) took 0.273 seconds.
  Mean final reward: -0.0392
  Mean return: -1.9327
  Mean final entropy: 0.0005
  Max final entropy: 0.0014
  Pseudo loss: 0.17313
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.2631798  0.5739741  0.07371953 0.45061877 0.25159284 0.3006184
 0.4364323  0.2278399 ]
  Mean final entropy: 0.000259
  Max final entropy: 0.001369
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [1.06780455e-07 6.19865250e-06 1.20069365e-06 5.21373124e-07
 1.63843197e-07 5.53194113e-08 9.95463665e-07 2.47116446e-07]
  Max Min Entropy: 6.1986524997337256e-06
  Best actions: [array([5, 0, 1, 1, 2, 2, 7, 5]), array([0, 1, 2, 2, 5, 0, 4, 2]), array([0, 7, 1, 1, 4, 0, 0, 7])]

####################################


step: 7
seed: 11949
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.258 seconds.
  Mean final reward: -1.1225
  Mean return: -9.5626
  Mean final entropy: 0.0168
  Max final entropy: 0.0954
  Pseudo loss: -1.16591
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.279 seconds.
  Mean final reward: -0.9046
  Mean return: -3.9741
  Mean final entropy: 0.0045
  Max final entropy: 0.0152
  Pseudo loss: -1.30587
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.278 seconds.
  Mean final reward: -0.6769
  Mean return: -3.6602
  Mean final entropy: 0.0098
  Max final entropy: 0.0736
  Pseudo loss: 0.24343
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.278 seconds.
  Mean final reward: -0.7862
  Mean return: -3.5891
  Mean final entropy: 0.0049
  Max final entropy: 0.0237
  Pseudo loss: -0.84982
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.274 seconds.
  Mean final reward: -0.4244
  Mean return: -3.2805
  Mean final entropy: 0.0040
  Max final entropy: 0.0298
  Pseudo loss: -0.73381
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.281 seconds.
  Mean final reward: -0.4398
  Mean return: -3.1771
  Mean final entropy: 0.0045
  Max final entropy: 0.0337
  Pseudo loss: -4.40959
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3102
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.02936
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.278 seconds.
  Mean final reward: -0.5769
  Mean return: -3.7105
  Mean final entropy: 0.0128
  Max final entropy: 0.1010
  Pseudo loss: -8.58137
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.283 seconds.
  Mean final reward: -0.2912
  Mean return: -3.2449
  Mean final entropy: 0.0015
  Max final entropy: 0.0103
  Pseudo loss: -1.40381
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2962
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: 0.01753
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.50391614 0.09640729 0.45139036 0.52493644 0.44969928 0.51373225
 0.20727934 0.57119846]
  Mean final entropy: 0.000173
  Max final entropy: 0.000633
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.6857887e-08 1.0641263e-06 7.7958675e-06 1.5168040e-06 3.0801066e-06
 2.1496963e-07 7.1095667e-07 6.3547873e-07]
  Max Min Entropy: 7.795867531967815e-06
  Best actions: [array([4, 4, 1, 2, 8, 4, 5, 2]), array([3, 1, 0, 5, 3, 1, 4, 1]), array([2, 4, 1, 6, 4, 1, 0, 3])]

####################################


step: 8
seed: 11855
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.274 seconds.
  Mean final reward: -1.2069
  Mean return: -7.6778
  Mean final entropy: 0.0168
  Max final entropy: 0.1157
  Pseudo loss: -2.10782
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.280 seconds.
  Mean final reward: -1.2813
  Mean return: -6.6257
  Mean final entropy: 0.0108
  Max final entropy: 0.0566
  Pseudo loss: -2.44456
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.267 seconds.
  Mean final reward: -0.6582
  Mean return: -5.5609
  Mean final entropy: 0.0032
  Max final entropy: 0.0168
  Pseudo loss: -8.93842
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.267 seconds.
  Mean final reward: -0.1220
  Mean return: -4.5313
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: -0.35656
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.271 seconds.
  Mean final reward: -0.1220
  Mean return: -4.5313
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: -0.57509
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6563
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.01959
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6563
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.01053
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6563
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.02506
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6563
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.02181
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8195
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.57026
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.56794226 0.19578284 0.6003779  0.00381707 0.39673656 0.36800322
 0.19931266 0.27673694]
  Mean final entropy: 0.000019
  Max final entropy: 0.000087
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.1059716e-05 8.6072284e-08 9.0564029e-08 3.1302179e-07 5.2339971e-07
 1.1172847e-07 2.4834424e-06 1.8316662e-07]
  Max Min Entropy: 7.105971599230543e-05
  Best actions: [array([4, 4, 2, 5, 5, 6, 4, 1]), array([7, 7, 0, 0, 2, 3, 5, 0]), array([0, 2, 7, 5, 5, 7, 4, 5])]

####################################


step: 9
seed: 11769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.277 seconds.
  Mean final reward: -1.3999
  Mean return: -6.5525
  Mean final entropy: 0.0103
  Max final entropy: 0.0438
  Pseudo loss: -0.15238
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.289 seconds.
  Mean final reward: -0.6231
  Mean return: -6.1249
  Mean final entropy: 0.0044
  Max final entropy: 0.0304
  Pseudo loss: -2.98184
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.280 seconds.
  Mean final reward: -0.3329
  Mean return: -4.3839
  Mean final entropy: 0.0019
  Max final entropy: 0.0143
  Pseudo loss: -0.81845
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.276 seconds.
  Mean final reward: -1.0126
  Mean return: -5.2042
  Mean final entropy: 0.0059
  Max final entropy: 0.0272
  Pseudo loss: -2.90226
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (50/100) took 0.285 seconds.
  Mean final reward: -0.0239
  Mean return: -3.9736
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -1.33424
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.5543
  Mean return: -4.4667
  Mean final entropy: 0.0018
  Max final entropy: 0.0050
  Pseudo loss: -1.52540
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (70/100) took 0.277 seconds.
  Mean final reward: -0.1482
  Mean return: -3.3843
  Mean final entropy: 0.0005
  Max final entropy: 0.0033
  Pseudo loss: 0.45350
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.276 seconds.
  Mean final reward: -0.5714
  Mean return: -4.2477
  Mean final entropy: 0.0022
  Max final entropy: 0.0110
  Pseudo loss: -0.10536
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (90/100) took 0.279 seconds.
  Mean final reward: -0.1482
  Mean return: -3.3843
  Mean final entropy: 0.0005
  Max final entropy: 0.0033
  Pseudo loss: 0.10660
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.283 seconds.
  Mean final reward: -0.1482
  Mean return: -3.5631
  Mean final entropy: 0.0006
  Max final entropy: 0.0033
  Pseudo loss: -0.11739
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.15726951 0.15311186 0.4541216  0.62283635 0.10774249 0.56590927
 0.04283874 0.43098813]
  Mean final entropy: 0.001025
  Max final entropy: 0.004119
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [4.6468017e-06 8.3003924e-09 2.0850969e-07 7.7467411e-07 2.4270750e-07
 1.5853122e-07 3.6420909e-07 1.5793440e-06]
  Max Min Entropy: 4.646801698982017e-06
  Best actions: [array([7, 1, 1, 8, 4, 6, 5, 0]), array([4, 2, 2, 7, 3, 0, 8, 7]), array([7, 6, 5, 3, 7, 6, 7, 4])]

####################################


step: 10
seed: 11697
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.279 seconds.
  Mean final reward: -0.9540
  Mean return: -6.7383
  Mean final entropy: 0.0333
  Max final entropy: 0.2600
  Pseudo loss: -1.08147
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.267 seconds.
  Mean final reward: -1.2164
  Mean return: -7.5997
  Mean final entropy: 0.0044
  Max final entropy: 0.0110
  Pseudo loss: -1.45372
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.290 seconds.
  Mean final reward: -0.4438
  Mean return: -4.4220
  Mean final entropy: 0.0017
  Max final entropy: 0.0062
  Pseudo loss: 0.64489
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.284 seconds.
  Mean final reward: -0.2204
  Mean return: -3.0326
  Mean final entropy: 0.0009
  Max final entropy: 0.0050
  Pseudo loss: 0.62237
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2785
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.29448
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (60/100) took 0.281 seconds.
  Mean final reward: -0.1784
  Mean return: -3.5682
  Mean final entropy: 0.0006
  Max final entropy: 0.0042
  Pseudo loss: -1.02463
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.280 seconds.
  Mean final reward: -0.4009
  Mean return: -3.3742
  Mean final entropy: 0.0029
  Max final entropy: 0.0214
  Pseudo loss: -0.70151
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7467
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.16575
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.287 seconds.
  Mean final reward: -0.1339
  Mean return: -2.7677
  Mean final entropy: 0.0005
  Max final entropy: 0.0029
  Pseudo loss: -0.11558
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6337
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.06942
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.30917364 0.09972235 0.0613288  0.15313223 0.27316827 0.2161517
 0.13971214 0.41891915]
  Mean final entropy: 0.000184
  Max final entropy: 0.000790
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.3024999e-07 5.9015058e-08 1.6379918e-06 7.0680113e-07 2.0525422e-07
 2.9097549e-05 3.0003891e-06 2.4977732e-07]
  Max Min Entropy: 2.909754948632326e-05
  Best actions: [array([1, 7, 7, 8, 4, 6, 2, 7]), array([7, 4, 6, 6, 3, 2, 0, 1]), array([4, 5, 8, 8, 2, 6, 2, 7])]

####################################


step: 11
seed: 11645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.278 seconds.
  Mean final reward: -1.3156
  Mean return: -8.2169
  Mean final entropy: 0.0102
  Max final entropy: 0.0385
  Pseudo loss: -1.07477
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.278 seconds.
  Mean final reward: -1.4098
  Mean return: -8.1311
  Mean final entropy: 0.0112
  Max final entropy: 0.0458
  Pseudo loss: -1.11337
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.278 seconds.
  Mean final reward: -0.2779
  Mean return: -4.1125
  Mean final entropy: 0.0012
  Max final entropy: 0.0075
  Pseudo loss: 0.14610
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.287 seconds.
  Mean final reward: -0.7623
  Mean return: -5.7174
  Mean final entropy: 0.0044
  Max final entropy: 0.0221
  Pseudo loss: -2.47278
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.285 seconds.
  Mean final reward: -0.6641
  Mean return: -4.9622
  Mean final entropy: 0.0026
  Max final entropy: 0.0136
  Pseudo loss: -0.61054
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (60/100) took 0.280 seconds.
  Mean final reward: -0.4156
  Mean return: -4.3266
  Mean final entropy: 0.0019
  Max final entropy: 0.0133
  Pseudo loss: -0.00685
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.275 seconds.
  Mean final reward: -0.4218
  Mean return: -4.2564
  Mean final entropy: 0.0016
  Max final entropy: 0.0092
  Pseudo loss: -0.91285
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (80/100) took 0.281 seconds.
  Mean final reward: -0.1664
  Mean return: -4.3042
  Mean final entropy: 0.0005
  Max final entropy: 0.0021
  Pseudo loss: -0.90354
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.275 seconds.
  Mean final reward: -0.5002
  Mean return: -4.5968
  Mean final entropy: 0.0036
  Max final entropy: 0.0261
  Pseudo loss: -2.72046
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.283 seconds.
  Mean final reward: -0.4674
  Mean return: -4.3020
  Mean final entropy: 0.0021
  Max final entropy: 0.0133
  Pseudo loss: -0.96449
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Testing with greedy policy:
  Start entropy: [0.28296292 0.04370847 0.42656752 0.4153368  0.5801036  0.06952325
 0.4704199  0.20436487]
  Mean final entropy: 0.000525
  Max final entropy: 0.002094
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [6.1219522e-05 5.4057364e-07 2.1055908e-07 7.3080338e-07 2.6726181e-07
 2.0003516e-07 6.0857383e-07 1.7457765e-07]
  Max Min Entropy: 6.12195217399858e-05
  Best actions: [array([8, 4, 6, 4, 7, 1, 8, 3]), array([5, 5, 7, 3, 1, 8, 2, 6]), array([8, 6, 6, 4, 5, 1, 6, 8])]

####################################


step: 12
seed: 11619
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.281 seconds.
  Mean final reward: -1.4270
  Mean return: -6.0502
  Mean final entropy: 0.0203
  Max final entropy: 0.0971
  Pseudo loss: -1.22140
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.274 seconds.
  Mean final reward: -0.1679
  Mean return: -5.6685
  Mean final entropy: 0.0007
  Max final entropy: 0.0020
  Pseudo loss: -3.33713
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.270 seconds.
  Mean final reward: -0.7134
  Mean return: -4.7611
  Mean final entropy: 0.0053
  Max final entropy: 0.0348
  Pseudo loss: -1.06975
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -3.3188
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.39681
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.272 seconds.
  Mean final reward: -0.1792
  Mean return: -3.6713
  Mean final entropy: 0.0006
  Max final entropy: 0.0042
  Pseudo loss: -0.38977
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.267 seconds.
  Mean final reward: -0.2689
  Mean return: -3.6757
  Mean final entropy: 0.0012
  Max final entropy: 0.0086
  Pseudo loss: -0.77115
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -3.3188
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.52259
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2334
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.17210
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2334
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.28332
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2334
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.20644
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.07583138 0.26920837 0.19072072 0.46765816 0.01995216 0.24711628
 0.15086776 0.6099125 ]
  Mean final entropy: 0.000132
  Max final entropy: 0.000352
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.1222847e-07  3.8590861e-05  1.4288681e-06 -4.3003503e-08
  2.3299745e-07  1.2121573e-05 -6.3547169e-08 -8.5484984e-08]
  Max Min Entropy: 3.8590860640397295e-05
  Best actions: [array([3, 6, 0, 5, 5, 1, 8, 8]), array([1, 3, 1, 4, 3, 4, 2, 7]), array([7, 6, 0, 4, 2, 7, 2, 3])]

####################################


step: 13
seed: 11625
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.278 seconds.
  Mean final reward: -1.9309
  Mean return: -7.8515
  Mean final entropy: 0.0262
  Max final entropy: 0.1167
  Pseudo loss: -0.96803
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.270 seconds.
  Mean final reward: -1.2770
  Mean return: -4.9239
  Mean final entropy: 0.0140
  Max final entropy: 0.0572
  Pseudo loss: 1.05329
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.273 seconds.
  Mean final reward: -1.0811
  Mean return: -4.5353
  Mean final entropy: 0.0157
  Max final entropy: 0.0657
  Pseudo loss: -2.53295
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.284 seconds.
  Mean final reward: -1.2699
  Mean return: -4.9970
  Mean final entropy: 0.0172
  Max final entropy: 0.0729
  Pseudo loss: -2.55034
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.272 seconds.
  Mean final reward: -0.0295
  Mean return: -3.0041
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.12136
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.0295
  Mean return: -3.0041
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.04327
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.276 seconds.
  Mean final reward: -0.1372
  Mean return: -3.2316
  Mean final entropy: 0.0007
  Max final entropy: 0.0025
  Pseudo loss: -0.29942
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.272 seconds.
  Mean final reward: -0.0295
  Mean return: -3.0111
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.64829
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.275 seconds.
  Mean final reward: -0.0295
  Mean return: -3.0504
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: 0.06451
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.273 seconds.
  Mean final reward: -0.0295
  Mean return: -3.0504
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: 0.06743
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5362513  0.22211348 0.43231204 0.12581736 0.23096493 0.31086287
 0.2344377  0.4729659 ]
  Mean final entropy: 0.000406
  Max final entropy: 0.001251
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [ 3.1400879e-07  1.3852477e-04 -4.2838955e-09  1.0035262e-07
  1.5203012e-06  1.9148519e-04 -6.8628175e-08  3.7779819e-06]
  Max Min Entropy: 0.00019148518913425505
  Best actions: [array([3, 3, 0, 3, 3, 1, 7, 2]), array([5, 1, 1, 8, 6, 2, 5, 0]), array([7, 2, 2, 3, 3, 4, 0, 2])]

####################################


step: 14
seed: 11669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.265 seconds.
  Mean final reward: -1.8107
  Mean return: -8.4192
  Mean final entropy: 0.0243
  Max final entropy: 0.1340
  Pseudo loss: -0.92909
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.269 seconds.
  Mean final reward: -1.3767
  Mean return: -8.1790
  Mean final entropy: 0.0076
  Max final entropy: 0.0224
  Pseudo loss: -2.18778
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.286 seconds.
  Mean final reward: -0.5649
  Mean return: -4.8715
  Mean final entropy: 0.0019
  Max final entropy: 0.0075
  Pseudo loss: -1.69068
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.4054
  Mean return: -4.3576
  Mean final entropy: 0.0022
  Max final entropy: 0.0131
  Pseudo loss: 0.01914
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9951
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.03779
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.275 seconds.
  Mean final reward: -0.7268
  Mean return: -4.3530
  Mean final entropy: 0.0156
  Max final entropy: 0.1204
  Pseudo loss: -1.26652
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.273 seconds.
  Mean final reward: -0.3953
  Mean return: -4.1826
  Mean final entropy: 0.0018
  Max final entropy: 0.0117
  Pseudo loss: -3.05374
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.276 seconds.
  Mean final reward: -0.1608
  Mean return: -3.4713
  Mean final entropy: 0.0006
  Max final entropy: 0.0021
  Pseudo loss: -0.96844
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.1481
  Mean return: -3.2478
  Mean final entropy: 0.0007
  Max final entropy: 0.0033
  Pseudo loss: -1.89114
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.270 seconds.
  Mean final reward: -0.2811
  Mean return: -3.5893
  Mean final entropy: 0.0013
  Max final entropy: 0.0095
  Pseudo loss: -1.87488
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.06893384 0.3854446  0.4350059  0.5475005  0.46722287 0.35286623
 0.4926498  0.57976174]
  Mean final entropy: 0.000162
  Max final entropy: 0.000771
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 9.1385181e-07  2.7569160e-05  1.7126034e-06  2.8955290e-06
  4.2516180e-07  6.3965766e-05 -3.7068855e-08  6.5204148e-08]
  Max Min Entropy: 6.39657664578408e-05
  Best actions: [array([3, 5, 0, 8, 0, 7, 8, 1]), array([6, 2, 2, 2, 1, 4, 3, 2]), array([3, 5, 4, 0, 0, 1, 0, 8])]

####################################


step: 15
seed: 11757
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.279 seconds.
  Mean final reward: -0.6197
  Mean return: -4.7413
  Mean final entropy: 0.0031
  Max final entropy: 0.0142
  Pseudo loss: 0.15883
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.276 seconds.
  Mean final reward: -1.2798
  Mean return: -5.5820
  Mean final entropy: 0.0266
  Max final entropy: 0.1838
  Pseudo loss: -3.34997
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.266 seconds.
  Mean final reward: -0.3594
  Mean return: -3.7333
  Mean final entropy: 0.0023
  Max final entropy: 0.0177
  Pseudo loss: -1.38050
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.288 seconds.
  Mean final reward: -0.1717
  Mean return: -3.1767
  Mean final entropy: 0.0005
  Max final entropy: 0.0040
  Pseudo loss: -0.94841
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.500
Episode (50/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7669
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.80615
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0056
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.26668
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8343
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.11795
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8343
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.09302
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8343
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.06884
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4942
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.17138
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.02414292 0.62981606 0.36798832 0.5391474  0.00610481 0.57285357
 0.04491936 0.5823115 ]
  Mean final entropy: 0.000045
  Max final entropy: 0.000275
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.6914537e-07  2.4897485e-08  1.7304797e-07  4.2929546e-06
  6.4010486e-08  8.3933219e-06  3.5892324e-07 -1.3813606e-07]
  Max Min Entropy: 8.393321877520066e-06
  Best actions: [array([0, 2, 6, 3, 5, 8, 0, 5]), array([6, 0, 3, 5, 4, 5, 6, 3]), array([0, 7, 3, 3, 5, 8, 6, 1])]

####################################


step: 16
seed: 11895
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.275 seconds.
  Mean final reward: -1.1147
  Mean return: -8.3370
  Mean final entropy: 0.0051
  Max final entropy: 0.0225
  Pseudo loss: -0.19841
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.279 seconds.
  Mean final reward: -1.0264
  Mean return: -6.4909
  Mean final entropy: 0.0122
  Max final entropy: 0.0689
  Pseudo loss: -1.69002
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.273 seconds.
  Mean final reward: -0.8050
  Mean return: -7.6084
  Mean final entropy: 0.0056
  Max final entropy: 0.0230
  Pseudo loss: -1.57573
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.284 seconds.
  Mean final reward: -1.1120
  Mean return: -5.9916
  Mean final entropy: 0.0342
  Max final entropy: 0.2616
  Pseudo loss: -2.55596
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: -0.8152
  Mean return: -5.3531
  Mean final entropy: 0.0048
  Max final entropy: 0.0236
  Pseudo loss: -2.65737
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.1496
  Mean return: -3.7599
  Mean final entropy: 0.0005
  Max final entropy: 0.0033
  Pseudo loss: -0.52823
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.286 seconds.
  Mean final reward: -0.1928
  Mean return: -3.6231
  Mean final entropy: 0.0006
  Max final entropy: 0.0047
  Pseudo loss: -3.53340
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.275 seconds.
  Mean final reward: -0.2925
  Mean return: -4.6944
  Mean final entropy: 0.0009
  Max final entropy: 0.0049
  Pseudo loss: -4.51300
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.0978
  Mean return: -3.2626
  Mean final entropy: 0.0005
  Max final entropy: 0.0022
  Pseudo loss: -0.17579
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0322
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.14787
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.136068   0.38135225 0.3794014  0.5904858  0.6633878  0.24860075
 0.6321653  0.15819575]
  Mean final entropy: 0.000133
  Max final entropy: 0.000928
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 3.0572521e-06 -8.7490948e-09  9.1707255e-07  1.4555455e-06
  2.1915930e-06  9.9224326e-07  2.3522296e-07  5.8585613e-07]
  Max Min Entropy: 3.0572521154681453e-06
  Best actions: [array([2, 3, 3, 0, 8, 3, 2, 4]), array([7, 6, 1, 3, 5, 5, 4, 5]), array([2, 1, 3, 8, 8, 3, 1, 6])]

####################################


step: 17
seed: 12089
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.275 seconds.
  Mean final reward: -0.4701
  Mean return: -6.5821
  Mean final entropy: 0.0040
  Max final entropy: 0.0286
  Pseudo loss: 0.03768
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.272 seconds.
  Mean final reward: -0.9944
  Mean return: -7.0169
  Mean final entropy: 0.0061
  Max final entropy: 0.0232
  Pseudo loss: -2.07463
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.6685
  Mean return: -5.8210
  Mean final entropy: 0.0060
  Max final entropy: 0.0411
  Pseudo loss: -3.36611
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.4818
  Mean return: -4.5352
  Mean final entropy: 0.0021
  Max final entropy: 0.0113
  Pseudo loss: -1.56580
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.273 seconds.
  Mean final reward: -0.1887
  Mean return: -4.9753
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -1.63660
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.273 seconds.
  Mean final reward: -0.5978
  Mean return: -4.3205
  Mean final entropy: 0.0066
  Max final entropy: 0.0491
  Pseudo loss: -3.36503
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.272 seconds.
  Mean final reward: -0.1887
  Mean return: -3.4247
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: 0.08185
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.271 seconds.
  Mean final reward: -0.2892
  Mean return: -3.5252
  Mean final entropy: 0.0008
  Max final entropy: 0.0024
  Pseudo loss: -0.05786
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (90/100) took 0.273 seconds.
  Mean final reward: -0.1887
  Mean return: -3.9114
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -0.76683
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.275 seconds.
  Mean final reward: -0.1111
  Mean return: -3.3304
  Mean final entropy: 0.0004
  Max final entropy: 0.0024
  Pseudo loss: -0.08058
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.6288773  0.5486342  0.08211609 0.6248544  0.5878686  0.51663595
 0.3990001  0.23534511]
  Mean final entropy: 0.000487
  Max final entropy: 0.002433
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 4.6077562e-07  3.9529201e-07 -2.0186668e-07  7.2786457e-07
  4.8235428e-08  1.5380917e-06  8.2900925e-08  5.9185786e-06]
  Max Min Entropy: 5.918578608543612e-06
  Best actions: [array([3, 5, 4, 2, 0, 1, 6, 0]), array([5, 3, 3, 1, 2, 0, 4, 6]), array([4, 0, 5, 8, 4, 1, 5, 7])]

####################################


step: 18
seed: 12345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.280 seconds.
  Mean final reward: -1.8611
  Mean return: -7.0057
  Mean final entropy: 0.0230
  Max final entropy: 0.0626
  Pseudo loss: -2.40941
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.288 seconds.
  Mean final reward: -0.5881
  Mean return: -4.4332
  Mean final entropy: 0.0034
  Max final entropy: 0.0197
  Pseudo loss: -1.91695
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.278 seconds.
  Mean final reward: -1.5787
  Mean return: -5.9477
  Mean final entropy: 0.0191
  Max final entropy: 0.0731
  Pseudo loss: 0.06949
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.281 seconds.
  Mean final reward: -0.5201
  Mean return: -4.8325
  Mean final entropy: 0.0081
  Max final entropy: 0.0641
  Pseudo loss: -1.73106
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.286 seconds.
  Mean final reward: -0.5130
  Mean return: -4.6128
  Mean final entropy: 0.0020
  Max final entropy: 0.0102
  Pseudo loss: -2.01173
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.291 seconds.
  Mean final reward: -0.0505
  Mean return: -2.8093
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: -0.89443
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4520
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.86747
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6389
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.09270
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0656
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.42830
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5273
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.00781
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.51143456 0.6105597  0.30092776 0.32688206 0.07879784 0.3808744
 0.26923123 0.39384168]
  Mean final entropy: 0.000057
  Max final entropy: 0.000301
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.0372347e-06 6.9580594e-08 5.7984352e-08 7.3475647e-08 7.9598922e-06
 1.4276135e-06 1.3447481e-06 3.0745858e-07]
  Max Min Entropy: 7.959892172948457e-06
  Best actions: [array([8, 7, 4, 8, 1, 0, 2, 4]), array([6, 8, 7, 4, 4, 2, 1, 5]), array([1, 0, 8, 1, 7, 0, 4, 8])]

####################################


step: 19
seed: 12669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.274 seconds.
  Mean final reward: -1.8449
  Mean return: -9.1151
  Mean final entropy: 0.0302
  Max final entropy: 0.1553
  Pseudo loss: 0.54352
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.277 seconds.
  Mean final reward: -0.4640
  Mean return: -4.6618
  Mean final entropy: 0.0026
  Max final entropy: 0.0183
  Pseudo loss: -1.64275
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.284 seconds.
  Mean final reward: -0.1786
  Mean return: -3.2871
  Mean final entropy: 0.0006
  Max final entropy: 0.0042
  Pseudo loss: -1.31911
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.287 seconds.
  Mean final reward: -0.3632
  Mean return: -3.3996
  Mean final entropy: 0.0023
  Max final entropy: 0.0183
  Pseudo loss: 0.11557
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.272 seconds.
  Mean final reward: -0.5966
  Mean return: -4.2976
  Mean final entropy: 0.0031
  Max final entropy: 0.0183
  Pseudo loss: -4.14028
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6616
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.79633
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2976
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -3.34296
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8276
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -1.15135
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2812
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.01808
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2812
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.01263
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.589431   0.5620564  0.5602576  0.4445774  0.23831972 0.51254976
 0.47692794 0.23674941]
  Mean final entropy: 0.000093
  Max final entropy: 0.000544
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.0145786e-08 9.6450526e-07 8.7820275e-08 6.6351197e-07 1.1047067e-06
 4.9075595e-07 3.5207228e-07 6.1844150e-07]
  Max Min Entropy: 1.1047067118852283e-06
  Best actions: [array([3, 5, 0, 3, 8, 7, 7, 3]), array([2, 4, 6, 1, 2, 6, 1, 7]), array([0, 1, 5, 2, 5, 6, 7, 3])]

####################################


step: 20
seed: 13067
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.272 seconds.
  Mean final reward: -1.0806
  Mean return: -7.5574
  Mean final entropy: 0.0089
  Max final entropy: 0.0542
  Pseudo loss: 1.01102
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.279 seconds.
  Mean final reward: -1.6469
  Mean return: -7.5866
  Mean final entropy: 0.0179
  Max final entropy: 0.0725
  Pseudo loss: 1.47846
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.284 seconds.
  Mean final reward: -1.0043
  Mean return: -6.8153
  Mean final entropy: 0.0081
  Max final entropy: 0.0508
  Pseudo loss: -2.71624
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.281 seconds.
  Mean final reward: -0.5633
  Mean return: -5.5771
  Mean final entropy: 0.0114
  Max final entropy: 0.0906
  Pseudo loss: -1.67272
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.278 seconds.
  Mean final reward: -0.3170
  Mean return: -4.3680
  Mean final entropy: 0.0016
  Max final entropy: 0.0126
  Pseudo loss: 0.00968
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.282 seconds.
  Mean final reward: -0.5557
  Mean return: -5.1308
  Mean final entropy: 0.0024
  Max final entropy: 0.0126
  Pseudo loss: -3.61145
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7751
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.17210
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.5519
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.18003
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6092
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: 0.03733
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9143
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.15069
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.54368883 0.07665768 0.22967413 0.0725438  0.26713276 0.12911962
 0.57932764 0.5063731 ]
  Mean final entropy: 0.000135
  Max final entropy: 0.000944
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.2427803e-05 6.7048387e-07 8.8272898e-08 1.1801338e-07 1.5564953e-04
 3.3383687e-06 5.0202615e-08 1.7520213e-05]
  Max Min Entropy: 0.00015564952627755702
  Best actions: [array([6, 3, 8, 4, 5, 0, 7, 4]), array([7, 5, 6, 5, 4, 3, 6, 5]), array([6, 0, 2, 4, 5, 0, 7, 4])]

####################################


step: 21
seed: 13545
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.277 seconds.
  Mean final reward: -2.0855
  Mean return: -8.7886
  Mean final entropy: 0.0338
  Max final entropy: 0.1478
  Pseudo loss: -2.09647
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.276 seconds.
  Mean final reward: -1.0962
  Mean return: -5.7864
  Mean final entropy: 0.0127
  Max final entropy: 0.0812
  Pseudo loss: -2.97354
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.281 seconds.
  Mean final reward: -1.3916
  Mean return: -6.2153
  Mean final entropy: 0.0183
  Max final entropy: 0.0659
  Pseudo loss: -3.60775
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -3.3313
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -3.21295
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.276 seconds.
  Mean final reward: -0.9454
  Mean return: -4.4962
  Mean final entropy: 0.0047
  Max final entropy: 0.0230
  Pseudo loss: 0.05225
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (60/100) took 0.269 seconds.
  Mean final reward: -0.4395
  Mean return: -3.0614
  Mean final entropy: 0.0032
  Max final entropy: 0.0230
  Pseudo loss: 0.36162
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (70/100) took 0.272 seconds.
  Mean final reward: -0.4142
  Mean return: -3.0362
  Mean final entropy: 0.0031
  Max final entropy: 0.0230
  Pseudo loss: 0.04525
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (80/100) took 0.269 seconds.
  Mean final reward: -0.4142
  Mean return: -3.2298
  Mean final entropy: 0.0032
  Max final entropy: 0.0230
  Pseudo loss: -0.33690
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (90/100) took 0.274 seconds.
  Mean final reward: -0.7754
  Mean return: -4.1305
  Mean final entropy: 0.0058
  Max final entropy: 0.0227
  Pseudo loss: -2.77532
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (100/100) took 0.350 seconds.
  Mean final reward: -0.0224
  Mean return: -2.2798
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: 0.02698
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Testing with greedy policy:
  Start entropy: [0.04647824 0.13149433 0.42152482 0.07464236 0.5206887  0.39186984
 0.2718081  0.10609989]
  Mean final entropy: 0.000157
  Max final entropy: 0.000831
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.8473293e-07 2.5229508e-07 6.6565512e-06 3.4517245e-07 1.9905619e-05
 5.9987719e-06 2.5438871e-07 1.4199959e-05]
  Max Min Entropy: 1.9905619410565123e-05
  Best actions: [array([3, 8, 6, 2, 0, 8, 8, 6]), array([4, 7, 0, 1, 2, 5, 6, 0]), array([3, 2, 6, 2, 4, 4, 7, 6])]

####################################


step: 22
seed: 14109
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.286 seconds.
  Mean final reward: -1.6540
  Mean return: -8.2204
  Mean final entropy: 0.0162
  Max final entropy: 0.0872
  Pseudo loss: 0.04205
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.280 seconds.
  Mean final reward: -1.6271
  Mean return: -8.1873
  Mean final entropy: 0.0129
  Max final entropy: 0.0543
  Pseudo loss: 0.59673
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.272 seconds.
  Mean final reward: -1.0307
  Mean return: -5.5209
  Mean final entropy: 0.0385
  Max final entropy: 0.2999
  Pseudo loss: 1.19414
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.275 seconds.
  Mean final reward: -1.0527
  Mean return: -6.1653
  Mean final entropy: 0.0104
  Max final entropy: 0.0583
  Pseudo loss: -0.47387
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.7041
  Mean return: -6.1317
  Mean final entropy: 0.0350
  Max final entropy: 0.2795
  Pseudo loss: -8.37164
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.272 seconds.
  Mean final reward: -0.3784
  Mean return: -4.6985
  Mean final entropy: 0.0012
  Max final entropy: 0.0050
  Pseudo loss: -2.14537
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.275 seconds.
  Mean final reward: -0.1168
  Mean return: -3.8013
  Mean final entropy: 0.0004
  Max final entropy: 0.0025
  Pseudo loss: -0.28784
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.268 seconds.
  Mean final reward: -0.2952
  Mean return: -3.7824
  Mean final entropy: 0.0009
  Max final entropy: 0.0042
  Pseudo loss: -0.13312
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.268 seconds.
  Mean final reward: -0.1784
  Mean return: -3.5905
  Mean final entropy: 0.0006
  Max final entropy: 0.0042
  Pseudo loss: -0.18070
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.270 seconds.
  Mean final reward: -0.2952
  Mean return: -3.6152
  Mean final entropy: 0.0009
  Max final entropy: 0.0042
  Pseudo loss: -0.38682
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.39453697 0.4287821  0.3604656  0.29634282 0.02739229 0.63334274
 0.07575864 0.0756263 ]
  Mean final entropy: 0.000564
  Max final entropy: 0.004166
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 1.5278319e-07  7.9678802e-07  1.0041764e-06  8.6799795e-05
 -2.0650552e-09  3.9049723e-06  3.2887517e-06  2.0688098e-07]
  Max Min Entropy: 8.679979509906843e-05
  Best actions: [array([1, 2, 8, 3, 5, 8, 7, 0]), array([8, 8, 2, 8, 3, 5, 2, 4]), array([8, 4, 4, 3, 7, 4, 5, 1])]

####################################


step: 23
seed: 14765
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.275 seconds.
  Mean final reward: -0.5428
  Mean return: -5.1796
  Mean final entropy: 0.0024
  Max final entropy: 0.0118
  Pseudo loss: -0.80321
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.281 seconds.
  Mean final reward: -0.9259
  Mean return: -5.8006
  Mean final entropy: 0.0094
  Max final entropy: 0.0570
  Pseudo loss: -7.99315
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.281 seconds.
  Mean final reward: -0.1218
  Mean return: -4.9145
  Mean final entropy: 0.0004
  Max final entropy: 0.0026
  Pseudo loss: -3.82448
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5335
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.13054
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5335
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.09367
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5335
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.05632
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4840
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.18600
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4840
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.06442
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4840
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.00195
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4840
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.02285
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.16214271 0.22646864 0.14798176 0.56312567 0.6008673  0.6693313
 0.3629556  0.30430967]
  Mean final entropy: 0.000076
  Max final entropy: 0.000312
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.2639377e-07 5.5965609e-07 4.3929961e-07 4.9819232e-07 2.9684415e-06
 5.9906489e-07 9.7330577e-08 5.8386490e-06]
  Max Min Entropy: 5.838649030920351e-06
  Best actions: [array([2, 2, 1, 8, 3, 8, 7, 3]), array([0, 0, 3, 2, 0, 2, 1, 5]), array([1, 4, 6, 0, 3, 4, 1, 3])]

####################################


step: 24
seed: 15519
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.286 seconds.
  Mean final reward: -2.5991
  Mean return: -9.6855
  Mean final entropy: 0.0517
  Max final entropy: 0.2983
  Pseudo loss: 0.24284
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.269 seconds.
  Mean final reward: -0.2661
  Mean return: -4.1372
  Mean final entropy: 0.0010
  Max final entropy: 0.0049
  Pseudo loss: -0.20455
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.272 seconds.
  Mean final reward: -0.5769
  Mean return: -4.6078
  Mean final entropy: 0.0024
  Max final entropy: 0.0111
  Pseudo loss: -1.91043
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.266 seconds.
  Mean final reward: -0.5818
  Mean return: -5.4475
  Mean final entropy: 0.0026
  Max final entropy: 0.0146
  Pseudo loss: -9.09487
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.266 seconds.
  Mean final reward: -0.2493
  Mean return: -3.9540
  Mean final entropy: 0.0008
  Max final entropy: 0.0049
  Pseudo loss: -0.01870
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.264 seconds.
  Mean final reward: -0.1990
  Mean return: -3.6519
  Mean final entropy: 0.0007
  Max final entropy: 0.0049
  Pseudo loss: 0.37707
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.267 seconds.
  Mean final reward: -0.2493
  Mean return: -3.9540
  Mean final entropy: 0.0008
  Max final entropy: 0.0049
  Pseudo loss: -0.03024
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.268 seconds.
  Mean final reward: -0.2493
  Mean return: -3.9540
  Mean final entropy: 0.0008
  Max final entropy: 0.0049
  Pseudo loss: -0.04462
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.291 seconds.
  Mean final reward: -0.2493
  Mean return: -3.9540
  Mean final entropy: 0.0008
  Max final entropy: 0.0049
  Pseudo loss: -0.04271
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: -0.2493
  Mean return: -3.9540
  Mean final entropy: 0.0008
  Max final entropy: 0.0049
  Pseudo loss: -0.03227
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.24102858 0.41662508 0.38577798 0.35704345 0.5270947  0.5818033
 0.1598583  0.4489251 ]
  Mean final entropy: 0.000824
  Max final entropy: 0.004915
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [2.6526186e-06 5.5071371e-07 9.2982458e-07 4.3001720e-07 3.1031849e-08
 5.4394843e-07 1.3569525e-07 7.1021026e-07]
  Max Min Entropy: 2.652618604770396e-06
  Best actions: [array([2, 3, 3, 1, 4, 5, 8, 2]), array([8, 5, 5, 5, 5, 4, 2, 1]), array([2, 3, 3, 2, 3, 6, 1, 6])]

####################################


step: 25
seed: 16377
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.271 seconds.
  Mean final reward: -1.4550
  Mean return: -8.2573
  Mean final entropy: 0.0325
  Max final entropy: 0.2410
  Pseudo loss: 0.19788
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.283 seconds.
  Mean final reward: -0.4522
  Mean return: -3.8689
  Mean final entropy: 0.0021
  Max final entropy: 0.0130
  Pseudo loss: -2.95457
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.284 seconds.
  Mean final reward: -0.1526
  Mean return: -2.8417
  Mean final entropy: 0.0005
  Max final entropy: 0.0021
  Pseudo loss: -1.15607
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.00285
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.02087
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.13369
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.07890
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.10650
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.31200
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3574
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.19381
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.25671166 0.25105315 0.10919044 0.3080816  0.53473336 0.23258965
 0.09082369 0.38856518]
  Mean final entropy: 0.000065
  Max final entropy: 0.000416
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.1677792e-09 1.4012134e-06 2.1430774e-06 2.1395308e-06 1.1486474e-06
 3.5312401e-05 1.5098997e-06 3.6311465e-06]
  Max Min Entropy: 3.5312401450937614e-05
  Best actions: [array([3, 1, 1, 3, 2, 6, 2, 2]), array([6, 7, 3, 6, 1, 3, 3, 1]), array([0, 1, 8, 3, 2, 1, 0, 7])]

####################################


step: 26
seed: 17345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.273 seconds.
  Mean final reward: -0.8030
  Mean return: -5.0552
  Mean final entropy: 0.0040
  Max final entropy: 0.0149
  Pseudo loss: -1.28042
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.276 seconds.
  Mean final reward: -0.8466
  Mean return: -5.5785
  Mean final entropy: 0.0135
  Max final entropy: 0.0986
  Pseudo loss: -3.74108
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.0276
  Mean return: -3.0013
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.27902
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1983
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: -0.33743
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2439
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.60202
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.274 seconds.
  Mean final reward: -0.1199
  Mean return: -3.0421
  Mean final entropy: 0.0005
  Max final entropy: 0.0025
  Pseudo loss: -0.21398
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6892
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -1.54837
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.275 seconds.
  Mean final reward: -0.1336
  Mean return: -3.0649
  Mean final entropy: 0.0005
  Max final entropy: 0.0029
  Pseudo loss: -1.11623
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.277 seconds.
  Mean final reward: -0.1944
  Mean return: -3.1994
  Mean final entropy: 0.0008
  Max final entropy: 0.0047
  Pseudo loss: -1.11759
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7311
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.48026
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.48232433 0.1254769  0.53714645 0.47888786 0.65507776 0.62695694
 0.5788009  0.56617707]
  Mean final entropy: 0.000160
  Max final entropy: 0.000653
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.2829838e-07 1.3290791e-07 2.6407264e-07 4.7887417e-05 2.1609874e-07
 3.2323891e-07 9.7787404e-07 1.1452216e-05]
  Max Min Entropy: 4.7887417167657986e-05
  Best actions: [array([3, 3, 5, 0, 6, 8, 8, 2]), array([5, 0, 8, 2, 0, 6, 6, 0]), array([1, 6, 7, 0, 5, 1, 8, 2])]

####################################


step: 27
seed: 18429
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.277 seconds.
  Mean final reward: -2.0320
  Mean return: -9.5941
  Mean final entropy: 0.0238
  Max final entropy: 0.0808
  Pseudo loss: -1.89643
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.269 seconds.
  Mean final reward: -0.1382
  Mean return: -5.6061
  Mean final entropy: 0.0006
  Max final entropy: 0.0028
  Pseudo loss: -0.67919
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.283 seconds.
  Mean final reward: -0.5980
  Mean return: -5.7725
  Mean final entropy: 0.0057
  Max final entropy: 0.0425
  Pseudo loss: -0.96478
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.280 seconds.
  Mean final reward: -0.6878
  Mean return: -6.1652
  Mean final entropy: 0.0048
  Max final entropy: 0.0294
  Pseudo loss: -1.18941
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.277 seconds.
  Mean final reward: -0.1382
  Mean return: -5.0156
  Mean final entropy: 0.0006
  Max final entropy: 0.0028
  Pseudo loss: -0.51894
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.275 seconds.
  Mean final reward: -0.5746
  Mean return: -5.8930
  Mean final entropy: 0.0024
  Max final entropy: 0.0136
  Pseudo loss: -2.95057
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.274 seconds.
  Mean final reward: -0.4419
  Mean return: -6.2254
  Mean final entropy: 0.0017
  Max final entropy: 0.0081
  Pseudo loss: -1.66202
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.278 seconds.
  Mean final reward: -0.1293
  Mean return: -5.0273
  Mean final entropy: 0.0004
  Max final entropy: 0.0028
  Pseudo loss: -0.99986
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.284 seconds.
  Mean final reward: -0.1293
  Mean return: -4.8124
  Mean final entropy: 0.0005
  Max final entropy: 0.0028
  Pseudo loss: -0.16576
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.277 seconds.
  Mean final reward: -0.1293
  Mean return: -4.5638
  Mean final entropy: 0.0004
  Max final entropy: 0.0028
  Pseudo loss: -0.02257
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.22289573 0.37845474 0.5159414  0.31939796 0.32967666 0.2069758
 0.31200448 0.56785405]
  Mean final entropy: 0.000398
  Max final entropy: 0.002812
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [4.6169685e-06 7.4267787e-06 1.4585248e-04 1.8725293e-06 6.0062271e-06
 4.8433885e-06 4.9495910e-08 2.6681280e-05]
  Max Min Entropy: 0.0001458524784538895
  Best actions: [array([0, 5, 1, 3, 3, 5, 0, 1]), array([5, 3, 7, 6, 6, 1, 1, 7]), array([4, 5, 1, 0, 3, 5, 3, 1])]

####################################


step: 28
seed: 19635
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.261 seconds.
  Mean final reward: -1.6546
  Mean return: -8.0632
  Mean final entropy: 0.0213
  Max final entropy: 0.1051
  Pseudo loss: -2.05635
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.266 seconds.
  Mean final reward: -1.0106
  Mean return: -6.1095
  Mean final entropy: 0.0144
  Max final entropy: 0.0960
  Pseudo loss: -2.71049
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.266 seconds.
  Mean final reward: -0.8086
  Mean return: -6.1839
  Mean final entropy: 0.0051
  Max final entropy: 0.0283
  Pseudo loss: -1.44374
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.267 seconds.
  Mean final reward: -0.4122
  Mean return: -4.9944
  Mean final entropy: 0.0016
  Max final entropy: 0.0082
  Pseudo loss: -0.47397
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.261 seconds.
  Mean final reward: -0.1855
  Mean return: -4.0718
  Mean final entropy: 0.0006
  Max final entropy: 0.0033
  Pseudo loss: -0.68751
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.269 seconds.
  Mean final reward: -0.5457
  Mean return: -4.3340
  Mean final entropy: 0.0035
  Max final entropy: 0.0238
  Pseudo loss: -1.75966
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.267 seconds.
  Mean final reward: -0.5457
  Mean return: -4.1732
  Mean final entropy: 0.0035
  Max final entropy: 0.0238
  Pseudo loss: -3.19842
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.264 seconds.
  Mean final reward: -0.1496
  Mean return: -3.4760
  Mean final entropy: 0.0005
  Max final entropy: 0.0033
  Pseudo loss: 0.16295
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.268 seconds.
  Mean final reward: -0.1496
  Mean return: -3.3811
  Mean final entropy: 0.0005
  Max final entropy: 0.0033
  Pseudo loss: 0.05132
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.266 seconds.
  Mean final reward: -0.4484
  Mean return: -4.3006
  Mean final entropy: 0.0014
  Max final entropy: 0.0053
  Pseudo loss: -2.12592
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.18636233 0.17721136 0.28492275 0.51472056 0.38994995 0.64564383
 0.28188622 0.28105697]
  Mean final entropy: 0.000461
  Max final entropy: 0.003310
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [6.7008976e-08 6.7730872e-07 3.2422970e-06 1.5125823e-06 3.1453447e-07
 2.1309906e-07 2.1017247e-06 2.6682748e-05]
  Max Min Entropy: 2.6682748284656554e-05
  Best actions: [array([6, 5, 7, 7, 8, 5, 2, 3]), array([3, 7, 6, 6, 6, 3, 8, 5]), array([1, 5, 7, 7, 0, 4, 5, 3])]

####################################


step: 29
seed: 20969
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.275 seconds.
  Mean final reward: -1.6069
  Mean return: -9.1699
  Mean final entropy: 0.0151
  Max final entropy: 0.0442
  Pseudo loss: -2.99730
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.266 seconds.
  Mean final reward: -1.2601
  Mean return: -6.9358
  Mean final entropy: 0.0117
  Max final entropy: 0.0468
  Pseudo loss: -1.39290
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.265 seconds.
  Mean final reward: -1.2538
  Mean return: -7.6269
  Mean final entropy: 0.0125
  Max final entropy: 0.0688
  Pseudo loss: -2.65796
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.280 seconds.
  Mean final reward: -0.7669
  Mean return: -5.9286
  Mean final entropy: 0.0093
  Max final entropy: 0.0680
  Pseudo loss: -2.29240
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.271 seconds.
  Mean final reward: -0.0178
  Mean return: -4.2264
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: 0.03032
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.266 seconds.
  Mean final reward: -0.0959
  Mean return: -4.4073
  Mean final entropy: 0.0003
  Max final entropy: 0.0022
  Pseudo loss: -1.22553
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1134
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.47777
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.273 seconds.
  Mean final reward: -0.2031
  Mean return: -4.0435
  Mean final entropy: 0.0006
  Max final entropy: 0.0051
  Pseudo loss: -0.68453
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9571
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.01351
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9571
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.01717
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.3146898  0.2998522  0.44521582 0.54004204 0.24115145 0.18622914
 0.26941088 0.5694567 ]
  Mean final entropy: 0.000051
  Max final entropy: 0.000159
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.3121781e-06 2.1868271e-05 1.3209874e-08 3.8219863e-08 2.0629486e-06
 4.0813674e-07 1.0620244e-07 3.6613633e-06]
  Max Min Entropy: 2.1868270778213628e-05
  Best actions: [array([0, 4, 0, 6, 7, 1, 2, 6]), array([6, 6, 6, 3, 1, 7, 0, 3]), array([7, 0, 1, 4, 7, 8, 1, 4])]

####################################


step: 30
seed: 22437
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.269 seconds.
  Mean final reward: -2.2477
  Mean return: -11.2597
  Mean final entropy: 0.0340
  Max final entropy: 0.2031
  Pseudo loss: 0.34212
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.278 seconds.
  Mean final reward: -1.3652
  Mean return: -7.3859
  Mean final entropy: 0.0111
  Max final entropy: 0.0450
  Pseudo loss: -2.80803
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.268 seconds.
  Mean final reward: -0.7814
  Mean return: -5.3263
  Mean final entropy: 0.0058
  Max final entropy: 0.0299
  Pseudo loss: 0.50302
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.270 seconds.
  Mean final reward: -0.8926
  Mean return: -4.7431
  Mean final entropy: 0.0098
  Max final entropy: 0.0552
  Pseudo loss: -1.97987
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.270 seconds.
  Mean final reward: -0.1875
  Mean return: -4.5353
  Mean final entropy: 0.0008
  Max final entropy: 0.0040
  Pseudo loss: -1.84489
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.2049
  Mean return: -3.6152
  Mean final entropy: 0.0008
  Max final entropy: 0.0052
  Pseudo loss: -1.22341
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.276 seconds.
  Mean final reward: -0.6787
  Mean return: -5.3587
  Mean final entropy: 0.0254
  Max final entropy: 0.2014
  Pseudo loss: -5.09422
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.272 seconds.
  Mean final reward: -0.4500
  Mean return: -4.2494
  Mean final entropy: 0.0027
  Max final entropy: 0.0188
  Pseudo loss: -0.61145
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.262 seconds.
  Mean final reward: -0.6428
  Mean return: -5.2983
  Mean final entropy: 0.0150
  Max final entropy: 0.1172
  Pseudo loss: -4.64699
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.268 seconds.
  Mean final reward: -0.0629
  Mean return: -3.6008
  Mean final entropy: 0.0004
  Max final entropy: 0.0015
  Pseudo loss: -0.32326
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.17026922 0.5418221  0.42785046 0.29684463 0.53076    0.45854092
 0.33372325 0.32720536]
  Mean final entropy: 0.005578
  Max final entropy: 0.042538
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [-4.5177480e-09  6.2635017e-06  5.9143480e-07  7.7623423e-05
 -2.1986399e-09  3.5976103e-05  3.6494811e-07  1.9159384e-06]
  Max Min Entropy: 7.762342283967882e-05
  Best actions: [array([2, 0, 2, 1, 6, 4, 4, 4]), array([7, 2, 0, 0, 7, 7, 1, 5]), array([0, 4, 1, 6, 5, 4, 4, 4])]

####################################


step: 31
seed: 24045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.272 seconds.
  Mean final reward: -1.3680
  Mean return: -8.1256
  Mean final entropy: 0.0120
  Max final entropy: 0.0691
  Pseudo loss: -0.48867
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.281 seconds.
  Mean final reward: -1.2032
  Mean return: -7.7633
  Mean final entropy: 0.0126
  Max final entropy: 0.0649
  Pseudo loss: -7.22951
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.275 seconds.
  Mean final reward: -1.3653
  Mean return: -6.6845
  Mean final entropy: 0.0230
  Max final entropy: 0.1264
  Pseudo loss: -11.10961
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.272 seconds.
  Mean final reward: -0.5539
  Mean return: -4.3900
  Mean final entropy: 0.0068
  Max final entropy: 0.0515
  Pseudo loss: -0.48190
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.273 seconds.
  Mean final reward: -0.2753
  Mean return: -3.9452
  Mean final entropy: 0.0012
  Max final entropy: 0.0090
  Pseudo loss: -0.13226
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.270 seconds.
  Mean final reward: -0.2753
  Mean return: -4.3969
  Mean final entropy: 0.0013
  Max final entropy: 0.0090
  Pseudo loss: -0.73299
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.278 seconds.
  Mean final reward: -0.0612
  Mean return: -3.5348
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: -0.06854
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.278 seconds.
  Mean final reward: -0.0612
  Mean return: -3.3558
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: 0.03653
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.275 seconds.
  Mean final reward: -0.5539
  Mean return: -4.4232
  Mean final entropy: 0.0068
  Max final entropy: 0.0515
  Pseudo loss: -4.52129
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.278 seconds.
  Mean final reward: -0.0650
  Mean return: -3.5269
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -1.46693
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.12629132 0.5687772  0.39924493 0.32687825 0.43583977 0.26857173
 0.29354477 0.3012815 ]
  Mean final entropy: 0.000237
  Max final entropy: 0.001632
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [6.5933861e-08 9.6222102e-05 1.2874898e-06 1.3772650e-06 3.3476388e-06
 9.3573581e-05 3.8193312e-07 1.4935919e-05]
  Max Min Entropy: 9.622210200177506e-05
  Best actions: [array([0, 1, 4, 4, 4, 3, 5, 7]), array([2, 7, 5, 5, 3, 7, 3, 1]), array([5, 1, 4, 4, 4, 1, 7, 7])]

####################################


step: 32
seed: 25799
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.262 seconds.
  Mean final reward: -2.2280
  Mean return: -10.9064
  Mean final entropy: 0.0530
  Max final entropy: 0.3122
  Pseudo loss: 0.45113
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.260 seconds.
  Mean final reward: -0.9815
  Mean return: -7.2918
  Mean final entropy: 0.0083
  Max final entropy: 0.0493
  Pseudo loss: 2.80518
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.282 seconds.
  Mean final reward: -1.5769
  Mean return: -7.5026
  Mean final entropy: 0.0405
  Max final entropy: 0.2942
  Pseudo loss: -0.00956
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.273 seconds.
  Mean final reward: -0.8476
  Mean return: -6.2864
  Mean final entropy: 0.0066
  Max final entropy: 0.0355
  Pseudo loss: -1.58496
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.272 seconds.
  Mean final reward: -0.4052
  Mean return: -4.4939
  Mean final entropy: 0.0021
  Max final entropy: 0.0131
  Pseudo loss: -0.98585
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.267 seconds.
  Mean final reward: -0.7912
  Mean return: -5.5031
  Mean final entropy: 0.0124
  Max final entropy: 0.0939
  Pseudo loss: -7.07612
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.280 seconds.
  Mean final reward: -0.2796
  Mean return: -4.4715
  Mean final entropy: 0.0010
  Max final entropy: 0.0045
  Pseudo loss: -1.85074
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.278 seconds.
  Mean final reward: -0.2242
  Mean return: -4.4984
  Mean final entropy: 0.0009
  Max final entropy: 0.0030
  Pseudo loss: -1.16210
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.0380
  Mean return: -3.4976
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -0.19797
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.274 seconds.
  Mean final reward: -0.0683
  Mean return: -3.8574
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -0.80162
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.20800811 0.43085074 0.51757205 0.48245746 0.4569833  0.3497115
 0.34133613 0.62941515]
  Mean final entropy: 0.000287
  Max final entropy: 0.001356
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [7.4149352e-07 3.2109858e-06 1.1220423e-07 1.5792898e-06 1.2363671e-07
 6.7684761e-07 4.4297682e-08 9.6334702e-08]
  Max Min Entropy: 3.210985823898227e-06
  Best actions: [array([4, 4, 8, 4, 3, 6, 6, 3]), array([1, 5, 5, 1, 4, 3, 3, 4]), array([6, 0, 7, 4, 0, 6, 1, 4])]

####################################


step: 33
seed: 27705
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.278 seconds.
  Mean final reward: -0.5953
  Mean return: -5.8571
  Mean final entropy: 0.0023
  Max final entropy: 0.0101
  Pseudo loss: -2.08329
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.300 seconds.
  Mean final reward: -1.1779
  Mean return: -6.4520
  Mean final entropy: 0.0076
  Max final entropy: 0.0366
  Pseudo loss: -1.03768
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.275 seconds.
  Mean final reward: -0.8101
  Mean return: -4.8077
  Mean final entropy: 0.0057
  Max final entropy: 0.0357
  Pseudo loss: -1.68145
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.278 seconds.
  Mean final reward: -0.4405
  Mean return: -3.8970
  Mean final entropy: 0.0015
  Max final entropy: 0.0064
  Pseudo loss: -1.41530
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.268 seconds.
  Mean final reward: -0.1944
  Mean return: -3.0800
  Mean final entropy: 0.0006
  Max final entropy: 0.0025
  Pseudo loss: -0.26224
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.276 seconds.
  Mean final reward: -0.5389
  Mean return: -3.8227
  Mean final entropy: 0.0025
  Max final entropy: 0.0122
  Pseudo loss: -1.32545
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.270 seconds.
  Mean final reward: -0.4310
  Mean return: -3.4103
  Mean final entropy: 0.0015
  Max final entropy: 0.0061
  Pseudo loss: -0.35646
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.277 seconds.
  Mean final reward: -0.0784
  Mean return: -4.2822
  Mean final entropy: 0.0005
  Max final entropy: 0.0019
  Pseudo loss: -4.18817
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.270 seconds.
  Mean final reward: -0.4240
  Mean return: -3.4249
  Mean final entropy: 0.0023
  Max final entropy: 0.0159
  Pseudo loss: -0.59958
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.272 seconds.
  Mean final reward: -0.1997
  Mean return: -3.7102
  Mean final entropy: 0.0007
  Max final entropy: 0.0023
  Pseudo loss: -1.24652
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Testing with greedy policy:
  Start entropy: [0.35717425 0.5985677  0.5365733  0.3387798  0.33360067 0.2551177
 0.56097144 0.55319273]
  Mean final entropy: 0.000666
  Max final entropy: 0.002530
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [ 2.5936359e-07  1.5629421e-05  2.9812133e-04  5.0759463e-06
  3.5672805e-08  6.5995658e-07  2.8328384e-07 -1.8626862e-08]
  Max Min Entropy: 0.0002981213328894228
  Best actions: [array([8, 8, 0, 7, 0, 1, 4, 3]), array([7, 7, 2, 6, 5, 7, 3, 5]), array([2, 0, 0, 7, 4, 5, 8, 0])]

####################################


step: 34
seed: 29769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.290 seconds.
  Mean final reward: -1.9767
  Mean return: -8.4033
  Mean final entropy: 0.0239
  Max final entropy: 0.0836
  Pseudo loss: -0.79509
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.277 seconds.
  Mean final reward: -1.1581
  Mean return: -7.5878
  Mean final entropy: 0.0129
  Max final entropy: 0.0726
  Pseudo loss: -1.48665
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.284 seconds.
  Mean final reward: -1.2397
  Mean return: -7.0360
  Mean final entropy: 0.0164
  Max final entropy: 0.1156
  Pseudo loss: -2.67614
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.298 seconds.
  Mean final reward: -1.0714
  Mean return: -5.5612
  Mean final entropy: 0.0098
  Max final entropy: 0.0557
  Pseudo loss: -2.39465
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.287 seconds.
  Mean final reward: -0.0523
  Mean return: -2.7202
  Mean final entropy: 0.0005
  Max final entropy: 0.0014
  Pseudo loss: -2.41982
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.0387
  Mean return: -3.1968
  Mean final entropy: 0.0002
  Max final entropy: 0.0014
  Pseudo loss: -1.05588
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.279 seconds.
  Mean final reward: -0.1539
  Mean return: -2.2085
  Mean final entropy: 0.0005
  Max final entropy: 0.0025
  Pseudo loss: 0.42606
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Episode (80/100) took 0.277 seconds.
  Mean final reward: -0.1539
  Mean return: -2.4324
  Mean final entropy: 0.0006
  Max final entropy: 0.0025
  Pseudo loss: -0.09338
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (90/100) took 0.277 seconds.
  Mean final reward: -0.1539
  Mean return: -2.2085
  Mean final entropy: 0.0005
  Max final entropy: 0.0025
  Pseudo loss: 0.19969
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Episode (100/100) took 0.281 seconds.
  Mean final reward: -0.1539
  Mean return: -2.2085
  Mean final entropy: 0.0005
  Max final entropy: 0.0025
  Pseudo loss: 0.10834
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Testing with greedy policy:
  Start entropy: [0.51320547 0.11602104 0.28394204 0.59927    0.30355358 0.13014305
 0.33214375 0.09768979]
  Mean final entropy: 0.000492
  Max final entropy: 0.002513
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [ 2.8427819e-06  1.2992066e-07  1.9007607e-07  6.2557956e-07
  2.8972432e-04 -1.2188697e-07  1.5805546e-07  1.1772223e-07]
  Max Min Entropy: 0.0002897243248298764
  Best actions: [array([0, 1, 8, 8, 3, 2, 4, 0]), array([1, 0, 2, 2, 6, 1, 5, 2]), array([6, 0, 4, 0, 3, 1, 0, 1])]

####################################


step: 35
seed: 31997
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.267 seconds.
  Mean final reward: -1.3771
  Mean return: -9.6191
  Mean final entropy: 0.0137
  Max final entropy: 0.0549
  Pseudo loss: -0.41175
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.281 seconds.
  Mean final reward: -0.9021
  Mean return: -6.7981
  Mean final entropy: 0.0034
  Max final entropy: 0.0113
  Pseudo loss: -0.25338
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9985
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.26925
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.279 seconds.
  Mean final reward: -0.3951
  Mean return: -3.3381
  Mean final entropy: 0.0018
  Max final entropy: 0.0122
  Pseudo loss: -1.54568
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.269 seconds.
  Mean final reward: -0.1590
  Mean return: -3.4961
  Mean final entropy: 0.0006
  Max final entropy: 0.0020
  Pseudo loss: -1.47181
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.0963
  Mean return: -2.9112
  Mean final entropy: 0.0004
  Max final entropy: 0.0018
  Pseudo loss: -2.03201
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2190
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -1.38095
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.280 seconds.
  Mean final reward: -0.0808
  Mean return: -2.2472
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: 0.17739
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.0808
  Mean return: -2.2472
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: -0.38975
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8809
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.04731
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.48827457 0.08637472 0.3494646  0.48312443 0.23460749 0.2584623
 0.0355786  0.26902065]
  Mean final entropy: 0.000039
  Max final entropy: 0.000152
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 5.8625702e-08  6.0019242e-06  1.1093706e-06  1.1206848e-05
 -9.0556036e-08  9.8672523e-08  1.1427490e-05  1.6655746e-05]
  Max Min Entropy: 1.6655745639582165e-05
  Best actions: [array([4, 6, 7, 4, 7, 5, 1, 1]), array([7, 3, 8, 5, 1, 4, 4, 3]), array([4, 6, 0, 4, 3, 6, 1, 6])]

####################################


step: 36
seed: 34395
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.273 seconds.
  Mean final reward: -1.2812
  Mean return: -7.1776
  Mean final entropy: 0.0065
  Max final entropy: 0.0153
  Pseudo loss: 0.25944
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.280 seconds.
  Mean final reward: -1.9202
  Mean return: -7.5178
  Mean final entropy: 0.0728
  Max final entropy: 0.4650
  Pseudo loss: -3.76067
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.279 seconds.
  Mean final reward: -1.2658
  Mean return: -7.5685
  Mean final entropy: 0.0104
  Max final entropy: 0.0446
  Pseudo loss: -1.43257
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.279 seconds.
  Mean final reward: -0.1073
  Mean return: -3.4090
  Mean final entropy: 0.0004
  Max final entropy: 0.0024
  Pseudo loss: -0.81141
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.276 seconds.
  Mean final reward: -0.0851
  Mean return: -2.2519
  Mean final entropy: 0.0004
  Max final entropy: 0.0020
  Pseudo loss: 0.61209
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.283 seconds.
  Mean final reward: -0.4323
  Mean return: -3.1862
  Mean final entropy: 0.0040
  Max final entropy: 0.0318
  Pseudo loss: -2.39969
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.282 seconds.
  Mean final reward: -0.0913
  Mean return: -2.4853
  Mean final entropy: 0.0005
  Max final entropy: 0.0020
  Pseudo loss: -0.51366
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.281 seconds.
  Mean final reward: -0.3266
  Mean return: -3.1820
  Mean final entropy: 0.0017
  Max final entropy: 0.0136
  Pseudo loss: -2.38979
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.285 seconds.
  Mean final reward: -0.0062
  Mean return: -2.2865
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: 0.00632
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.280 seconds.
  Mean final reward: -0.0062
  Mean return: -2.2865
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.00825
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.61792123 0.04784407 0.40873253 0.5584007  0.17704609 0.24182665
 0.5128986  0.65643036]
  Mean final entropy: 0.000110
  Max final entropy: 0.000615
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.4375722e-07  5.7117325e-07 -1.7014423e-07  2.2948129e-07
  8.5186940e-07  2.2081215e-07  1.7130675e-07  2.5903244e-08]
  Max Min Entropy: 8.518694016856898e-07
  Best actions: [array([3, 3, 5, 7, 3, 7, 1, 3]), array([6, 6, 3, 6, 2, 6, 7, 6]), array([0, 3, 7, 7, 3, 0, 0, 2])]

####################################


step: 37
seed: 36969
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.275 seconds.
  Mean final reward: -1.7507
  Mean return: -8.6727
  Mean final entropy: 0.0187
  Max final entropy: 0.0618
  Pseudo loss: -0.53470
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.269 seconds.
  Mean final reward: -1.6563
  Mean return: -7.9604
  Mean final entropy: 0.0253
  Max final entropy: 0.1178
  Pseudo loss: 0.80374
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.274 seconds.
  Mean final reward: -1.0867
  Mean return: -6.8500
  Mean final entropy: 0.0065
  Max final entropy: 0.0242
  Pseudo loss: -5.62623
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.271 seconds.
  Mean final reward: -0.9377
  Mean return: -6.1235
  Mean final entropy: 0.0202
  Max final entropy: 0.1541
  Pseudo loss: -5.77592
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.271 seconds.
  Mean final reward: -0.0134
  Mean return: -4.2510
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: 0.06679
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.0134
  Mean return: -4.4067
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -0.17789
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.271 seconds.
  Mean final reward: -0.1271
  Mean return: -4.7383
  Mean final entropy: 0.0005
  Max final entropy: 0.0025
  Pseudo loss: -1.62655
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.269 seconds.
  Mean final reward: -0.0134
  Mean return: -4.2510
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.00170
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.269 seconds.
  Mean final reward: -0.0134
  Mean return: -4.2510
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: 0.00504
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.268 seconds.
  Mean final reward: -0.0134
  Mean return: -4.2510
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: 0.00648
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.22214673 0.52412486 0.16555336 0.4697311  0.26265562 0.44468808
 0.45136487 0.52554846]
  Mean final entropy: 0.000203
  Max final entropy: 0.001113
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 3.3045497e-05  1.2336670e-07  2.3473299e-07  2.6499978e-04
  2.3354119e-06  1.7423646e-06 -5.3994569e-08  7.9072546e-05]
  Max Min Entropy: 0.0002649997768457979
  Best actions: [array([2, 2, 7, 7, 0, 0, 6, 8]), array([5, 0, 1, 8, 8, 6, 3, 5]), array([2, 4, 5, 2, 0, 5, 1, 3])]

####################################


step: 38
seed: 39725
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.325 seconds.
  Mean final reward: -0.6270
  Mean return: -4.8134
  Mean final entropy: 0.0032
  Max final entropy: 0.0154
  Pseudo loss: 0.13848
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.293 seconds.
  Mean final reward: -0.6190
  Mean return: -4.9679
  Mean final entropy: 0.0040
  Max final entropy: 0.0259
  Pseudo loss: -5.74121
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.275 seconds.
  Mean final reward: -0.6816
  Mean return: -4.4184
  Mean final entropy: 0.0085
  Max final entropy: 0.0627
  Pseudo loss: -5.95595
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.284 seconds.
  Mean final reward: -0.2369
  Mean return: -3.4525
  Mean final entropy: 0.0008
  Max final entropy: 0.0042
  Pseudo loss: -1.09998
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.278 seconds.
  Mean final reward: -0.1059
  Mean return: -2.8231
  Mean final entropy: 0.0004
  Max final entropy: 0.0023
  Pseudo loss: -0.29028
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.276 seconds.
  Mean final reward: -0.1493
  Mean return: -3.0911
  Mean final entropy: 0.0004
  Max final entropy: 0.0033
  Pseudo loss: -0.91335
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: -0.3542
  Mean return: -4.1797
  Mean final entropy: 0.0012
  Max final entropy: 0.0067
  Pseudo loss: -2.11349
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.279 seconds.
  Mean final reward: -0.0523
  Mean return: -2.9480
  Mean final entropy: 0.0002
  Max final entropy: 0.0015
  Pseudo loss: -0.38561
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.277 seconds.
  Mean final reward: -0.1059
  Mean return: -2.4821
  Mean final entropy: 0.0003
  Max final entropy: 0.0023
  Pseudo loss: 0.09148
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4083
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.47334
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.251796   0.45789224 0.13441683 0.5266731  0.31999862 0.31691676
 0.6280445  0.01874727]
  Mean final entropy: 0.000365
  Max final entropy: 0.002333
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [6.5914592e-06 3.2608125e-07 6.7330902e-08 6.2375398e-07 1.0430824e-05
 2.9886800e-07 4.6645783e-07 3.2992435e-07]
  Max Min Entropy: 1.043082374962978e-05
  Best actions: [array([5, 6, 0, 3, 3, 1, 8, 3]), array([4, 8, 6, 5, 6, 4, 2, 7]), array([5, 2, 3, 7, 1, 1, 5, 8])]

####################################


step: 39
seed: 42669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.283 seconds.
  Mean final reward: -2.5218
  Mean return: -9.0425
  Mean final entropy: 0.0505
  Max final entropy: 0.2146
  Pseudo loss: 1.95820
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.9236
  Mean return: -5.6598
  Mean final entropy: 0.0121
  Max final entropy: 0.0745
  Pseudo loss: -2.42303
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.280 seconds.
  Mean final reward: -1.1259
  Mean return: -6.1818
  Mean final entropy: 0.0259
  Max final entropy: 0.1528
  Pseudo loss: -6.46550
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3285
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.59759
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.282 seconds.
  Mean final reward: -0.1593
  Mean return: -2.7138
  Mean final entropy: 0.0006
  Max final entropy: 0.0036
  Pseudo loss: -3.47369
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.281 seconds.
  Mean final reward: -0.0209
  Mean return: -1.8070
  Mean final entropy: 0.0004
  Max final entropy: 0.0012
  Pseudo loss: 0.04536
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.278 seconds.
  Mean final reward: -0.0360
  Mean return: -1.9143
  Mean final entropy: 0.0002
  Max final entropy: 0.0013
  Pseudo loss: -0.23839
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7861
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.09821
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7861
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.01271
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7861
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.00097
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.28191432 0.47405863 0.4410162  0.432478   0.32633865 0.48266515
 0.23394212 0.3707316 ]
  Mean final entropy: 0.000223
  Max final entropy: 0.000891
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.07303777e-06 2.28446652e-06 1.13423390e-03 7.78343383e-06
 4.10152716e-05 2.88144037e-07 1.07146275e-06 1.08000706e-04]
  Max Min Entropy: 0.0011342338984832168
  Best actions: None

####################################


step: 40
seed: 45807
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.274 seconds.
  Mean final reward: -0.6363
  Mean return: -4.5464
  Mean final entropy: 0.0031
  Max final entropy: 0.0140
  Pseudo loss: -0.13352
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.281 seconds.
  Mean final reward: -0.8974
  Mean return: -4.0362
  Mean final entropy: 0.0112
  Max final entropy: 0.0771
  Pseudo loss: 1.21077
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.284 seconds.
  Mean final reward: -1.2819
  Mean return: -5.3181
  Mean final entropy: 0.0141
  Max final entropy: 0.0771
  Pseudo loss: -3.05680
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.292 seconds.
  Mean final reward: -1.4711
  Mean return: -6.1599
  Mean final entropy: 0.0104
  Max final entropy: 0.0357
  Pseudo loss: -3.46956
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (50/100) took 0.279 seconds.
  Mean final reward: -0.1760
  Mean return: -2.1652
  Mean final entropy: 0.0006
  Max final entropy: 0.0041
  Pseudo loss: -0.45835
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.283 seconds.
  Mean final reward: -0.4470
  Mean return: -3.0035
  Mean final entropy: 0.0046
  Max final entropy: 0.0357
  Pseudo loss: -2.34690
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.283 seconds.
  Mean final reward: -0.2547
  Mean return: -2.5192
  Mean final entropy: 0.0009
  Max final entropy: 0.0055
  Pseudo loss: -1.67555
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (80/100) took 0.277 seconds.
  Mean final reward: -0.2093
  Mean return: -2.3533
  Mean final entropy: 0.0007
  Max final entropy: 0.0038
  Pseudo loss: -1.45350
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (90/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8955
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.03223
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8955
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.02262
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.5428634  0.20643468 0.2357664  0.41031113 0.11700587 0.00978512
 0.53302926 0.1536851 ]
  Mean final entropy: 0.000073
  Max final entropy: 0.000294
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 9.3387229e-08  3.9995479e-04  2.0669218e-06  8.1466943e-08
  3.9327442e-06  3.9810391e-07 -4.2634955e-08  1.4962991e-06]
  Max Min Entropy: 0.00039995479164645076
  Best actions: [array([5, 2, 1, 3, 4, 6, 5, 2]), array([4, 1, 6, 5, 7, 7, 3, 0]), array([3, 8, 3, 6, 2, 5, 8, 6])]

####################################


step: 41
seed: 49145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.281 seconds.
  Mean final reward: -2.3945
  Mean return: -8.8950
  Mean final entropy: 0.0565
  Max final entropy: 0.2919
  Pseudo loss: -1.57110
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.293 seconds.
  Mean final reward: -0.9124
  Mean return: -4.7185
  Mean final entropy: 0.0039
  Max final entropy: 0.0121
  Pseudo loss: -2.02783
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.4507
  Mean return: -3.3707
  Mean final entropy: 0.0019
  Max final entropy: 0.0110
  Pseudo loss: -1.93307
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.280 seconds.
  Mean final reward: -0.6817
  Mean return: -3.4697
  Mean final entropy: 0.0207
  Max final entropy: 0.1639
  Pseudo loss: -4.76259
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8827
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00149
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0421
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.02960
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8827
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00571
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8827
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.00089
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8827
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.03342
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8227
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.02076
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.2035397  0.3870981  0.41859978 0.12515047 0.29102036 0.24306934
 0.00974193 0.6651116 ]
  Mean final entropy: 0.000035
  Max final entropy: 0.000138
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.2359539e-07 3.3069622e-08 1.8240343e-06 4.2723386e-07 1.2766581e-05
 5.9780410e-07 1.2894938e-07 1.3051578e-06]
  Max Min Entropy: 1.27665807667654e-05
  Best actions: [array([8, 4, 4, 7, 2, 1, 7, 4]), array([7, 3, 1, 1, 1, 4, 1, 7]), array([3, 6, 4, 7, 2, 1, 4, 4])]

####################################


step: 42
seed: 52689
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.1720
  Mean return: -6.4694
  Mean final entropy: 0.0250
  Max final entropy: 0.1848
  Pseudo loss: 1.46251
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.311 seconds.
  Mean final reward: -0.9002
  Mean return: -5.0372
  Mean final entropy: 0.0261
  Max final entropy: 0.2023
  Pseudo loss: 0.32414
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.5756
  Mean return: -4.1000
  Mean final entropy: 0.0028
  Max final entropy: 0.0132
  Pseudo loss: 0.31047
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.278 seconds.
  Mean final reward: -0.7457
  Mean return: -4.8825
  Mean final entropy: 0.0092
  Max final entropy: 0.0668
  Pseudo loss: -2.70025
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.289 seconds.
  Mean final reward: -0.2205
  Mean return: -3.2216
  Mean final entropy: 0.0008
  Max final entropy: 0.0058
  Pseudo loss: -1.91232
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5300
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -2.75118
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.273 seconds.
  Mean final reward: -0.2078
  Mean return: -2.4926
  Mean final entropy: 0.0008
  Max final entropy: 0.0053
  Pseudo loss: -0.20853
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8434
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.02949
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8434
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.02090
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7313
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.10895
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.12140991 0.32198098 0.45473891 0.20929918 0.3525951  0.2596941
 0.30141503 0.07190952]
  Mean final entropy: 0.000220
  Max final entropy: 0.000930
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.7140177e-07 3.6975933e-07 1.5030378e-05 2.6894138e-06 1.7347680e-06
 5.9941747e-05 1.0394257e-07 6.9165651e-07]
  Max Min Entropy: 5.994174716761336e-05
  Best actions: [array([8, 3, 7, 8, 8, 7, 6, 0]), array([2, 4, 6, 6, 3, 4, 0, 2]), array([2, 5, 7, 8, 8, 7, 8, 0])]

####################################


step: 43
seed: 56445
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.269 seconds.
  Mean final reward: -1.3176
  Mean return: -6.8629
  Mean final entropy: 0.0192
  Max final entropy: 0.1169
  Pseudo loss: 1.25566
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.274 seconds.
  Mean final reward: -1.9065
  Mean return: -8.4300
  Mean final entropy: 0.0225
  Max final entropy: 0.0833
  Pseudo loss: 1.52179
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.277 seconds.
  Mean final reward: -1.9261
  Mean return: -8.0866
  Mean final entropy: 0.0379
  Max final entropy: 0.2288
  Pseudo loss: -6.84636
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.275 seconds.
  Mean final reward: -0.4621
  Mean return: -5.6164
  Mean final entropy: 0.0027
  Max final entropy: 0.0181
  Pseudo loss: -1.21286
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.272 seconds.
  Mean final reward: -0.1267
  Mean return: -5.0104
  Mean final entropy: 0.0007
  Max final entropy: 0.0021
  Pseudo loss: -0.59328
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.279 seconds.
  Mean final reward: -0.3786
  Mean return: -4.9930
  Mean final entropy: 0.0021
  Max final entropy: 0.0131
  Pseudo loss: -0.00909
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.281 seconds.
  Mean final reward: -0.8448
  Mean return: -6.5045
  Mean final entropy: 0.0040
  Max final entropy: 0.0131
  Pseudo loss: -1.06433
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.286 seconds.
  Mean final reward: -0.0265
  Mean return: -4.5975
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -0.01814
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.0265
  Mean return: -4.5975
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -0.01571
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.285 seconds.
  Mean final reward: -0.3244
  Mean return: -4.8954
  Mean final entropy: 0.0018
  Max final entropy: 0.0134
  Pseudo loss: -0.36553
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.12965673 0.65156794 0.31124872 0.24140455 0.20951429 0.5298332
 0.5996409  0.68098605]
  Mean final entropy: 0.000175
  Max final entropy: 0.001236
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 3.2641208e-07 -1.1734729e-08  3.1200094e-07  1.1687473e-06
 -1.3800098e-08  2.7816533e-05  3.1982461e-06  1.9528995e-06]
  Max Min Entropy: 2.7816533474833705e-05
  Best actions: [array([3, 8, 8, 7, 4, 5, 0, 5]), array([6, 7, 2, 1, 5, 2, 6, 3]), array([3, 0, 4, 3, 3, 7, 0, 0])]

####################################


step: 44
seed: 60419
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.279 seconds.
  Mean final reward: -1.2409
  Mean return: -7.1875
  Mean final entropy: 0.0126
  Max final entropy: 0.0633
  Pseudo loss: -1.50647
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.279 seconds.
  Mean final reward: -0.5621
  Mean return: -6.2737
  Mean final entropy: 0.0033
  Max final entropy: 0.0218
  Pseudo loss: -0.86920
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.278 seconds.
  Mean final reward: -1.1702
  Mean return: -5.7029
  Mean final entropy: 0.0110
  Max final entropy: 0.0660
  Pseudo loss: -6.35112
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (40/100) took 0.282 seconds.
  Mean final reward: -0.8944
  Mean return: -4.9543
  Mean final entropy: 0.0058
  Max final entropy: 0.0292
  Pseudo loss: -4.11234
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.2243
  Mean return: -3.2584
  Mean final entropy: 0.0008
  Max final entropy: 0.0039
  Pseudo loss: -1.24854
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.276 seconds.
  Mean final reward: -0.2649
  Mean return: -3.7145
  Mean final entropy: 0.0012
  Max final entropy: 0.0083
  Pseudo loss: -2.46388
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.278 seconds.
  Mean final reward: -0.2648
  Mean return: -3.1050
  Mean final entropy: 0.0010
  Max final entropy: 0.0060
  Pseudo loss: -0.62644
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (80/100) took 0.287 seconds.
  Mean final reward: -0.0411
  Mean return: -2.7229
  Mean final entropy: 0.0003
  Max final entropy: 0.0014
  Pseudo loss: 0.29864
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.280 seconds.
  Mean final reward: -0.1999
  Mean return: -2.8817
  Mean final entropy: 0.0007
  Max final entropy: 0.0036
  Pseudo loss: 0.05851
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (100/100) took 0.284 seconds.
  Mean final reward: -0.1999
  Mean return: -2.8817
  Mean final entropy: 0.0007
  Max final entropy: 0.0036
  Pseudo loss: 0.29379
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Testing with greedy policy:
  Start entropy: [0.30472976 0.16628695 0.23084065 0.35342282 0.32438883 0.0806787
 0.5575242  0.4069143 ]
  Mean final entropy: 0.000309
  Max final entropy: 0.001389
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [-2.9319674e-08  8.5430446e-07  5.9015593e-07  3.7845862e-07
  9.7220029e-05  8.4742322e-07  1.1443295e-08  5.6808540e-07]
  Max Min Entropy: 9.722002869239077e-05
  Best actions: [array([1, 8, 3, 5, 3, 0, 8, 8]), array([7, 5, 6, 7, 1, 6, 7, 7]), array([4, 1, 4, 1, 2, 5, 3, 1])]

####################################


step: 45
seed: 64617
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.286 seconds.
  Mean final reward: -2.0320
  Mean return: -9.4807
  Mean final entropy: 0.0390
  Max final entropy: 0.1829
  Pseudo loss: 0.34161
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.274 seconds.
  Mean final reward: -1.0547
  Mean return: -6.9179
  Mean final entropy: 0.0056
  Max final entropy: 0.0261
  Pseudo loss: -0.45052
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.276 seconds.
  Mean final reward: -0.0918
  Mean return: -2.9999
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: -1.03246
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.280 seconds.
  Mean final reward: -0.3506
  Mean return: -5.0924
  Mean final entropy: 0.0011
  Max final entropy: 0.0037
  Pseudo loss: -2.52742
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8256
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.53970
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.269 seconds.
  Mean final reward: -0.1806
  Mean return: -3.0839
  Mean final entropy: 0.0006
  Max final entropy: 0.0022
  Pseudo loss: -0.37264
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.270 seconds.
  Mean final reward: -0.1008
  Mean return: -2.9897
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.39428
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0162
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.38157
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8778
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.60679
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.271 seconds.
  Mean final reward: -0.1779
  Mean return: -2.9284
  Mean final entropy: 0.0006
  Max final entropy: 0.0022
  Pseudo loss: -0.02423
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Testing with greedy policy:
  Start entropy: [0.36143953 0.02615923 0.2605117  0.35227072 0.51599604 0.45560855
 0.46347094 0.5592286 ]
  Mean final entropy: 0.000405
  Max final entropy: 0.002240
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [1.0606757e-06 1.8716467e-07 7.7350592e-07 1.8773210e-07 3.3036719e-07
 7.3063342e-07 7.4085699e-07 1.7134101e-06]
  Max Min Entropy: 1.7134101426563575e-06
  Best actions: [array([6, 6, 6, 3, 3, 1, 4, 0]), array([0, 5, 1, 8, 5, 7, 1, 3]), array([5, 8, 5, 6, 3, 8, 0, 0])]

####################################


step: 46
seed: 69045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.273 seconds.
  Mean final reward: -2.0550
  Mean return: -9.6110
  Mean final entropy: 0.0284
  Max final entropy: 0.1097
  Pseudo loss: -0.91210
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.266 seconds.
  Mean final reward: -1.5908
  Mean return: -8.3972
  Mean final entropy: 0.0163
  Max final entropy: 0.0630
  Pseudo loss: 0.28910
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.275 seconds.
  Mean final reward: -0.5713
  Mean return: -4.6477
  Mean final entropy: 0.0064
  Max final entropy: 0.0469
  Pseudo loss: -0.36728
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.273 seconds.
  Mean final reward: -0.2887
  Mean return: -3.9095
  Mean final entropy: 0.0015
  Max final entropy: 0.0101
  Pseudo loss: -1.21150
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.273 seconds.
  Mean final reward: -0.4694
  Mean return: -4.7019
  Mean final entropy: 0.0055
  Max final entropy: 0.0427
  Pseudo loss: -5.09867
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.264 seconds.
  Mean final reward: -0.5053
  Mean return: -4.4702
  Mean final entropy: 0.0045
  Max final entropy: 0.0325
  Pseudo loss: -5.34695
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.270 seconds.
  Mean final reward: -0.8732
  Mean return: -5.3413
  Mean final entropy: 0.0775
  Max final entropy: 0.6170
  Pseudo loss: 0.10016
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2650
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.08991
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2650
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.33731
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.267 seconds.
  Mean final reward: -0.0701
  Mean return: -3.3351
  Mean final entropy: 0.0004
  Max final entropy: 0.0018
  Pseudo loss: 0.02536
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.64723355 0.20566665 0.05453049 0.4184865  0.49145734 0.39088815
 0.44441053 0.2514823 ]
  Mean final entropy: 0.000415
  Max final entropy: 0.001752
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 2.5578152e-07 -1.7841863e-07  1.8795784e-07  5.7628313e-07
  2.2856077e-06  2.2328833e-05  3.5409784e-07  9.4107349e-07]
  Max Min Entropy: 2.2328833438223228e-05
  Best actions: [array([0, 1, 6, 8, 7, 6, 1, 1]), array([1, 7, 3, 7, 4, 0, 2, 3]), array([4, 8, 6, 0, 7, 6, 6, 5])]

####################################


step: 47
seed: 73709
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.276 seconds.
  Mean final reward: -2.3887
  Mean return: -9.4831
  Mean final entropy: 0.0849
  Max final entropy: 0.3226
  Pseudo loss: -1.01884
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.293 seconds.
  Mean final reward: -1.2219
  Mean return: -7.7889
  Mean final entropy: 0.0153
  Max final entropy: 0.0865
  Pseudo loss: -1.67020
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.285 seconds.
  Mean final reward: -1.3309
  Mean return: -6.7187
  Mean final entropy: 0.0318
  Max final entropy: 0.2356
  Pseudo loss: -1.37604
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.277 seconds.
  Mean final reward: -0.8799
  Mean return: -5.1610
  Mean final entropy: 0.0044
  Max final entropy: 0.0171
  Pseudo loss: -1.30023
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9704
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.02710
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.287 seconds.
  Mean final reward: -0.1828
  Mean return: -4.1533
  Mean final entropy: 0.0006
  Max final entropy: 0.0043
  Pseudo loss: -0.18503
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.280 seconds.
  Mean final reward: -0.1215
  Mean return: -4.0818
  Mean final entropy: 0.0004
  Max final entropy: 0.0026
  Pseudo loss: -0.96908
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.279 seconds.
  Mean final reward: -0.1215
  Mean return: -4.0818
  Mean final entropy: 0.0004
  Max final entropy: 0.0026
  Pseudo loss: -1.55469
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8918
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.03347
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9704
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.04468
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.35574338 0.486212   0.08187588 0.36352363 0.50770974 0.5390388
 0.34997532 0.51032495]
  Mean final entropy: 0.000102
  Max final entropy: 0.000583
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.3810577e-06 7.3870024e-07 3.6778431e-07 2.7454521e-06 2.7977960e-04
 1.7010106e-07 4.8339103e-07 6.8697812e-09]
  Max Min Entropy: 0.00027977960417047143
  Best actions: [array([3, 0, 5, 2, 7, 1, 3, 6]), array([6, 1, 3, 8, 1, 2, 2, 3]), array([3, 7, 5, 4, 5, 6, 1, 7])]

####################################


step: 48
seed: 78615
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.274 seconds.
  Mean final reward: -1.9483
  Mean return: -9.7380
  Mean final entropy: 0.0315
  Max final entropy: 0.1189
  Pseudo loss: -0.19651
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.279 seconds.
  Mean final reward: -1.3170
  Mean return: -6.1501
  Mean final entropy: 0.0333
  Max final entropy: 0.2389
  Pseudo loss: -5.17121
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.285 seconds.
  Mean final reward: -0.3932
  Mean return: -3.8229
  Mean final entropy: 0.0016
  Max final entropy: 0.0098
  Pseudo loss: -2.40286
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.280 seconds.
  Mean final reward: -0.1013
  Mean return: -4.5873
  Mean final entropy: 0.0005
  Max final entropy: 0.0020
  Pseudo loss: -1.94932
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.289 seconds.
  Mean final reward: -0.2117
  Mean return: -3.9272
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -0.35253
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.3697
  Mean return: -4.0645
  Mean final entropy: 0.0025
  Max final entropy: 0.0193
  Pseudo loss: -2.37261
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.276 seconds.
  Mean final reward: -0.2530
  Mean return: -4.6236
  Mean final entropy: 0.0010
  Max final entropy: 0.0076
  Pseudo loss: -1.50110
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.282 seconds.
  Mean final reward: -0.3374
  Mean return: -4.3407
  Mean final entropy: 0.0012
  Max final entropy: 0.0076
  Pseudo loss: -3.02480
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.2530
  Mean return: -3.7866
  Mean final entropy: 0.0010
  Max final entropy: 0.0076
  Pseudo loss: -0.10549
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.279 seconds.
  Mean final reward: -0.2530
  Mean return: -3.7866
  Mean final entropy: 0.0011
  Max final entropy: 0.0076
  Pseudo loss: -0.28430
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.13512352 0.64223003 0.5606326  0.33789515 0.59614855 0.24673554
 0.40397006 0.38811517]
  Mean final entropy: 0.000364
  Max final entropy: 0.002296
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [5.7033765e-07 3.2246334e-07 1.5140133e-08 1.2452916e-03 3.8951319e-07
 2.1363819e-06 1.7095172e-05 1.9851343e-06]
  Max Min Entropy: 0.0012452915543690324
  Best actions: [array([1, 8, 7, 4, 4, 6, 0, 7]), array([6, 5, 4, 1, 5, 3, 6, 4]), array([8, 4, 2, 6, 0, 1, 0, 7])]

####################################


step: 49
seed: 83769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.293 seconds.
  Mean final reward: -1.3675
  Mean return: -9.0355
  Mean final entropy: 0.0156
  Max final entropy: 0.0607
  Pseudo loss: 0.11133
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.271 seconds.
  Mean final reward: -2.7264
  Mean return: -10.2982
  Mean final entropy: 0.0313
  Max final entropy: 0.0726
  Pseudo loss: -1.25299
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.272 seconds.
  Mean final reward: -0.7380
  Mean return: -6.6565
  Mean final entropy: 0.0142
  Max final entropy: 0.1095
  Pseudo loss: -3.85861
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8204
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00933
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4519
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.30641
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (60/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8204
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.19653
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.271 seconds.
  Mean final reward: -0.1031
  Mean return: -4.2772
  Mean final entropy: 0.0004
  Max final entropy: 0.0023
  Pseudo loss: -1.15205
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.272 seconds.
  Mean final reward: -0.1589
  Mean return: -3.7768
  Mean final entropy: 0.0005
  Max final entropy: 0.0036
  Pseudo loss: -1.09850
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4519
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.08123
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4519
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.11749
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.3674819  0.24078569 0.3570621  0.31006232 0.33933583 0.3316903
 0.6637559  0.19674686]
  Mean final entropy: 0.000100
  Max final entropy: 0.000621
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.3374451e-05 1.6602566e-07 5.0358921e-07 4.5590619e-07 9.9881363e-05
 5.9419165e-07 1.3874901e-06 8.2438692e-07]
  Max Min Entropy: 9.988136298488826e-05
  Best actions: [array([1, 7, 5, 0, 0, 2, 5, 4]), array([8, 1, 6, 6, 4, 1, 2, 7]), array([1, 3, 6, 0, 0, 2, 6, 2])]

####################################


step: 50
seed: 89177
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.281 seconds.
  Mean final reward: -1.5408
  Mean return: -6.9749
  Mean final entropy: 0.0224
  Max final entropy: 0.1255
  Pseudo loss: -0.40590
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.280 seconds.
  Mean final reward: -0.4348
  Mean return: -5.7811
  Mean final entropy: 0.0019
  Max final entropy: 0.0097
  Pseudo loss: 0.45150
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.271 seconds.
  Mean final reward: -1.3737
  Mean return: -5.8422
  Mean final entropy: 0.0219
  Max final entropy: 0.1099
  Pseudo loss: -3.06959
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.4275
  Mean return: -3.5217
  Mean final entropy: 0.0016
  Max final entropy: 0.0097
  Pseudo loss: -0.62177
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.267 seconds.
  Mean final reward: -0.0711
  Mean return: -2.7973
  Mean final entropy: 0.0002
  Max final entropy: 0.0018
  Pseudo loss: -0.68598
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7458
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -4.62223
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.272 seconds.
  Mean final reward: -0.0307
  Mean return: -2.4159
  Mean final entropy: 0.0002
  Max final entropy: 0.0013
  Pseudo loss: -0.61384
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2517
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.07261
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0378
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00700
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0378
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.01289
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5495424  0.40100554 0.20617768 0.5959757  0.24781501 0.47036967
 0.30474842 0.54668057]
  Mean final entropy: 0.000069
  Max final entropy: 0.000397
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 7.5663934e-08  2.9401477e-07  1.5708009e-07  2.3189998e-05
 -3.9796848e-08  4.5754865e-07  5.0565809e-08  1.5680527e-07]
  Max Min Entropy: 2.3189997591543943e-05
  Best actions: [array([4, 4, 1, 0, 8, 1, 2, 7]), array([5, 1, 7, 3, 5, 2, 1, 6]), array([7, 7, 5, 7, 0, 1, 7, 5])]

####################################


step: 51
seed: 94845
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.285 seconds.
  Mean final reward: -1.7414
  Mean return: -8.6815
  Mean final entropy: 0.0202
  Max final entropy: 0.0917
  Pseudo loss: 0.43862
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.272 seconds.
  Mean final reward: -1.4042
  Mean return: -8.3285
  Mean final entropy: 0.0074
  Max final entropy: 0.0180
  Pseudo loss: -0.60067
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.277 seconds.
  Mean final reward: -0.3393
  Mean return: -4.5012
  Mean final entropy: 0.0011
  Max final entropy: 0.0069
  Pseudo loss: -0.91834
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.279 seconds.
  Mean final reward: -0.3191
  Mean return: -5.1960
  Mean final entropy: 0.0009
  Max final entropy: 0.0042
  Pseudo loss: -0.99967
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.4411
  Mean return: -4.8430
  Mean final entropy: 0.0037
  Max final entropy: 0.0285
  Pseudo loss: -2.01876
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.4187
  Mean return: -4.0992
  Mean final entropy: 0.0036
  Max final entropy: 0.0285
  Pseudo loss: 0.17209
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.277 seconds.
  Mean final reward: -0.4187
  Mean return: -4.0992
  Mean final entropy: 0.0036
  Max final entropy: 0.0285
  Pseudo loss: 0.21947
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.276 seconds.
  Mean final reward: -0.4187
  Mean return: -4.0992
  Mean final entropy: 0.0036
  Max final entropy: 0.0285
  Pseudo loss: 0.19383
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.4594
  Mean return: -4.6689
  Mean final entropy: 0.0038
  Max final entropy: 0.0285
  Pseudo loss: -1.49375
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.275 seconds.
  Mean final reward: -0.5103
  Mean return: -4.1908
  Mean final entropy: 0.0074
  Max final entropy: 0.0593
  Pseudo loss: -1.25463
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.36892024 0.22476462 0.133328   0.44765645 0.3464113  0.36657995
 0.47632778 0.10538782]
  Mean final entropy: 0.003577
  Max final entropy: 0.028495
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [4.6316287e-05 5.4781765e-07 1.6895402e-07 1.0228845e-07 1.7964300e-06
 1.9964693e-06 5.2558090e-07 2.9013365e-05]
  Max Min Entropy: 4.6316286898218095e-05
  Best actions: [array([7, 8, 6, 8, 5, 1, 1, 7]), array([1, 2, 5, 2, 2, 0, 4, 6]), array([7, 0, 4, 4, 5, 1, 6, 4])]

####################################


step: 52
seed: 100779
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.280 seconds.
  Mean final reward: -2.0525
  Mean return: -9.5187
  Mean final entropy: 0.0235
  Max final entropy: 0.1164
  Pseudo loss: -3.02920
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.286 seconds.
  Mean final reward: -1.2349
  Mean return: -6.2735
  Mean final entropy: 0.0159
  Max final entropy: 0.0701
  Pseudo loss: -4.06028
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.266 seconds.
  Mean final reward: -0.1638
  Mean return: -4.3902
  Mean final entropy: 0.0005
  Max final entropy: 0.0037
  Pseudo loss: -1.34488
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (40/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4500
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.42176
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (50/100) took 0.260 seconds.
  Mean final reward: -0.2468
  Mean return: -5.4489
  Mean final entropy: 0.0010
  Max final entropy: 0.0072
  Pseudo loss: -4.70481
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1078
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.09302
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (70/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1078
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.02061
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1078
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.02218
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1078
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.05336
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1078
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.12335
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.5644661  0.20883134 0.5914304  0.11630271 0.63150346 0.28273976
 0.18151802 0.44703585]
  Mean final entropy: 0.000133
  Max final entropy: 0.000827
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.7879644e-06 1.4478854e-06 9.7539476e-07 1.5092188e-07 3.2965465e-07
 4.5996380e-06 7.9038966e-07 2.2841633e-09]
  Max Min Entropy: 4.599638032232178e-06
  Best actions: [array([3, 4, 0, 4, 7, 2, 1, 7]), array([5, 1, 1, 5, 8, 5, 3, 2]), array([3, 4, 7, 4, 7, 2, 6, 7])]

####################################


step: 53
seed: 106985
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.269 seconds.
  Mean final reward: -1.4233
  Mean return: -8.3553
  Mean final entropy: 0.0236
  Max final entropy: 0.1198
  Pseudo loss: -1.05206
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.265 seconds.
  Mean final reward: -0.2864
  Mean return: -4.7856
  Mean final entropy: 0.0014
  Max final entropy: 0.0099
  Pseudo loss: -3.37847
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.274 seconds.
  Mean final reward: -0.5995
  Mean return: -4.8204
  Mean final entropy: 0.0021
  Max final entropy: 0.0076
  Pseudo loss: -3.16732
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.267 seconds.
  Mean final reward: -0.1682
  Mean return: -2.9737
  Mean final entropy: 0.0007
  Max final entropy: 0.0028
  Pseudo loss: -1.45544
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.266 seconds.
  Mean final reward: -0.2382
  Mean return: -3.0686
  Mean final entropy: 0.0007
  Max final entropy: 0.0029
  Pseudo loss: -1.73291
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.0514
  Mean return: -2.8830
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: -0.67130
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9531
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.05618
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.270 seconds.
  Mean final reward: -0.0204
  Mean return: -2.1458
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -1.44757
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.270 seconds.
  Mean final reward: -0.0110
  Mean return: -2.0543
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.08592
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0143
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.06427
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.40478057 0.12044843 0.34159547 0.39339975 0.65226847 0.23045504
 0.09252841 0.3060223 ]
  Mean final entropy: 0.000376
  Max final entropy: 0.001401
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [2.1362363e-08 6.1636541e-07 7.3220242e-08 9.9950483e-08 2.1896797e-07
 1.8253846e-05 1.9933982e-07 6.4416817e-08]
  Max Min Entropy: 1.825384606490843e-05
  Best actions: [array([0, 5, 1, 0, 7, 4, 1, 3]), array([6, 0, 7, 6, 6, 7, 2, 5]), array([6, 1, 2, 7, 2, 4, 1, 4])]

####################################


step: 54
seed: 113469
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.281 seconds.
  Mean final reward: -2.7816
  Mean return: -10.9533
  Mean final entropy: 0.0665
  Max final entropy: 0.4124
  Pseudo loss: -2.43017
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.278 seconds.
  Mean final reward: -0.3021
  Mean return: -4.7137
  Mean final entropy: 0.0015
  Max final entropy: 0.0112
  Pseudo loss: -3.37543
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.280 seconds.
  Mean final reward: -0.1726
  Mean return: -3.6490
  Mean final entropy: 0.0007
  Max final entropy: 0.0040
  Pseudo loss: -1.30984
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.271 seconds.
  Mean final reward: -0.0273
  Mean return: -3.1580
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.03507
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1171
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.04361
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1171
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.02077
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1171
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.13857
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1171
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.20639
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.274 seconds.
  Mean final reward: -0.0273
  Mean return: -3.1580
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.06535
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.269 seconds.
  Mean final reward: -0.3294
  Mean return: -4.2736
  Mean final entropy: 0.0017
  Max final entropy: 0.0112
  Pseudo loss: -5.33216
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.5508987  0.43912405 0.66369057 0.257262   0.6092731  0.18352793
 0.07462485 0.04560135]
  Mean final entropy: 0.000261
  Max final entropy: 0.001244
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 7.8476523e-07  2.8090804e-07 -2.5484598e-08  3.2896409e-05
  3.8468374e-07  5.7556184e-07  5.7428400e-07  4.3417440e-08]
  Max Min Entropy: 3.2896408811211586e-05
  Best actions: [array([8, 4, 0, 6, 2, 7, 1, 1]), array([5, 5, 6, 7, 1, 1, 7, 0]), array([1, 0, 7, 2, 3, 7, 5, 4])]

####################################


step: 55
seed: 120237
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.262 seconds.
  Mean final reward: -2.3057
  Mean return: -10.7842
  Mean final entropy: 0.0338
  Max final entropy: 0.1284
  Pseudo loss: -0.05568
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.268 seconds.
  Mean final reward: -1.7289
  Mean return: -9.0316
  Mean final entropy: 0.0201
  Max final entropy: 0.0716
  Pseudo loss: -1.20484
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.275 seconds.
  Mean final reward: -1.8682
  Mean return: -8.4630
  Mean final entropy: 0.0232
  Max final entropy: 0.1167
  Pseudo loss: -0.80386
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.276 seconds.
  Mean final reward: -1.5693
  Mean return: -8.1549
  Mean final entropy: 0.0201
  Max final entropy: 0.0636
  Pseudo loss: -1.68946
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.5116
  Mean return: -6.2832
  Mean final entropy: 0.0016
  Max final entropy: 0.0038
  Pseudo loss: 0.06222
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.276 seconds.
  Mean final reward: -0.4513
  Mean return: -5.9351
  Mean final entropy: 0.0014
  Max final entropy: 0.0038
  Pseudo loss: -0.08626
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.282 seconds.
  Mean final reward: -0.5608
  Mean return: -6.2402
  Mean final entropy: 0.0025
  Max final entropy: 0.0127
  Pseudo loss: -2.33842
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.280 seconds.
  Mean final reward: -0.6439
  Mean return: -6.6369
  Mean final entropy: 0.0022
  Max final entropy: 0.0050
  Pseudo loss: -1.81787
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.3340
  Mean return: -5.8266
  Mean final entropy: 0.0012
  Max final entropy: 0.0038
  Pseudo loss: -0.03519
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.284 seconds.
  Mean final reward: -0.1677
  Mean return: -5.6470
  Mean final entropy: 0.0007
  Max final entropy: 0.0038
  Pseudo loss: 0.00965
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.41567397 0.4267506  0.55621886 0.37956372 0.08337178 0.682912
 0.134968   0.0965794 ]
  Mean final entropy: 0.000693
  Max final entropy: 0.003825
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [1.4060317e-03 1.7330132e-05 3.3044199e-07 3.5357764e-06 5.7417537e-07
 1.1255022e-06 1.3807204e-05 3.8237719e-08]
  Max Min Entropy: 0.0014060316607356071
  Best actions: [array([0, 5, 0, 1, 4, 5, 1, 1]), array([6, 4, 2, 2, 3, 8, 2, 2]), array([8, 5, 0, 1, 4, 1, 1, 5])]

####################################


step: 56
seed: 127295
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.278 seconds.
  Mean final reward: -0.6779
  Mean return: -5.4064
  Mean final entropy: 0.0088
  Max final entropy: 0.0655
  Pseudo loss: -1.02414
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.276 seconds.
  Mean final reward: -1.2833
  Mean return: -6.9335
  Mean final entropy: 0.0194
  Max final entropy: 0.0784
  Pseudo loss: -4.52257
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.275 seconds.
  Mean final reward: -0.3585
  Mean return: -3.7453
  Mean final entropy: 0.0020
  Max final entropy: 0.0139
  Pseudo loss: 0.22302
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.283 seconds.
  Mean final reward: -0.7195
  Mean return: -4.2162
  Mean final entropy: 0.0028
  Max final entropy: 0.0102
  Pseudo loss: -3.11056
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.125
Episode (50/100) took 0.277 seconds.
  Mean final reward: -0.0289
  Mean return: -3.1355
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.19695
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.0289
  Mean return: -3.2485
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -1.85506
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.280 seconds.
  Mean final reward: -0.0289
  Mean return: -2.8425
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.02912
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.281 seconds.
  Mean final reward: -0.0289
  Mean return: -2.8425
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.08584
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.0289
  Mean return: -3.1355
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.63047
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (100/100) took 0.286 seconds.
  Mean final reward: -0.0289
  Mean return: -2.8425
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.33928
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Testing with greedy policy:
  Start entropy: [0.6385242  0.31355232 0.6644405  0.5360925  0.2643959  0.2957096
 0.589803   0.09978722]
  Mean final entropy: 0.000114
  Max final entropy: 0.000701
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 5.6528660e-07  9.4260791e-08  2.2695904e-08  1.7342285e-07
 -5.4489181e-08  1.2597784e-03  1.6059282e-07  3.0374221e-07]
  Max Min Entropy: 0.001259778393432498
  Best actions: [array([3, 8, 1, 1, 4, 0, 2, 1]), array([6, 2, 2, 4, 7, 6, 5, 2]), array([7, 3, 5, 7, 5, 0, 1, 3])]

####################################


step: 57
seed: 134649
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.272 seconds.
  Mean final reward: -1.7574
  Mean return: -9.4758
  Mean final entropy: 0.0140
  Max final entropy: 0.0515
  Pseudo loss: -0.88672
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.277 seconds.
  Mean final reward: -1.5071
  Mean return: -7.6751
  Mean final entropy: 0.0312
  Max final entropy: 0.1284
  Pseudo loss: 2.06713
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.276 seconds.
  Mean final reward: -0.6818
  Mean return: -5.3526
  Mean final entropy: 0.0026
  Max final entropy: 0.0112
  Pseudo loss: -1.77185
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.279 seconds.
  Mean final reward: -0.5829
  Mean return: -4.8011
  Mean final entropy: 0.0027
  Max final entropy: 0.0157
  Pseudo loss: -1.62676
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.279 seconds.
  Mean final reward: -0.8647
  Mean return: -5.0334
  Mean final entropy: 0.0117
  Max final entropy: 0.0815
  Pseudo loss: -3.72400
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.274 seconds.
  Mean final reward: -0.1101
  Mean return: -3.5165
  Mean final entropy: 0.0003
  Max final entropy: 0.0024
  Pseudo loss: -0.69531
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: -0.8302
  Mean return: -4.9074
  Mean final entropy: 0.0073
  Max final entropy: 0.0401
  Pseudo loss: -3.54071
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6288
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.03288
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.282 seconds.
  Mean final reward: -0.4969
  Mean return: -4.1935
  Mean final entropy: 0.0026
  Max final entropy: 0.0172
  Pseudo loss: -0.34792
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.277 seconds.
  Mean final reward: -0.1701
  Mean return: -3.6339
  Mean final entropy: 0.0005
  Max final entropy: 0.0039
  Pseudo loss: -1.49223
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.12160556 0.469364   0.5110966  0.67066586 0.27059126 0.56426466
 0.0887193  0.5086615 ]
  Mean final entropy: 0.000155
  Max final entropy: 0.001110
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 1.8917513e-07  1.2348150e-04  3.7535436e-08  1.3733367e-07
  9.3511499e-06 -1.9003123e-08  1.8220215e-06  3.8182708e-07]
  Max Min Entropy: 0.0001234814990311861
  Best actions: [array([1, 6, 8, 3, 5, 3, 2, 3]), array([6, 7, 6, 6, 2, 6, 4, 5]), array([8, 6, 1, 0, 8, 8, 7, 4])]

####################################


step: 58
seed: 142305
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.280 seconds.
  Mean final reward: -1.9140
  Mean return: -8.5999
  Mean final entropy: 0.0179
  Max final entropy: 0.0578
  Pseudo loss: -2.85486
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.277 seconds.
  Mean final reward: -1.3603
  Mean return: -8.3195
  Mean final entropy: 0.0052
  Max final entropy: 0.0111
  Pseudo loss: -1.19469
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.277 seconds.
  Mean final reward: -1.1531
  Mean return: -6.3374
  Mean final entropy: 0.0060
  Max final entropy: 0.0268
  Pseudo loss: -1.88791
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.261 seconds.
  Mean final reward: -1.8520
  Mean return: -8.0385
  Mean final entropy: 0.0358
  Max final entropy: 0.1330
  Pseudo loss: -1.23876
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (50/100) took 0.268 seconds.
  Mean final reward: -0.9818
  Mean return: -6.4340
  Mean final entropy: 0.0062
  Max final entropy: 0.0358
  Pseudo loss: -2.13972
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.125
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.4943
  Mean return: -6.0805
  Mean final entropy: 0.0016
  Max final entropy: 0.0051
  Pseudo loss: -1.94279
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.275 seconds.
  Mean final reward: -0.1349
  Mean return: -5.2369
  Mean final entropy: 0.0007
  Max final entropy: 0.0019
  Pseudo loss: -0.36003
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.273 seconds.
  Mean final reward: -0.3809
  Mean return: -5.6764
  Mean final entropy: 0.0013
  Max final entropy: 0.0039
  Pseudo loss: -1.20499
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (90/100) took 0.268 seconds.
  Mean final reward: -0.4799
  Mean return: -5.4224
  Mean final entropy: 0.0020
  Max final entropy: 0.0120
  Pseudo loss: -2.56039
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.271 seconds.
  Mean final reward: -0.4211
  Mean return: -5.3020
  Mean final entropy: 0.0014
  Max final entropy: 0.0052
  Pseudo loss: -0.49334
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.40379813 0.17863613 0.14379889 0.3201338  0.43383288 0.50258815
 0.4181059  0.62866396]
  Mean final entropy: 0.001635
  Max final entropy: 0.005430
  Solved trajectories: 5 / 8
Exaushtive search:
  Minimum entropy: [ 8.3296662e-05 -1.0289003e-08  1.9345400e-05  6.0012895e-05
  5.5448116e-05  4.5509574e-07  1.8460457e-07  1.6610128e-04]
  Max Min Entropy: 0.000166101279319264
  Best actions: [array([3, 3, 3, 5, 3, 8, 7, 8]), array([5, 1, 2, 6, 5, 5, 5, 6]), array([3, 0, 3, 7, 7, 1, 2, 8])]

####################################


step: 59
seed: 150269
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.281 seconds.
  Mean final reward: -1.6085
  Mean return: -8.4329
  Mean final entropy: 0.0270
  Max final entropy: 0.1841
  Pseudo loss: -2.33977
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.271 seconds.
  Mean final reward: -1.2328
  Mean return: -6.3092
  Mean final entropy: 0.0160
  Max final entropy: 0.1079
  Pseudo loss: -1.45490
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.276 seconds.
  Mean final reward: -0.4673
  Mean return: -3.9708
  Mean final entropy: 0.0030
  Max final entropy: 0.0218
  Pseudo loss: 0.59986
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.282 seconds.
  Mean final reward: -0.1750
  Mean return: -4.5710
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -1.56841
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.4856
  Mean return: -4.2873
  Mean final entropy: 0.0032
  Max final entropy: 0.0218
  Pseudo loss: -0.43173
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.3851
  Mean return: -4.1632
  Mean final entropy: 0.0028
  Max final entropy: 0.0218
  Pseudo loss: -2.31698
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.277 seconds.
  Mean final reward: -0.4977
  Mean return: -4.5114
  Mean final entropy: 0.0052
  Max final entropy: 0.0402
  Pseudo loss: -1.51958
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.273 seconds.
  Mean final reward: -0.0592
  Mean return: -3.3629
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.03046
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.277 seconds.
  Mean final reward: -0.0406
  Mean return: -2.9495
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: 0.01735
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9089
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: 0.02289
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.03877244 0.27512234 0.64306223 0.6026173  0.5182191  0.5029878
 0.44521338 0.2346566 ]
  Mean final entropy: 0.000179
  Max final entropy: 0.001203
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 1.7221312e-07  8.1081133e-08 -2.3035749e-08  3.8508847e-07
  6.9940990e-07  3.8709001e-07  3.5663490e-07  6.2800214e-06]
  Max Min Entropy: 6.280021352722542e-06
  Best actions: [array([8, 2, 0, 1, 4, 8, 4, 6]), array([1, 1, 2, 4, 7, 6, 1, 3]), array([8, 4, 0, 6, 4, 4, 3, 8])]

####################################


step: 60
seed: 158547
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.280 seconds.
  Mean final reward: -1.4729
  Mean return: -8.0958
  Mean final entropy: 0.0111
  Max final entropy: 0.0341
  Pseudo loss: -0.87125
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.276 seconds.
  Mean final reward: -2.4080
  Mean return: -9.7415
  Mean final entropy: 0.0525
  Max final entropy: 0.2043
  Pseudo loss: 0.44126
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.283 seconds.
  Mean final reward: -1.3548
  Mean return: -7.4723
  Mean final entropy: 0.0159
  Max final entropy: 0.0815
  Pseudo loss: -6.14764
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.273 seconds.
  Mean final reward: -0.6549
  Mean return: -6.1238
  Mean final entropy: 0.0050
  Max final entropy: 0.0347
  Pseudo loss: -0.85355
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.274 seconds.
  Mean final reward: -0.3456
  Mean return: -5.3719
  Mean final entropy: 0.0021
  Max final entropy: 0.0159
  Pseudo loss: -0.53723
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (60/100) took 0.280 seconds.
  Mean final reward: -0.4338
  Mean return: -5.0670
  Mean final entropy: 0.0041
  Max final entropy: 0.0322
  Pseudo loss: -1.35706
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.310 seconds.
  Mean final reward: -0.5763
  Mean return: -5.9401
  Mean final entropy: 0.0106
  Max final entropy: 0.0831
  Pseudo loss: -1.27999
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.279 seconds.
  Mean final reward: -0.2793
  Mean return: -4.8033
  Mean final entropy: 0.0011
  Max final entropy: 0.0047
  Pseudo loss: -0.54229
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.281 seconds.
  Mean final reward: -0.0237
  Mean return: -4.5439
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -1.04154
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.295 seconds.
  Mean final reward: -0.4020
  Mean return: -5.1403
  Mean final entropy: 0.0017
  Max final entropy: 0.0109
  Pseudo loss: -2.35978
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.39174208 0.35342216 0.3432171  0.34266266 0.3829327  0.4423964
 0.5367217  0.23516755]
  Mean final entropy: 0.000486
  Max final entropy: 0.002390
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [ 3.1276168e-06  1.4313304e-06  1.9013965e-05  9.5365493e-07
 -4.5353264e-08  2.7759002e-06  2.3771381e-05  1.0230052e-05]
  Max Min Entropy: 2.3771381165715866e-05
  Best actions: [array([4, 1, 2, 2, 5, 7, 6, 2]), array([7, 0, 8, 0, 4, 4, 8, 8]), array([2, 8, 2, 2, 3, 2, 1, 2])]

####################################


step: 61
seed: 167145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.330 seconds.
  Mean final reward: -0.8288
  Mean return: -5.6678
  Mean final entropy: 0.0070
  Max final entropy: 0.0468
  Pseudo loss: -1.75434
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.293 seconds.
  Mean final reward: -1.2263
  Mean return: -5.6472
  Mean final entropy: 0.0220
  Max final entropy: 0.1603
  Pseudo loss: -3.56268
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.299 seconds.
  Mean final reward: -1.4367
  Mean return: -5.2728
  Mean final entropy: 0.0115
  Max final entropy: 0.0546
  Pseudo loss: 2.78398
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.282 seconds.
  Mean final reward: -0.6383
  Mean return: -4.0758
  Mean final entropy: 0.0026
  Max final entropy: 0.0110
  Pseudo loss: -2.84648
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.283 seconds.
  Mean final reward: -0.6936
  Mean return: -3.8374
  Mean final entropy: 0.0043
  Max final entropy: 0.0217
  Pseudo loss: -0.34596
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.283 seconds.
  Mean final reward: -0.1000
  Mean return: -2.9151
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.21838
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.276 seconds.
  Mean final reward: -0.3214
  Mean return: -3.9623
  Mean final entropy: 0.0017
  Max final entropy: 0.0104
  Pseudo loss: -5.28152
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.280 seconds.
  Mean final reward: -0.3624
  Mean return: -3.1122
  Mean final entropy: 0.0021
  Max final entropy: 0.0144
  Pseudo loss: -2.34229
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.275 seconds.
  Mean final reward: -0.1373
  Mean return: -2.8838
  Mean final entropy: 0.0005
  Max final entropy: 0.0030
  Pseudo loss: -0.08923
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.279 seconds.
  Mean final reward: -0.1005
  Mean return: -2.4933
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.09540
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.38318682 0.11870982 0.47235548 0.02628596 0.6355748  0.5824427
 0.26149517 0.50044394]
  Mean final entropy: 0.000289
  Max final entropy: 0.001262
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [2.2134628e-07 1.9575076e-07 2.7356526e-07 3.7502721e-07 3.7805128e-06
 7.6192657e-07 8.4335425e-06 2.1844494e-07]
  Max Min Entropy: 8.43354246171657e-06
  Best actions: [array([4, 3, 0, 1, 8, 8, 8, 8]), array([5, 1, 1, 2, 5, 2, 6, 2]), array([4, 5, 3, 1, 4, 1, 8, 6])]

####################################


step: 62
seed: 176069
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.286 seconds.
  Mean final reward: -0.8229
  Mean return: -5.9241
  Mean final entropy: 0.0068
  Max final entropy: 0.0280
  Pseudo loss: -2.49745
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.294 seconds.
  Mean final reward: -1.0850
  Mean return: -5.7857
  Mean final entropy: 0.0267
  Max final entropy: 0.1970
  Pseudo loss: -3.32714
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.294 seconds.
  Mean final reward: -1.0137
  Mean return: -5.7017
  Mean final entropy: 0.0340
  Max final entropy: 0.2580
  Pseudo loss: -7.65437
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5325
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.83517
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9147
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: -2.80433
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2863
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: -0.36686
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.281 seconds.
  Mean final reward: -0.1654
  Mean return: -2.9812
  Mean final entropy: 0.0006
  Max final entropy: 0.0038
  Pseudo loss: -1.30204
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1705
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: -0.22216
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6030
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -1.31608
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2863
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.37891
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.542202   0.14457339 0.53074586 0.06992701 0.3747933  0.5886277
 0.1277879  0.48854673]
  Mean final entropy: 0.000162
  Max final entropy: 0.000640
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 2.7718770e-07  3.0534193e-07  3.3128112e-05  1.8165629e-07
  6.5818575e-08  6.7840057e-07  4.2001977e-07 -8.5413676e-09]
  Max Min Entropy: 3.312811168143526e-05
  Best actions: [array([6, 2, 1, 0, 5, 8, 2, 7]), array([3, 1, 7, 6, 4, 2, 3, 0]), array([6, 4, 1, 3, 6, 3, 4, 2])]

####################################


step: 63
seed: 185325
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.8185
  Mean return: -8.6369
  Mean final entropy: 0.0197
  Max final entropy: 0.1174
  Pseudo loss: 0.37917
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.396 seconds.
  Mean final reward: -1.6311
  Mean return: -6.4330
  Mean final entropy: 0.0317
  Max final entropy: 0.1682
  Pseudo loss: -1.93409
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.299 seconds.
  Mean final reward: -0.2476
  Mean return: -3.8097
  Mean final entropy: 0.0008
  Max final entropy: 0.0051
  Pseudo loss: -0.58440
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.300 seconds.
  Mean final reward: -0.2038
  Mean return: -3.5244
  Mean final entropy: 0.0006
  Max final entropy: 0.0051
  Pseudo loss: -0.00899
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.305 seconds.
  Mean final reward: -0.2038
  Mean return: -3.5244
  Mean final entropy: 0.0006
  Max final entropy: 0.0051
  Pseudo loss: -0.03409
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.324 seconds.
  Mean final reward: -0.2038
  Mean return: -3.5244
  Mean final entropy: 0.0006
  Max final entropy: 0.0051
  Pseudo loss: -0.12783
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.01917
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0135
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: 0.01763
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00346
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00533
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.42488873 0.45219964 0.6578483  0.3041306  0.59030366 0.1639812
 0.6063987  0.35400683]
  Mean final entropy: 0.000006
  Max final entropy: 0.000024
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 2.8103813e-07 -3.3458700e-09  1.0854219e-06  5.5262274e-07
  7.8304701e-08  8.9619817e-08  3.1498783e-07  2.4503840e-06]
  Max Min Entropy: 2.450384045005194e-06
  Best actions: [array([6, 6, 3, 5, 3, 5, 6, 7]), array([7, 0, 5, 7, 6, 3, 3, 4]), array([3, 5, 1, 5, 8, 7, 5, 7])]

####################################


step: 64
seed: 194919
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.291 seconds.
  Mean final reward: -1.0853
  Mean return: -6.0559
  Mean final entropy: 0.0101
  Max final entropy: 0.0631
  Pseudo loss: -0.13615
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.282 seconds.
  Mean final reward: -1.1637
  Mean return: -6.0398
  Mean final entropy: 0.0071
  Max final entropy: 0.0375
  Pseudo loss: -1.94998
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.280 seconds.
  Mean final reward: -0.1518
  Mean return: -2.6067
  Mean final entropy: 0.0006
  Max final entropy: 0.0022
  Pseudo loss: -0.19846
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.281 seconds.
  Mean final reward: -1.1040
  Mean return: -4.9351
  Mean final entropy: 0.0073
  Max final entropy: 0.0364
  Pseudo loss: -1.73414
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (50/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8009
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.01394
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1607
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: -0.18954
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.282 seconds.
  Mean final reward: -0.4533
  Mean return: -3.1837
  Mean final entropy: 0.0042
  Max final entropy: 0.0310
  Pseudo loss: -3.19121
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8281
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.07471
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.321 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8095
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.08694
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8009
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.04948
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.51383054 0.15526433 0.3761812  0.05274632 0.55977255 0.11078039
 0.47156423 0.26894298]
  Mean final entropy: 0.000123
  Max final entropy: 0.000689
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.9738886e-07  1.1557490e-06  3.3521175e-07  1.5183265e-07
  3.0405630e-07 -1.2497290e-07  1.7063300e-08  2.1464114e-06]
  Max Min Entropy: 2.146411361536593e-06
  Best actions: [array([8, 6, 1, 8, 5, 8, 5, 4]), array([7, 0, 7, 5, 4, 2, 4, 1]), array([0, 3, 0, 8, 5, 5, 3, 8])]

####################################


step: 65
seed: 204857
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.301 seconds.
  Mean final reward: -2.2812
  Mean return: -9.6262
  Mean final entropy: 0.0262
  Max final entropy: 0.0752
  Pseudo loss: -2.26171
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.284 seconds.
  Mean final reward: -0.5145
  Mean return: -4.6224
  Mean final entropy: 0.0044
  Max final entropy: 0.0313
  Pseudo loss: -1.75332
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.276 seconds.
  Mean final reward: -0.9784
  Mean return: -4.4309
  Mean final entropy: 0.0072
  Max final entropy: 0.0354
  Pseudo loss: -0.74965
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (40/100) took 0.301 seconds.
  Mean final reward: -0.9074
  Mean return: -5.2261
  Mean final entropy: 0.0047
  Max final entropy: 0.0190
  Pseudo loss: -0.24298
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (50/100) took 0.287 seconds.
  Mean final reward: -0.1826
  Mean return: -4.1353
  Mean final entropy: 0.0010
  Max final entropy: 0.0015
  Pseudo loss: 0.11460
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.301 seconds.
  Mean final reward: -0.4382
  Mean return: -4.5738
  Mean final entropy: 0.0045
  Max final entropy: 0.0333
  Pseudo loss: 0.39342
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.286 seconds.
  Mean final reward: -0.6071
  Mean return: -4.4577
  Mean final entropy: 0.0084
  Max final entropy: 0.0630
  Pseudo loss: -0.48252
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.287 seconds.
  Mean final reward: -0.5326
  Mean return: -4.0429
  Mean final entropy: 0.0039
  Max final entropy: 0.0258
  Pseudo loss: 0.19652
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (90/100) took 0.282 seconds.
  Mean final reward: -0.6351
  Mean return: -4.4983
  Mean final entropy: 0.0041
  Max final entropy: 0.0258
  Pseudo loss: -1.30263
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.308 seconds.
  Mean final reward: -0.4903
  Mean return: -3.9517
  Mean final entropy: 0.0037
  Max final entropy: 0.0258
  Pseudo loss: -0.18439
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.39253986 0.35554433 0.07465758 0.5328082  0.42303765 0.5801872
 0.38458392 0.04952932]
  Mean final entropy: 0.010947
  Max final entropy: 0.083102
  Solved trajectories: 5 / 8
Exaushtive search:
  Minimum entropy: [ 2.5908887e-05  1.1226232e-05  1.0376074e-06  4.1192896e-05
  1.8523787e-07 -2.9953370e-08  6.0691509e-06  4.7351833e-08]
  Max Min Entropy: 4.1192895878339186e-05
  Best actions: [array([0, 8, 6, 7, 5, 0, 0, 6]), array([2, 2, 3, 1, 3, 6, 2, 4]), array([0, 3, 4, 7, 7, 5, 0, 4])]

####################################


step: 66
seed: 215145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.387 seconds.
  Mean final reward: -1.4192
  Mean return: -7.1880
  Mean final entropy: 0.0209
  Max final entropy: 0.0855
  Pseudo loss: -0.34704
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.276 seconds.
  Mean final reward: -1.3144
  Mean return: -6.6329
  Mean final entropy: 0.0333
  Max final entropy: 0.2408
  Pseudo loss: -1.64444
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.302 seconds.
  Mean final reward: -0.9906
  Mean return: -5.8988
  Mean final entropy: 0.0155
  Max final entropy: 0.0940
  Pseudo loss: -1.43161
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.283 seconds.
  Mean final reward: -0.2242
  Mean return: -5.0076
  Mean final entropy: 0.0007
  Max final entropy: 0.0031
  Pseudo loss: 0.18877
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.313 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9531
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.13286
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.5999
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.24409
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7605
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.25743
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.346 seconds.
  Mean final reward: -0.0990
  Mean return: -3.6990
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.01726
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.293 seconds.
  Mean final reward: -0.0990
  Mean return: -4.0848
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.32742
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.274 seconds.
  Mean final reward: -0.1400
  Mean return: -4.1046
  Mean final entropy: 0.0005
  Max final entropy: 0.0031
  Pseudo loss: -1.38670
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.13083623 0.24996728 0.00610776 0.33464733 0.5593765  0.50233364
 0.60366476 0.23124078]
  Mean final entropy: 0.000381
  Max final entropy: 0.002208
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 4.22792073e-06  1.52195554e-07  1.40093150e-07  1.11140944e-04
  9.45546105e-07  1.31633243e-07  7.24499068e-06 -4.41632260e-08]
  Max Min Entropy: 0.00011114094377262518
  Best actions: [array([6, 3, 2, 6, 7, 2, 8, 2]), array([0, 5, 8, 7, 4, 1, 2, 0]), array([6, 2, 5, 6, 7, 2, 8, 5])]

####################################


step: 67
seed: 225789
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.284 seconds.
  Mean final reward: -0.8849
  Mean return: -6.3567
  Mean final entropy: 0.0356
  Max final entropy: 0.2789
  Pseudo loss: -3.22367
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.308 seconds.
  Mean final reward: -1.3331
  Mean return: -7.0478
  Mean final entropy: 0.0133
  Max final entropy: 0.0707
  Pseudo loss: -3.00984
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.302 seconds.
  Mean final reward: -0.5769
  Mean return: -4.5409
  Mean final entropy: 0.0023
  Max final entropy: 0.0095
  Pseudo loss: -2.56849
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.278 seconds.
  Mean final reward: -1.1030
  Mean return: -4.5481
  Mean final entropy: 0.0073
  Max final entropy: 0.0256
  Pseudo loss: -0.73228
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (50/100) took 0.297 seconds.
  Mean final reward: -1.0495
  Mean return: -5.3137
  Mean final entropy: 0.0134
  Max final entropy: 0.0799
  Pseudo loss: -7.29556
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.296 seconds.
  Mean final reward: -0.1118
  Mean return: -1.9139
  Mean final entropy: 0.0005
  Max final entropy: 0.0021
  Pseudo loss: 0.08966
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.297 seconds.
  Mean final reward: -0.0206
  Mean return: -1.8585
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.39633
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.294 seconds.
  Mean final reward: -0.0407
  Mean return: -2.4185
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -3.38002
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.347 seconds.
  Mean final reward: -0.0206
  Mean return: -1.7190
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: 0.08507
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.306 seconds.
  Mean final reward: -0.0206
  Mean return: -1.7190
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: 0.16482
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.2539503  0.07293599 0.2641676  0.45214188 0.04959299 0.49184626
 0.47229555 0.48148572]
  Mean final entropy: 0.000235
  Max final entropy: 0.001179
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [5.4613696e-07 1.5325326e-06 3.0406349e-07 1.7546819e-06 1.4770295e-07
 2.7184017e-06 9.0259888e-08 5.1255371e-05]
  Max Min Entropy: 5.1255370635772124e-05
  Best actions: [array([6, 1, 4, 6, 4, 3, 7, 3]), array([7, 3, 2, 0, 7, 5, 0, 0]), array([2, 5, 4, 6, 0, 3, 7, 7])]

####################################


step: 68
seed: 236795
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.307 seconds.
  Mean final reward: -2.5222
  Mean return: -10.1805
  Mean final entropy: 0.0716
  Max final entropy: 0.3159
  Pseudo loss: 0.79408
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.280 seconds.
  Mean final reward: -0.5061
  Mean return: -5.5790
  Mean final entropy: 0.0033
  Max final entropy: 0.0234
  Pseudo loss: -0.79824
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.6055
  Mean return: -5.6385
  Mean final entropy: 0.0023
  Max final entropy: 0.0091
  Pseudo loss: -1.20030
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.300 seconds.
  Mean final reward: -1.4924
  Mean return: -7.8522
  Mean final entropy: 0.0180
  Max final entropy: 0.1105
  Pseudo loss: -3.18268
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.328 seconds.
  Mean final reward: -0.5903
  Mean return: -5.5845
  Mean final entropy: 0.0050
  Max final entropy: 0.0359
  Pseudo loss: -0.97281
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.329 seconds.
  Mean final reward: -0.2866
  Mean return: -5.0608
  Mean final entropy: 0.0011
  Max final entropy: 0.0050
  Pseudo loss: -0.39101
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3368
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.04228
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.323 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3899
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.02165
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (90/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3368
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.01491
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3779
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.34388
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Testing with greedy policy:
  Start entropy: [0.57363725 0.10856146 0.23332173 0.10651627 0.37469426 0.4330464
 0.57719934 0.64027905]
  Mean final entropy: 0.000371
  Max final entropy: 0.001985
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [3.3107774e-06 4.4845203e-07 3.5219850e-07 1.4585704e-07 4.5185598e-06
 2.4673827e-07 3.2218375e-07 4.6636831e-07]
  Max Min Entropy: 4.518559762800578e-06
  Best actions: [array([8, 3, 8, 3, 8, 0, 8, 8]), array([2, 6, 5, 6, 5, 1, 6, 7]), array([4, 0, 8, 0, 0, 8, 0, 1])]

####################################


step: 69
seed: 248169
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.326 seconds.
  Mean final reward: -1.5738
  Mean return: -9.1667
  Mean final entropy: 0.0163
  Max final entropy: 0.0736
  Pseudo loss: -1.17480
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.272 seconds.
  Mean final reward: -0.5319
  Mean return: -5.8103
  Mean final entropy: 0.0020
  Max final entropy: 0.0094
  Pseudo loss: -1.71855
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.273 seconds.
  Mean final reward: -0.0072
  Mean return: -4.1823
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -1.54648
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.271 seconds.
  Mean final reward: -0.5643
  Mean return: -4.4188
  Mean final entropy: 0.0033
  Max final entropy: 0.0221
  Pseudo loss: -3.11885
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7631
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -4.56837
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2342
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.41353
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0573
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00362
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0573
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.05015
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0573
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.02646
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0573
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.01977
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.4202901  0.13223663 0.37371445 0.21299186 0.16771947 0.67562556
 0.18059018 0.53628623]
  Mean final entropy: 0.000137
  Max final entropy: 0.000562
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 6.7104241e-07  5.8567895e-07  1.8288638e-07  1.7040784e-07
  1.2131356e-07  1.3921540e-07  1.2670165e-06 -1.4319363e-07]
  Max Min Entropy: 1.2670165006056777e-06
  Best actions: [array([0, 7, 5, 4, 4, 8, 6, 4]), array([2, 5, 4, 5, 7, 7, 2, 1]), array([6, 2, 6, 0, 0, 6, 5, 6])]

####################################


step: 70
seed: 259917
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.289 seconds.
  Mean final reward: -1.6358
  Mean return: -8.0934
  Mean final entropy: 0.0115
  Max final entropy: 0.0351
  Pseudo loss: -2.20622
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.325 seconds.
  Mean final reward: -1.3124
  Mean return: -6.6020
  Mean final entropy: 0.0108
  Max final entropy: 0.0424
  Pseudo loss: -1.88653
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.287 seconds.
  Mean final reward: -0.4782
  Mean return: -5.1499
  Mean final entropy: 0.0022
  Max final entropy: 0.0132
  Pseudo loss: -5.22500
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.321 seconds.
  Mean final reward: -0.3091
  Mean return: -3.9648
  Mean final entropy: 0.0016
  Max final entropy: 0.0119
  Pseudo loss: -1.17777
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.350 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4594
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.41496
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.387 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1194
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.17469
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.344 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4594
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -1.85984
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1194
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.09456
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1194
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.00325
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1194
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.02558
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.68224645 0.29441154 0.587644   0.55491143 0.20060188 0.24444598
 0.2403858  0.6335839 ]
  Mean final entropy: 0.000111
  Max final entropy: 0.000817
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.2846492e-07 2.5766933e-05 8.4178032e-07 5.7286388e-07 4.1783202e-05
 1.1517089e-05 1.8093680e-05 6.5725649e-06]
  Max Min Entropy: 4.17832015955355e-05
  Best actions: None

####################################


step: 71
seed: 272045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.328 seconds.
  Mean final reward: -1.3871
  Mean return: -7.1713
  Mean final entropy: 0.0081
  Max final entropy: 0.0205
  Pseudo loss: -1.12494
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.308 seconds.
  Mean final reward: -0.4745
  Mean return: -4.7861
  Mean final entropy: 0.0016
  Max final entropy: 0.0056
  Pseudo loss: -1.51427
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.1667
  Mean return: -3.0639
  Mean final entropy: 0.0006
  Max final entropy: 0.0038
  Pseudo loss: -2.99063
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3263
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.36893
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.323 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2946
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.10530
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.345 seconds.
  Mean final reward: -0.0266
  Mean return: -2.3212
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -0.08188
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2946
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.02550
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.311 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2946
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.02313
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.318 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2946
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.04421
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2946
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.05537
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.09293094 0.02121276 0.16223212 0.4198799  0.24604572 0.4066625
 0.21459228 0.2006088 ]
  Mean final entropy: 0.000039
  Max final entropy: 0.000166
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.9132096e-07 2.0360286e-08 3.6480961e-07 1.1827470e-07 4.5916991e-07
 1.5383819e-07 3.0069707e-06 3.1953437e-07]
  Max Min Entropy: 3.006970700880629e-06
  Best actions: [array([6, 5, 1, 1, 6, 1, 5, 0]), array([5, 4, 3, 4, 0, 7, 0, 3]), array([4, 6, 2, 5, 4, 0, 5, 0])]

####################################


step: 72
seed: 284559
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.299 seconds.
  Mean final reward: -1.5852
  Mean return: -8.3817
  Mean final entropy: 0.0154
  Max final entropy: 0.0708
  Pseudo loss: -3.33444
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.279 seconds.
  Mean final reward: -0.8793
  Mean return: -7.2206
  Mean final entropy: 0.0032
  Max final entropy: 0.0088
  Pseudo loss: -2.68496
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.306 seconds.
  Mean final reward: -0.2397
  Mean return: -6.1565
  Mean final entropy: 0.0008
  Max final entropy: 0.0036
  Pseudo loss: -0.46528
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.266 seconds.
  Mean final reward: -0.4810
  Mean return: -6.4588
  Mean final entropy: 0.0055
  Max final entropy: 0.0419
  Pseudo loss: -1.39575
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.282 seconds.
  Mean final reward: -0.1894
  Mean return: -5.3493
  Mean final entropy: 0.0007
  Max final entropy: 0.0046
  Pseudo loss: -0.13018
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.4668
  Mean return: -5.7481
  Mean final entropy: 0.0054
  Max final entropy: 0.0419
  Pseudo loss: -2.48589
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.278 seconds.
  Mean final reward: -0.0782
  Mean return: -5.1726
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: 0.30555
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.277 seconds.
  Mean final reward: -0.0782
  Mean return: -5.1726
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: 0.20684
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.0782
  Mean return: -5.1471
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: 0.11999
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.273 seconds.
  Mean final reward: -0.0782
  Mean return: -5.0284
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: 0.45935
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.5763465  0.47631612 0.440843   0.49230596 0.2882772  0.6798022
 0.01715299 0.5640964 ]
  Mean final entropy: 0.000312
  Max final entropy: 0.001869
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 1.17036514e-07  1.49509981e-06  7.85692770e-04  2.15539853e-06
  5.47491865e-08  1.58563773e-10  4.92387585e-07 -3.61268917e-08]
  Max Min Entropy: 0.0007856927695684135
  Best actions: [array([3, 7, 7, 3, 8, 5, 3, 8]), array([0, 4, 4, 5, 2, 4, 5, 7]), array([1, 7, 5, 3, 4, 5, 1, 1])]

####################################


step: 73
seed: 297465
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.289 seconds.
  Mean final reward: -0.9299
  Mean return: -7.2795
  Mean final entropy: 0.0051
  Max final entropy: 0.0243
  Pseudo loss: 0.75580
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.284 seconds.
  Mean final reward: -1.3785
  Mean return: -7.6875
  Mean final entropy: 0.0365
  Max final entropy: 0.2698
  Pseudo loss: 0.37100
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.281 seconds.
  Mean final reward: -1.0962
  Mean return: -6.1951
  Mean final entropy: 0.0083
  Max final entropy: 0.0392
  Pseudo loss: -2.17536
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.279 seconds.
  Mean final reward: -0.9113
  Mean return: -6.9699
  Mean final entropy: 0.0077
  Max final entropy: 0.0506
  Pseudo loss: -2.31593
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.289 seconds.
  Mean final reward: -0.3845
  Mean return: -5.6044
  Mean final entropy: 0.0016
  Max final entropy: 0.0103
  Pseudo loss: -0.60698
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.293 seconds.
  Mean final reward: -0.0575
  Mean return: -4.3534
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: 0.36760
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.293 seconds.
  Mean final reward: -0.6761
  Mean return: -5.7771
  Mean final entropy: 0.0065
  Max final entropy: 0.0467
  Pseudo loss: -5.29541
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1016
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.08668
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1016
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.04577
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.297 seconds.
  Mean final reward: -0.2887
  Mean return: -4.3903
  Mean final entropy: 0.0014
  Max final entropy: 0.0101
  Pseudo loss: -1.84969
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.0972608  0.5407725  0.0590746  0.68319905 0.20507258 0.35148826
 0.58814776 0.64514256]
  Mean final entropy: 0.000182
  Max final entropy: 0.000792
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.16492842e-07 -4.21250697e-08 -2.82987589e-08  1.43683678e-07
  2.26135148e-06  1.15642346e-04  2.11767419e-05  1.26753037e-06]
  Max Min Entropy: 0.00011564234591787681
  Best actions: [array([6, 4, 1, 3, 4, 4, 3, 2]), array([0, 5, 2, 6, 1, 7, 5, 0]), array([7, 2, 1, 1, 4, 4, 2, 7])]

####################################


step: 74
seed: 310769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.281 seconds.
  Mean final reward: -1.8567
  Mean return: -7.8302
  Mean final entropy: 0.0293
  Max final entropy: 0.1815
  Pseudo loss: -3.05015
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.281 seconds.
  Mean final reward: -0.7391
  Mean return: -4.3429
  Mean final entropy: 0.0025
  Max final entropy: 0.0070
  Pseudo loss: -0.48492
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.284 seconds.
  Mean final reward: -0.2426
  Mean return: -3.6353
  Mean final entropy: 0.0010
  Max final entropy: 0.0070
  Pseudo loss: -0.02394
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0071
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.51928
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9382
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.27848
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6958
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.51131
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.288 seconds.
  Mean final reward: -0.1580
  Mean return: -3.2975
  Mean final entropy: 0.0006
  Max final entropy: 0.0035
  Pseudo loss: -0.93279
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.284 seconds.
  Mean final reward: -0.7773
  Mean return: -4.2209
  Mean final entropy: 0.0112
  Max final entropy: 0.0822
  Pseudo loss: -4.26181
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0071
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: -0.39426
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.312 seconds.
  Mean final reward: -0.1580
  Mean return: -3.4396
  Mean final entropy: 0.0006
  Max final entropy: 0.0035
  Pseudo loss: -2.05138
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.63479555 0.5263207  0.38356325 0.5604657  0.4621529  0.6137159
 0.29966003 0.36649895]
  Mean final entropy: 0.000154
  Max final entropy: 0.000477
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 6.6849069e-08  3.3059323e-06 -1.7158090e-08  4.5681844e-08
  1.0315188e-07  8.3834550e-07  1.1983707e-06  2.0301513e-06]
  Max Min Entropy: 3.3059322959161364e-06
  Best actions: [array([4, 5, 8, 5, 2, 5, 5, 0]), array([5, 3, 7, 7, 8, 4, 3, 2]), array([2, 5, 4, 8, 8, 0, 5, 7])]

####################################


step: 75
seed: 324477
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.275 seconds.
  Mean final reward: -1.5269
  Mean return: -6.6854
  Mean final entropy: 0.0183
  Max final entropy: 0.0845
  Pseudo loss: -1.63913
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.275 seconds.
  Mean final reward: -0.4472
  Mean return: -4.3444
  Mean final entropy: 0.0014
  Max final entropy: 0.0047
  Pseudo loss: -0.98043
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.3282
  Mean return: -4.1995
  Mean final entropy: 0.0010
  Max final entropy: 0.0040
  Pseudo loss: -2.97727
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9911
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00048
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.278 seconds.
  Mean final reward: -0.4698
  Mean return: -4.0707
  Mean final entropy: 0.0056
  Max final entropy: 0.0429
  Pseudo loss: -3.46505
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0846
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.03867
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.277 seconds.
  Mean final reward: -0.2416
  Mean return: -3.4906
  Mean final entropy: 0.0009
  Max final entropy: 0.0069
  Pseudo loss: -0.88369
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9911
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.01766
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9911
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00230
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9911
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.00513
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.17378122 0.06389036 0.37400237 0.46680805 0.58319056 0.5807849
 0.20544818 0.20018694]
  Mean final entropy: 0.000082
  Max final entropy: 0.000615
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 9.0791978e-08  9.1617856e-08  7.0251900e-08  1.8960368e-06
  1.4817728e-06 -6.5343734e-08  3.2666744e-06  2.6263733e-06]
  Max Min Entropy: 3.266674411861459e-06
  Best actions: [array([4, 4, 4, 0, 5, 8, 1, 4]), array([1, 7, 5, 1, 2, 2, 6, 5]), array([2, 0, 6, 5, 0, 0, 1, 0])]

####################################


step: 76
seed: 338595
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.7280
  Mean return: -8.3084
  Mean final entropy: 0.0302
  Max final entropy: 0.1557
  Pseudo loss: -5.64544
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.278 seconds.
  Mean final reward: -0.3103
  Mean return: -6.1216
  Mean final entropy: 0.0017
  Max final entropy: 0.0114
  Pseudo loss: -3.48474
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.322 seconds.
  Mean final reward: -0.6787
  Mean return: -6.1176
  Mean final entropy: 0.0096
  Max final entropy: 0.0716
  Pseudo loss: -4.09033
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.293 seconds.
  Mean final reward: -0.4539
  Mean return: -3.7587
  Mean final entropy: 0.0049
  Max final entropy: 0.0378
  Pseudo loss: -2.42212
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4653
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.20348
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4675
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.07770
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3447
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00029
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4306
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: -0.27795
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3447
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.01428
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3447
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00679
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.05739372 0.08525655 0.2959296  0.11688353 0.34887192 0.6730438
 0.39889878 0.3642606 ]
  Mean final entropy: 0.000191
  Max final entropy: 0.000603
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.1608756e-05 7.2584444e-06 6.5665432e-08 6.5274816e-07 4.2901681e-07
 3.3831832e-08 8.6039890e-08 2.9190198e-07]
  Max Min Entropy: 6.160875636851415e-05
  Best actions: [array([1, 5, 6, 1, 0, 3, 1, 8]), array([4, 1, 3, 6, 6, 4, 7, 5]), array([6, 5, 0, 8, 3, 2, 2, 2])]

####################################


step: 77
seed: 353129
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.263 seconds.
  Mean final reward: -1.8138
  Mean return: -8.7687
  Mean final entropy: 0.0134
  Max final entropy: 0.0375
  Pseudo loss: 0.40444
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.262 seconds.
  Mean final reward: -1.1563
  Mean return: -6.2674
  Mean final entropy: 0.0171
  Max final entropy: 0.0950
  Pseudo loss: -1.24927
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.258 seconds.
  Mean final reward: -1.2083
  Mean return: -5.8389
  Mean final entropy: 0.0206
  Max final entropy: 0.1220
  Pseudo loss: -0.63423
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.263 seconds.
  Mean final reward: -1.0460
  Mean return: -5.7476
  Mean final entropy: 0.0082
  Max final entropy: 0.0380
  Pseudo loss: -1.59659
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.267 seconds.
  Mean final reward: -0.7722
  Mean return: -4.4946
  Mean final entropy: 0.0056
  Max final entropy: 0.0225
  Pseudo loss: -0.66822
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.266 seconds.
  Mean final reward: -0.9286
  Mean return: -4.5790
  Mean final entropy: 0.0189
  Max final entropy: 0.1394
  Pseudo loss: -5.53719
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Episode (70/100) took 0.260 seconds.
  Mean final reward: -0.6093
  Mean return: -4.2715
  Mean final entropy: 0.0034
  Max final entropy: 0.0214
  Pseudo loss: 0.03032
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.250
Episode (80/100) took 0.260 seconds.
  Mean final reward: -0.2265
  Mean return: -3.3650
  Mean final entropy: 0.0007
  Max final entropy: 0.0028
  Pseudo loss: 0.70499
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Episode (90/100) took 0.264 seconds.
  Mean final reward: -0.1283
  Mean return: -3.1106
  Mean final entropy: 0.0004
  Max final entropy: 0.0028
  Pseudo loss: 0.97234
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.500
Episode (100/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6537
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.32678
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.2663336  0.00359581 0.6740854  0.6035404  0.08840924 0.47132948
 0.57207227 0.05821943]
  Mean final entropy: 0.000409
  Max final entropy: 0.002792
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [1.0316974e-06 2.4789190e-06 7.8344146e-07 4.4083270e-05 3.6190445e-07
 1.4438148e-06 3.5625486e-05 3.3486311e-05]
  Max Min Entropy: 4.408327004057355e-05
  Best actions: None

####################################


step: 78
seed: 368085
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.271 seconds.
  Mean final reward: -1.5513
  Mean return: -9.3131
  Mean final entropy: 0.0124
  Max final entropy: 0.0676
  Pseudo loss: 0.05766
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.268 seconds.
  Mean final reward: -0.6567
  Mean return: -6.5665
  Mean final entropy: 0.0024
  Max final entropy: 0.0073
  Pseudo loss: -1.11206
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.268 seconds.
  Mean final reward: -1.6642
  Mean return: -8.0264
  Mean final entropy: 0.0163
  Max final entropy: 0.0680
  Pseudo loss: -1.19245
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.260 seconds.
  Mean final reward: -0.0756
  Mean return: -4.1537
  Mean final entropy: 0.0002
  Max final entropy: 0.0018
  Pseudo loss: -0.83063
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8069
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.03993
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.274 seconds.
  Mean final reward: -0.1997
  Mean return: -4.7654
  Mean final entropy: 0.0008
  Max final entropy: 0.0049
  Pseudo loss: -1.62304
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8069
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.08868
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.260 seconds.
  Mean final reward: -0.0133
  Mean return: -3.2855
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: 0.03811
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6711
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.77713
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.267 seconds.
  Mean final reward: -0.0133
  Mean return: -3.2855
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: 0.06428
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.52688634 0.4188549  0.14689878 0.52651536 0.2667952  0.09487944
 0.34116033 0.6900029 ]
  Mean final entropy: 0.000217
  Max final entropy: 0.001112
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [ 7.6037359e-05  6.1168844e-06  1.0227357e-05  1.0686582e-06
  2.2320743e-03  4.6338943e-07 -4.6216728e-09  1.6651168e-07]
  Max Min Entropy: 0.002232074271887541
  Best actions: [array([0, 7, 7, 8, 1, 0, 7, 1]), array([1, 6, 1, 6, 5, 8, 4, 0]), array([0, 7, 7, 4, 2, 7, 0, 5])]

####################################


step: 79
seed: 383469
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.262 seconds.
  Mean final reward: -2.1302
  Mean return: -9.5784
  Mean final entropy: 0.0286
  Max final entropy: 0.1125
  Pseudo loss: -0.05943
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.299 seconds.
  Mean final reward: -1.1084
  Mean return: -6.7391
  Mean final entropy: 0.0053
  Max final entropy: 0.0185
  Pseudo loss: -2.70446
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.283 seconds.
  Mean final reward: -0.5916
  Mean return: -5.4233
  Mean final entropy: 0.0049
  Max final entropy: 0.0339
  Pseudo loss: -2.84285
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.312 seconds.
  Mean final reward: -0.8493
  Mean return: -5.5290
  Mean final entropy: 0.0074
  Max final entropy: 0.0498
  Pseudo loss: -0.67730
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (50/100) took 0.290 seconds.
  Mean final reward: -0.2948
  Mean return: -4.3901
  Mean final entropy: 0.0010
  Max final entropy: 0.0035
  Pseudo loss: -0.24960
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.279 seconds.
  Mean final reward: -0.2908
  Mean return: -4.4215
  Mean final entropy: 0.0010
  Max final entropy: 0.0034
  Pseudo loss: -0.87513
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.293 seconds.
  Mean final reward: -0.9289
  Mean return: -5.3908
  Mean final entropy: 0.0142
  Max final entropy: 0.0938
  Pseudo loss: -5.85495
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9446
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -1.05581
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.3439
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.01187
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -3.3439
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.03874
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.16137962 0.05186894 0.6143316  0.54890704 0.5915563  0.4954499
 0.25281426 0.5638536 ]
  Mean final entropy: 0.000344
  Max final entropy: 0.000881
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 1.8614467e-06  4.3225037e-07 -6.7063553e-08  3.7662372e-07
  1.5583852e-05  1.1154240e-06  5.1824522e-05  2.6655460e-05]
  Max Min Entropy: 5.18245215062052e-05
  Best actions: [array([1, 3, 3, 1, 5, 1, 6, 8]), array([2, 4, 5, 4, 2, 4, 1, 6]), array([1, 3, 0, 8, 5, 1, 5, 8])]

####################################


step: 80
seed: 399287
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.270 seconds.
  Mean final reward: -0.2504
  Mean return: -7.2949
  Mean final entropy: 0.0009
  Max final entropy: 0.0042
  Pseudo loss: 0.47017
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.279 seconds.
  Mean final reward: -0.1762
  Mean return: -5.1567
  Mean final entropy: 0.0006
  Max final entropy: 0.0025
  Pseudo loss: -0.62591
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.286 seconds.
  Mean final reward: -1.5128
  Mean return: -7.5899
  Mean final entropy: 0.0219
  Max final entropy: 0.1287
  Pseudo loss: -2.81185
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.276 seconds.
  Mean final reward: -0.7962
  Mean return: -6.1047
  Mean final entropy: 0.0047
  Max final entropy: 0.0206
  Pseudo loss: -2.47368
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.262 seconds.
  Mean final reward: -1.0999
  Mean return: -5.7413
  Mean final entropy: 0.0191
  Max final entropy: 0.1386
  Pseudo loss: -3.42802
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.258 seconds.
  Mean final reward: -0.1966
  Mean return: -4.7528
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -0.51784
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.268 seconds.
  Mean final reward: -0.2780
  Mean return: -4.8417
  Mean final entropy: 0.0009
  Max final entropy: 0.0038
  Pseudo loss: 0.05516
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.273 seconds.
  Mean final reward: -0.2959
  Mean return: -5.0266
  Mean final entropy: 0.0012
  Max final entropy: 0.0070
  Pseudo loss: -1.40842
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.260 seconds.
  Mean final reward: -0.5413
  Mean return: -4.9759
  Mean final entropy: 0.0024
  Max final entropy: 0.0109
  Pseudo loss: -1.11895
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.269 seconds.
  Mean final reward: -0.1637
  Mean return: -4.4662
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -0.91103
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.3527142  0.20207079 0.47877824 0.56603456 0.41254252 0.54129463
 0.67584324 0.35414478]
  Mean final entropy: 0.000532
  Max final entropy: 0.002419
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [1.1985604e-07 8.1995013e-06 9.9429715e-05 2.1444272e-07 8.0094702e-04
 2.8151426e-06 1.8517636e-07 3.6074587e-07]
  Max Min Entropy: 0.0008009470184333622
  Best actions: [array([0, 2, 4, 6, 0, 3, 8, 8]), array([8, 7, 3, 7, 6, 5, 5, 6]), array([5, 6, 2, 1, 0, 2, 6, 7])]

####################################


step: 81
seed: 415545
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.274 seconds.
  Mean final reward: -1.5672
  Mean return: -7.4651
  Mean final entropy: 0.0128
  Max final entropy: 0.0408
  Pseudo loss: 3.22985
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.308 seconds.
  Mean final reward: -1.0794
  Mean return: -5.8954
  Mean final entropy: 0.0056
  Max final entropy: 0.0241
  Pseudo loss: -1.75676
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.276 seconds.
  Mean final reward: -1.5024
  Mean return: -6.4046
  Mean final entropy: 0.0346
  Max final entropy: 0.1912
  Pseudo loss: -1.34741
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.296 seconds.
  Mean final reward: -0.3369
  Mean return: -3.1689
  Mean final entropy: 0.0018
  Max final entropy: 0.0113
  Pseudo loss: 0.02976
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.317 seconds.
  Mean final reward: -0.8431
  Mean return: -5.3467
  Mean final entropy: 0.0045
  Max final entropy: 0.0183
  Pseudo loss: -2.95262
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.359 seconds.
  Mean final reward: -0.1885
  Mean return: -2.8785
  Mean final entropy: 0.0007
  Max final entropy: 0.0039
  Pseudo loss: 0.16379
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.362 seconds.
  Mean final reward: -0.3597
  Mean return: -3.5365
  Mean final entropy: 0.0013
  Max final entropy: 0.0041
  Pseudo loss: -4.49037
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.275 seconds.
  Mean final reward: -0.2529
  Mean return: -3.2144
  Mean final entropy: 0.0010
  Max final entropy: 0.0049
  Pseudo loss: -2.57887
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.1916
  Mean return: -2.9372
  Mean final entropy: 0.0008
  Max final entropy: 0.0038
  Pseudo loss: -0.28542
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.277 seconds.
  Mean final reward: -0.2350
  Mean return: -3.2591
  Mean final entropy: 0.0009
  Max final entropy: 0.0039
  Pseudo loss: 0.78898
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Testing with greedy policy:
  Start entropy: [0.05611594 0.6580095  0.02554175 0.11150831 0.46063006 0.214923
 0.39143962 0.13443391]
  Mean final entropy: 0.000817
  Max final entropy: 0.003900
  Solved trajectories: 6 / 8
Exaushtive search:
  Minimum entropy: [3.5452530e-07 1.5273399e-06 8.4591996e-08 2.1719460e-07 5.3532731e-06
 4.3126679e-06 1.0355409e-07 5.3715698e-06]
  Max Min Entropy: 5.371569841372548e-06
  Best actions: [array([6, 6, 4, 7, 0, 8, 8, 3]), array([3, 8, 1, 5, 6, 5, 6, 6]), array([5, 5, 5, 3, 0, 8, 1, 3])]

####################################


step: 82
seed: 432249
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.271 seconds.
  Mean final reward: -1.0820
  Mean return: -6.3182
  Mean final entropy: 0.0177
  Max final entropy: 0.1195
  Pseudo loss: -0.72763
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.327 seconds.
  Mean final reward: -1.3551
  Mean return: -7.6379
  Mean final entropy: 0.0141
  Max final entropy: 0.0858
  Pseudo loss: -2.34787
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.325 seconds.
  Mean final reward: -1.8659
  Mean return: -8.5497
  Mean final entropy: 0.0155
  Max final entropy: 0.0589
  Pseudo loss: -2.72574
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.277 seconds.
  Mean final reward: -1.4366
  Mean return: -7.0326
  Mean final entropy: 0.0205
  Max final entropy: 0.1159
  Pseudo loss: 0.06819
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.280 seconds.
  Mean final reward: -0.8609
  Mean return: -6.1594
  Mean final entropy: 0.0057
  Max final entropy: 0.0267
  Pseudo loss: -2.49189
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.267 seconds.
  Mean final reward: -0.0047
  Mean return: -3.7997
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.56321
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.291 seconds.
  Mean final reward: -0.1720
  Mean return: -3.0910
  Mean final entropy: 0.0005
  Max final entropy: 0.0040
  Pseudo loss: 0.13897
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.265 seconds.
  Mean final reward: -0.5768
  Mean return: -4.6791
  Mean final entropy: 0.0018
  Max final entropy: 0.0053
  Pseudo loss: -3.59400
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.394 seconds.
  Mean final reward: -0.1720
  Mean return: -3.1384
  Mean final entropy: 0.0005
  Max final entropy: 0.0040
  Pseudo loss: 0.13777
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.324 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9237
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.04705
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.0833179  0.14587955 0.20851095 0.04923202 0.535949   0.5433078
 0.3529889  0.30799407]
  Mean final entropy: 0.000011
  Max final entropy: 0.000024
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [ 2.4513329e-07  4.6178269e-07  1.3445933e-04  5.5051310e-06
  1.0777107e-05 -2.6131080e-08  1.1045362e-06  2.1506328e-06]
  Max Min Entropy: 0.000134459332912229
  Best actions: [array([7, 6, 4, 3, 4, 3, 7, 1]), array([8, 7, 1, 5, 5, 0, 0, 7]), array([3, 2, 6, 7, 4, 1, 1, 3])]

####################################


step: 83
seed: 449405
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.284 seconds.
  Mean final reward: -0.8601
  Mean return: -9.1116
  Mean final entropy: 0.0048
  Max final entropy: 0.0236
  Pseudo loss: -0.60730
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.294 seconds.
  Mean final reward: -0.7150
  Mean return: -6.1416
  Mean final entropy: 0.0029
  Max final entropy: 0.0100
  Pseudo loss: 0.99984
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.314 seconds.
  Mean final reward: -0.6326
  Mean return: -5.4493
  Mean final entropy: 0.0019
  Max final entropy: 0.0046
  Pseudo loss: -1.08046
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.314 seconds.
  Mean final reward: -0.6653
  Mean return: -5.0428
  Mean final entropy: 0.0115
  Max final entropy: 0.0878
  Pseudo loss: -1.24083
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4451
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.15877
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (60/100) took 0.278 seconds.
  Mean final reward: -0.3151
  Mean return: -3.9621
  Mean final entropy: 0.0014
  Max final entropy: 0.0082
  Pseudo loss: -2.48778
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.284 seconds.
  Mean final reward: -0.8175
  Mean return: -4.6518
  Mean final entropy: 0.0375
  Max final entropy: 0.2966
  Pseudo loss: -3.65988
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.407 seconds.
  Mean final reward: -0.4464
  Mean return: -3.7488
  Mean final entropy: 0.0017
  Max final entropy: 0.0071
  Pseudo loss: -2.43679
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.290 seconds.
  Mean final reward: -0.0204
  Mean return: -2.4413
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -0.39919
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.280 seconds.
  Mean final reward: -0.2030
  Mean return: -4.1275
  Mean final entropy: 0.0007
  Max final entropy: 0.0051
  Pseudo loss: -3.57355
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.12111246 0.37748897 0.3460057  0.5006462  0.41208065 0.5258914
 0.4774613  0.64824504]
  Mean final entropy: 0.000187
  Max final entropy: 0.000548
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.1356350e-06 2.8536479e-07 9.5363396e-07 9.9666892e-08 1.8249635e-06
 3.0731840e-06 2.2011909e-06 5.6852497e-07]
  Max Min Entropy: 3.073183961532777e-06
  Best actions: [array([6, 5, 6, 7, 3, 8, 3, 3]), array([3, 7, 0, 6, 5, 6, 4, 0]), array([6, 1, 4, 2, 4, 1, 7, 3])]

####################################


step: 84
seed: 467019
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.270 seconds.
  Mean final reward: -1.3564
  Mean return: -7.1591
  Mean final entropy: 0.0175
  Max final entropy: 0.0947
  Pseudo loss: 1.02782
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.270 seconds.
  Mean final reward: -1.0019
  Mean return: -6.4738
  Mean final entropy: 0.0073
  Max final entropy: 0.0401
  Pseudo loss: -2.46232
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.282 seconds.
  Mean final reward: -0.6178
  Mean return: -6.2222
  Mean final entropy: 0.0176
  Max final entropy: 0.1401
  Pseudo loss: -1.35321
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.269 seconds.
  Mean final reward: -0.8784
  Mean return: -5.1650
  Mean final entropy: 0.0046
  Max final entropy: 0.0174
  Pseudo loss: -4.37805
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (50/100) took 0.265 seconds.
  Mean final reward: -1.3495
  Mean return: -5.8654
  Mean final entropy: 0.0251
  Max final entropy: 0.1346
  Pseudo loss: -6.21993
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (60/100) took 0.270 seconds.
  Mean final reward: -0.7866
  Mean return: -4.7827
  Mean final entropy: 0.0072
  Max final entropy: 0.0460
  Pseudo loss: -1.80126
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (70/100) took 0.272 seconds.
  Mean final reward: -0.1868
  Mean return: -3.7466
  Mean final entropy: 0.0008
  Max final entropy: 0.0037
  Pseudo loss: -1.28832
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.273 seconds.
  Mean final reward: -0.2259
  Mean return: -3.6618
  Mean final entropy: 0.0009
  Max final entropy: 0.0061
  Pseudo loss: -0.92109
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.273 seconds.
  Mean final reward: -0.2793
  Mean return: -3.6927
  Mean final entropy: 0.0010
  Max final entropy: 0.0061
  Pseudo loss: -2.55118
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (100/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1983
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.12884
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.6392812  0.03214277 0.27779868 0.58073294 0.52065504 0.5811077
 0.59583604 0.19544138]
  Mean final entropy: 0.000044
  Max final entropy: 0.000191
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.39538789e-08 1.51716108e-07 3.06411010e-07 1.32787633e-07
 3.15473449e-06 3.61675148e-07 1.01182295e-07 4.16568673e-06]
  Max Min Entropy: 4.165686732449103e-06
  Best actions: [array([5, 4, 1, 0, 6, 8, 0, 3]), array([4, 6, 4, 6, 8, 5, 6, 2]), array([1, 4, 1, 0, 6, 8, 4, 3])]

####################################


step: 85
seed: 485097
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.342 seconds.
  Mean final reward: -1.0844
  Mean return: -7.6587
  Mean final entropy: 0.0083
  Max final entropy: 0.0353
  Pseudo loss: 0.84294
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.295 seconds.
  Mean final reward: -0.3713
  Mean return: -4.9122
  Mean final entropy: 0.0012
  Max final entropy: 0.0038
  Pseudo loss: 0.48442
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.296 seconds.
  Mean final reward: -0.9276
  Mean return: -5.1819
  Mean final entropy: 0.0041
  Max final entropy: 0.0132
  Pseudo loss: -0.23471
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (40/100) took 0.268 seconds.
  Mean final reward: -0.7728
  Mean return: -4.8880
  Mean final entropy: 0.0037
  Max final entropy: 0.0133
  Pseudo loss: 1.04266
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.5882
  Mean return: -5.0421
  Mean final entropy: 0.0025
  Max final entropy: 0.0108
  Pseudo loss: -1.41165
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.310 seconds.
  Mean final reward: -0.3851
  Mean return: -4.4626
  Mean final entropy: 0.0020
  Max final entropy: 0.0133
  Pseudo loss: 0.31322
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.297 seconds.
  Mean final reward: -0.3854
  Mean return: -4.3431
  Mean final entropy: 0.0029
  Max final entropy: 0.0218
  Pseudo loss: -0.12275
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.272 seconds.
  Mean final reward: -0.3234
  Mean return: -4.0910
  Mean final entropy: 0.0018
  Max final entropy: 0.0133
  Pseudo loss: -0.39070
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.279 seconds.
  Mean final reward: -0.4973
  Mean return: -4.6408
  Mean final entropy: 0.0016
  Max final entropy: 0.0062
  Pseudo loss: -3.18023
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (100/100) took 0.269 seconds.
  Mean final reward: -0.0816
  Mean return: -3.4465
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: -0.08129
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.5545269  0.12149173 0.35171732 0.43962896 0.08152608 0.6515223
 0.11479246 0.11997287]
  Mean final entropy: 0.000374
  Max final entropy: 0.001922
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [-2.1321435e-07  5.8000467e-07  1.6367221e-05  2.5368021e-05
  2.7362741e-08 -1.7105481e-08  1.4288223e-06  5.2180205e-08]
  Max Min Entropy: 2.5368020942551084e-05
  Best actions: [array([6, 7, 2, 6, 4, 0, 0, 4]), array([3, 8, 7, 8, 8, 1, 7, 5]), array([7, 5, 6, 1, 5, 8, 0, 3])]

####################################


step: 86
seed: 503645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.270 seconds.
  Mean final reward: -0.8722
  Mean return: -5.7893
  Mean final entropy: 0.0058
  Max final entropy: 0.0371
  Pseudo loss: -0.24385
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.267 seconds.
  Mean final reward: -0.5093
  Mean return: -5.1088
  Mean final entropy: 0.0019
  Max final entropy: 0.0086
  Pseudo loss: -1.03456
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.264 seconds.
  Mean final reward: -1.2082
  Mean return: -6.0426
  Mean final entropy: 0.0185
  Max final entropy: 0.1300
  Pseudo loss: -0.48068
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.268 seconds.
  Mean final reward: -0.4142
  Mean return: -4.4014
  Mean final entropy: 0.0017
  Max final entropy: 0.0101
  Pseudo loss: -0.51977
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.270 seconds.
  Mean final reward: -0.8153
  Mean return: -4.5867
  Mean final entropy: 0.0031
  Max final entropy: 0.0101
  Pseudo loss: -0.25696
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (60/100) took 0.266 seconds.
  Mean final reward: -0.2896
  Mean return: -4.1292
  Mean final entropy: 0.0013
  Max final entropy: 0.0101
  Pseudo loss: -0.25322
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.266 seconds.
  Mean final reward: -0.3276
  Mean return: -3.9740
  Mean final entropy: 0.0012
  Max final entropy: 0.0071
  Pseudo loss: -1.42448
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.264 seconds.
  Mean final reward: -0.2088
  Mean return: -3.5886
  Mean final entropy: 0.0008
  Max final entropy: 0.0033
  Pseudo loss: -0.58847
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.311 seconds.
  Mean final reward: -0.5753
  Mean return: -3.7297
  Mean final entropy: 0.0026
  Max final entropy: 0.0101
  Pseudo loss: -0.57510
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (100/100) took 0.288 seconds.
  Mean final reward: -0.1475
  Mean return: -3.2341
  Mean final entropy: 0.0006
  Max final entropy: 0.0033
  Pseudo loss: -0.12344
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.52061045 0.13312326 0.4359299  0.53665197 0.3956443  0.42632842
 0.41933042 0.18427081]
  Mean final entropy: 0.000552
  Max final entropy: 0.003255
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [-6.7297805e-08 -6.1936305e-08  1.7124896e-07  1.5931736e-06
 -6.9351891e-08  1.1320899e-05 -9.5030572e-10  3.3131414e-07]
  Max Min Entropy: 1.1320898920530453e-05
  Best actions: [array([1, 0, 7, 2, 0, 4, 4, 1]), array([4, 1, 0, 0, 6, 5, 1, 3]), array([8, 0, 3, 4, 4, 4, 8, 4])]

####################################


step: 87
seed: 522669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.280 seconds.
  Mean final reward: -1.6511
  Mean return: -7.6989
  Mean final entropy: 0.0244
  Max final entropy: 0.1469
  Pseudo loss: 0.95892
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.272 seconds.
  Mean final reward: -1.8815
  Mean return: -9.7427
  Mean final entropy: 0.0287
  Max final entropy: 0.1557
  Pseudo loss: 2.03070
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.277 seconds.
  Mean final reward: -1.0260
  Mean return: -6.8642
  Mean final entropy: 0.0090
  Max final entropy: 0.0466
  Pseudo loss: 0.41681
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.6318
  Mean return: -6.7590
  Mean final entropy: 0.0026
  Max final entropy: 0.0102
  Pseudo loss: -4.03170
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.291 seconds.
  Mean final reward: -0.6400
  Mean return: -5.4331
  Mean final entropy: 0.0037
  Max final entropy: 0.0238
  Pseudo loss: -1.89043
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9501
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.66505
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.292 seconds.
  Mean final reward: -0.0877
  Mean return: -3.5307
  Mean final entropy: 0.0004
  Max final entropy: 0.0020
  Pseudo loss: -0.04373
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4430
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.08561
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.307 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1831
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.12887
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.268 seconds.
  Mean final reward: -0.0877
  Mean return: -3.2623
  Mean final entropy: 0.0004
  Max final entropy: 0.0020
  Pseudo loss: -0.15253
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.50396264 0.03881117 0.5607377  0.22259292 0.6115476  0.5094331
 0.6367056  0.35039055]
  Mean final entropy: 0.000390
  Max final entropy: 0.002017
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [5.0637721e-07 1.5053905e-06 1.8695370e-06 1.9450876e-07 4.4698135e-07
 1.4097427e-05 1.9299542e-07 8.5882004e-07]
  Max Min Entropy: 1.4097427083470393e-05
  Best actions: [array([8, 6, 7, 5, 8, 1, 3, 6]), array([2, 5, 8, 0, 5, 0, 5, 7]), array([0, 4, 2, 2, 8, 1, 8, 4])]

####################################


step: 88
seed: 542175
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.272 seconds.
  Mean final reward: -0.6347
  Mean return: -5.8918
  Mean final entropy: 0.0026
  Max final entropy: 0.0105
  Pseudo loss: -1.46965
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.284 seconds.
  Mean final reward: -1.9143
  Mean return: -7.3301
  Mean final entropy: 0.0254
  Max final entropy: 0.0651
  Pseudo loss: 0.23694
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.269 seconds.
  Mean final reward: -1.3485
  Mean return: -6.4026
  Mean final entropy: 0.0232
  Max final entropy: 0.1330
  Pseudo loss: -8.09851
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.262 seconds.
  Mean final reward: -1.7316
  Mean return: -6.6256
  Mean final entropy: 0.0260
  Max final entropy: 0.1369
  Pseudo loss: -0.56925
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (50/100) took 0.257 seconds.
  Mean final reward: -0.6128
  Mean return: -4.9003
  Mean final entropy: 0.0022
  Max final entropy: 0.0078
  Pseudo loss: -1.74981
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.287 seconds.
  Mean final reward: -0.0550
  Mean return: -3.7250
  Mean final entropy: 0.0002
  Max final entropy: 0.0016
  Pseudo loss: 0.08113
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.281 seconds.
  Mean final reward: -0.0956
  Mean return: -3.7655
  Mean final entropy: 0.0004
  Max final entropy: 0.0021
  Pseudo loss: 0.41443
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.287 seconds.
  Mean final reward: -0.2906
  Mean return: -4.4216
  Mean final entropy: 0.0009
  Max final entropy: 0.0046
  Pseudo loss: -0.39890
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.272 seconds.
  Mean final reward: -0.4774
  Mean return: -4.3774
  Mean final entropy: 0.0032
  Max final entropy: 0.0229
  Pseudo loss: -1.43278
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.266 seconds.
  Mean final reward: -0.1160
  Mean return: -3.4714
  Mean final entropy: 0.0005
  Max final entropy: 0.0017
  Pseudo loss: -0.86460
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.20913216 0.04668805 0.2979728  0.50144416 0.20614201 0.18271011
 0.5337926  0.11658069]
  Mean final entropy: 0.000068
  Max final entropy: 0.000451
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.4583893e-06 6.9582882e-07 9.6005024e-08 4.0784937e-07 1.0792524e-05
 4.7312966e-07 1.3455063e-04 1.8161994e-06]
  Max Min Entropy: 0.00013455063162837178
  Best actions: [array([8, 3, 7, 1, 8, 2, 6, 5]), array([2, 5, 4, 2, 6, 1, 7, 8]), array([8, 4, 0, 0, 5, 2, 6, 7])]

####################################


step: 89
seed: 562169
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.279 seconds.
  Mean final reward: -2.4932
  Mean return: -8.9880
  Mean final entropy: 0.0697
  Max final entropy: 0.2371
  Pseudo loss: -1.92006
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.266 seconds.
  Mean final reward: -1.3500
  Mean return: -5.7872
  Mean final entropy: 0.0225
  Max final entropy: 0.1504
  Pseudo loss: -2.08322
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.2251
  Mean return: -3.8100
  Mean final entropy: 0.0007
  Max final entropy: 0.0031
  Pseudo loss: -0.51478
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.272 seconds.
  Mean final reward: -0.1431
  Mean return: -3.1548
  Mean final entropy: 0.0005
  Max final entropy: 0.0031
  Pseudo loss: -1.25578
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.277 seconds.
  Mean final reward: -0.5238
  Mean return: -4.1666
  Mean final entropy: 0.0021
  Max final entropy: 0.0109
  Pseudo loss: -0.96351
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.274 seconds.
  Mean final reward: -0.4418
  Mean return: -3.5522
  Mean final entropy: 0.0018
  Max final entropy: 0.0109
  Pseudo loss: -0.58186
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.302 seconds.
  Mean final reward: -0.2793
  Mean return: -3.4039
  Mean final entropy: 0.0013
  Max final entropy: 0.0093
  Pseudo loss: -0.71694
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.322 seconds.
  Mean final reward: -0.6537
  Mean return: -3.7010
  Mean final entropy: 0.0026
  Max final entropy: 0.0118
  Pseudo loss: -2.17204
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (90/100) took 0.287 seconds.
  Mean final reward: -0.3328
  Mean return: -3.2851
  Mean final entropy: 0.0010
  Max final entropy: 0.0046
  Pseudo loss: -0.75484
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5500
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.10425
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.23264599 0.1807812  0.30507877 0.2575463  0.03428225 0.6649103
 0.39599505 0.5117127 ]
  Mean final entropy: 0.000088
  Max final entropy: 0.000395
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.3947865e-07 2.8541501e-07 7.1768204e-07 4.7929327e-05 4.5991801e-08
 3.6145232e-06 2.0635348e-07 1.3868156e-07]
  Max Min Entropy: 4.792932668351568e-05
  Best actions: [array([8, 6, 2, 0, 4, 8, 1, 5]), array([7, 2, 6, 2, 5, 6, 7, 8]), array([8, 2, 8, 4, 3, 8, 6, 1])]

####################################


step: 90
seed: 582657
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.264 seconds.
  Mean final reward: -2.0509
  Mean return: -9.6052
  Mean final entropy: 0.0271
  Max final entropy: 0.1096
  Pseudo loss: -0.81372
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.267 seconds.
  Mean final reward: -0.3994
  Mean return: -5.8104
  Mean final entropy: 0.0026
  Max final entropy: 0.0188
  Pseudo loss: -1.72067
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (30/100) took 0.296 seconds.
  Mean final reward: -0.3705
  Mean return: -5.4992
  Mean final entropy: 0.0022
  Max final entropy: 0.0149
  Pseudo loss: -1.49850
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1671
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.23743
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.5368
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.17543
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (60/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0199
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.04168
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (70/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9679
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: 0.03268
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2502
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -2.21216
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0562
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.00708
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0199
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.03937
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Testing with greedy policy:
  Start entropy: [0.2556694  0.09879914 0.06986743 0.17594257 0.3418876  0.46899933
 0.60303974 0.19743285]
  Mean final entropy: 0.000031
  Max final entropy: 0.000102
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.9862233e-07 3.0401852e-05 1.0095138e-06 2.0236103e-08 4.6802885e-05
 9.7235124e-07 9.9897261e-05 2.9494467e-07]
  Max Min Entropy: 9.989726095227525e-05
  Best actions: [array([3, 0, 4, 4, 3, 5, 3, 3]), array([6, 6, 3, 1, 5, 2, 0, 6]), array([3, 0, 4, 4, 2, 0, 1, 0])]

####################################


step: 91
seed: 603645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.269 seconds.
  Mean final reward: -2.5605
  Mean return: -10.3489
  Mean final entropy: 0.0423
  Max final entropy: 0.1191
  Pseudo loss: -0.39322
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.288 seconds.
  Mean final reward: -1.1392
  Mean return: -6.6661
  Mean final entropy: 0.0267
  Max final entropy: 0.1863
  Pseudo loss: -2.12867
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.289 seconds.
  Mean final reward: -0.6984
  Mean return: -4.5096
  Mean final entropy: 0.0269
  Max final entropy: 0.2134
  Pseudo loss: 0.43657
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5013
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.34967
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.9656
  Mean return: -5.6061
  Mean final entropy: 0.0089
  Max final entropy: 0.0505
  Pseudo loss: -6.45987
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3498
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -1.13489
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.276 seconds.
  Mean final reward: -0.5062
  Mean return: -4.1378
  Mean final entropy: 0.0073
  Max final entropy: 0.0574
  Pseudo loss: -1.74436
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.276 seconds.
  Mean final reward: -0.1432
  Mean return: -2.6301
  Mean final entropy: 0.0006
  Max final entropy: 0.0031
  Pseudo loss: -0.22084
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.281 seconds.
  Mean final reward: -0.0967
  Mean return: -2.3038
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.05529
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (100/100) took 0.278 seconds.
  Mean final reward: -0.2953
  Mean return: -3.1782
  Mean final entropy: 0.0015
  Max final entropy: 0.0106
  Pseudo loss: -2.91798
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.6249006  0.5806468  0.5762399  0.3795387  0.39038557 0.36641783
 0.1041046  0.61049795]
  Mean final entropy: 0.000145
  Max final entropy: 0.000711
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.2409584e-08 6.2846466e-06 4.7622698e-06 1.1904418e-06 1.7837000e-07
 4.1936470e-08 1.0885657e-07 3.1528214e-06]
  Max Min Entropy: 6.284646588028409e-06
  Best actions: [array([2, 0, 2, 3, 2, 7, 8, 0]), array([5, 6, 5, 6, 0, 4, 5, 3]), array([4, 0, 6, 1, 0, 3, 2, 5])]

####################################


step: 92
seed: 625139
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.267 seconds.
  Mean final reward: -1.8927
  Mean return: -8.0867
  Mean final entropy: 0.0194
  Max final entropy: 0.0471
  Pseudo loss: -2.97584
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.271 seconds.
  Mean final reward: -1.7822
  Mean return: -7.9262
  Mean final entropy: 0.0441
  Max final entropy: 0.1961
  Pseudo loss: -9.00509
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.305 seconds.
  Mean final reward: -0.6312
  Mean return: -6.3448
  Mean final entropy: 0.0196
  Max final entropy: 0.1559
  Pseudo loss: -4.32479
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (40/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4093
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.03357
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.307 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4058
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -2.52823
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4093
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00884
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4093
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.03417
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4093
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00463
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4093
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00598
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4093
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00598
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.4064443  0.26548678 0.19027738 0.35401607 0.5473144  0.51277286
 0.42466676 0.48290914]
  Mean final entropy: 0.000011
  Max final entropy: 0.000049
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.6466371e-07 1.5071574e-08 1.6220960e-07 2.2905864e-08 6.4828186e-05
 1.0067059e-07 2.9174889e-06 3.7468871e-04]
  Max Min Entropy: 0.0003746887086890638
  Best actions: [array([0, 8, 0, 8, 3, 1, 0, 8]), array([1, 8, 4, 2, 6, 0, 2, 7]), array([2, 2, 0, 7, 7, 2, 0, 4])]

####################################


step: 93
seed: 647145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.291 seconds.
  Mean final reward: -1.0946
  Mean return: -5.1393
  Mean final entropy: 0.0241
  Max final entropy: 0.1768
  Pseudo loss: -4.27734
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.280 seconds.
  Mean final reward: -1.4882
  Mean return: -6.9341
  Mean final entropy: 0.0095
  Max final entropy: 0.0413
  Pseudo loss: -3.31773
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.279 seconds.
  Mean final reward: -1.8190
  Mean return: -6.2327
  Mean final entropy: 0.0694
  Max final entropy: 0.4988
  Pseudo loss: -5.56600
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.286 seconds.
  Mean final reward: -0.2785
  Mean return: -2.8059
  Mean final entropy: 0.0013
  Max final entropy: 0.0093
  Pseudo loss: -0.81249
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.278 seconds.
  Mean final reward: -0.7866
  Mean return: -3.8598
  Mean final entropy: 0.0654
  Max final entropy: 0.5215
  Pseudo loss: -4.66425
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.301 seconds.
  Mean final reward: -0.2867
  Mean return: -2.7734
  Mean final entropy: 0.0010
  Max final entropy: 0.0046
  Pseudo loss: -0.78826
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.288 seconds.
  Mean final reward: -0.0962
  Mean return: -2.3794
  Mean final entropy: 0.0003
  Max final entropy: 0.0022
  Pseudo loss: 0.12163
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (80/100) took 0.323 seconds.
  Mean final reward: -0.1285
  Mean return: -2.4116
  Mean final entropy: 0.0004
  Max final entropy: 0.0028
  Pseudo loss: -0.16337
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.277 seconds.
  Mean final reward: -0.0962
  Mean return: -2.6558
  Mean final entropy: 0.0003
  Max final entropy: 0.0022
  Pseudo loss: -0.38831
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2832
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.01283
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.58045316 0.14169084 0.59280586 0.6806974  0.67201984 0.490186
 0.19002436 0.26521608]
  Mean final entropy: 0.000281
  Max final entropy: 0.002159
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [3.0147380e-08 4.9911966e-07 1.8207055e-07 4.5356799e-08 9.4178290e-07
 1.2530026e-07 1.2783421e-07 7.2300367e-07]
  Max Min Entropy: 9.417829005542444e-07
  Best actions: [array([3, 3, 4, 3, 4, 5, 4, 2]), array([5, 8, 7, 5, 7, 8, 5, 0]), array([1, 2, 6, 4, 6, 3, 0, 2])]

####################################


step: 94
seed: 669669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.9378
  Mean return: -11.1009
  Mean final entropy: 0.0145
  Max final entropy: 0.0378
  Pseudo loss: 1.00573
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.261 seconds.
  Mean final reward: -1.4745
  Mean return: -7.5596
  Mean final entropy: 0.0118
  Max final entropy: 0.0375
  Pseudo loss: 0.15633
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.264 seconds.
  Mean final reward: -0.7386
  Mean return: -7.1195
  Mean final entropy: 0.0031
  Max final entropy: 0.0127
  Pseudo loss: -1.08484
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.395 seconds.
  Mean final reward: -1.3125
  Mean return: -6.8590
  Mean final entropy: 0.0091
  Max final entropy: 0.0317
  Pseudo loss: -4.21508
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.265 seconds.
  Mean final reward: -0.5165
  Mean return: -4.6809
  Mean final entropy: 0.0039
  Max final entropy: 0.0285
  Pseudo loss: 0.42229
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.280 seconds.
  Mean final reward: -0.6005
  Mean return: -5.0062
  Mean final entropy: 0.0042
  Max final entropy: 0.0285
  Pseudo loss: 0.05739
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.283 seconds.
  Mean final reward: -0.0129
  Mean return: -4.6742
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -1.54228
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.256 seconds.
  Mean final reward: -0.4849
  Mean return: -5.2013
  Mean final entropy: 0.0062
  Max final entropy: 0.0484
  Pseudo loss: -2.39310
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.264 seconds.
  Mean final reward: -0.3606
  Mean return: -4.2513
  Mean final entropy: 0.0025
  Max final entropy: 0.0179
  Pseudo loss: -1.72338
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.263 seconds.
  Mean final reward: -0.0553
  Mean return: -3.8519
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: -1.93184
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.3462963  0.5995767  0.21097052 0.2145117  0.36777645 0.46844023
 0.2037954  0.13343331]
  Mean final entropy: 0.000195
  Max final entropy: 0.000661
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.0066075e-05 2.9010903e-06 4.0241008e-07 1.9279552e-05 1.7505403e-06
 7.9249460e-08 1.0200092e-05 1.2635574e-07]
  Max Min Entropy: 5.0066075345966965e-05
  Best actions: [array([1, 8, 1, 2, 1, 6, 5, 8]), array([0, 2, 2, 5, 7, 3, 4, 7]), array([1, 8, 1, 6, 1, 1, 5, 4])]

####################################


step: 95
seed: 692717
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.295 seconds.
  Mean final reward: -0.4991
  Mean return: -3.7626
  Mean final entropy: 0.0027
  Max final entropy: 0.0160
  Pseudo loss: -0.06688
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.277 seconds.
  Mean final reward: -0.9303
  Mean return: -5.6485
  Mean final entropy: 0.0060
  Max final entropy: 0.0270
  Pseudo loss: -5.54416
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.299 seconds.
  Mean final reward: -1.3888
  Mean return: -7.5056
  Mean final entropy: 0.0226
  Max final entropy: 0.1385
  Pseudo loss: -2.49565
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.305 seconds.
  Mean final reward: -0.0495
  Mean return: -2.4025
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: 0.14479
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.340 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0554
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.37862
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.309 seconds.
  Mean final reward: -0.1670
  Mean return: -2.5012
  Mean final entropy: 0.0009
  Max final entropy: 0.0038
  Pseudo loss: -0.30555
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.280 seconds.
  Mean final reward: -0.1713
  Mean return: -2.3177
  Mean final entropy: 0.0009
  Max final entropy: 0.0039
  Pseudo loss: 0.14844
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: -0.1713
  Mean return: -2.2268
  Mean final entropy: 0.0008
  Max final entropy: 0.0039
  Pseudo loss: 0.75928
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.278 seconds.
  Mean final reward: -0.2067
  Mean return: -3.2148
  Mean final entropy: 0.0009
  Max final entropy: 0.0039
  Pseudo loss: -2.38038
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (100/100) took 0.276 seconds.
  Mean final reward: -0.2049
  Mean return: -3.0203
  Mean final entropy: 0.0010
  Max final entropy: 0.0039
  Pseudo loss: -0.68470
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.11490625 0.29021063 0.11412078 0.37691054 0.40470177 0.49250802
 0.36963844 0.6567217 ]
  Mean final entropy: 0.000930
  Max final entropy: 0.003937
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [1.2925494e-07 1.0298774e-06 1.4859158e-07 1.0404771e-04 2.8586503e-06
 9.9171052e-07 5.4051856e-07 3.0717550e-07]
  Max Min Entropy: 0.00010404770728200674
  Best actions: [array([1, 3, 5, 6, 1, 6, 1, 7]), array([7, 4, 4, 8, 7, 7, 2, 1]), array([6, 3, 7, 6, 5, 8, 2, 0])]

####################################


step: 96
seed: 716295
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.267 seconds.
  Mean final reward: -1.4449
  Mean return: -8.8114
  Mean final entropy: 0.0320
  Max final entropy: 0.2124
  Pseudo loss: -0.58290
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.275 seconds.
  Mean final reward: -0.4884
  Mean return: -5.4781
  Mean final entropy: 0.0021
  Max final entropy: 0.0112
  Pseudo loss: -2.11991
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.270 seconds.
  Mean final reward: -0.9004
  Mean return: -4.4979
  Mean final entropy: 0.0046
  Max final entropy: 0.0161
  Pseudo loss: -1.11335
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.269 seconds.
  Mean final reward: -0.1262
  Mean return: -3.0744
  Mean final entropy: 0.0005
  Max final entropy: 0.0027
  Pseudo loss: -1.55659
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.274 seconds.
  Mean final reward: -0.0582
  Mean return: -2.5452
  Mean final entropy: 0.0002
  Max final entropy: 0.0016
  Pseudo loss: -0.00881
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.271 seconds.
  Mean final reward: -0.1615
  Mean return: -3.3879
  Mean final entropy: 0.0006
  Max final entropy: 0.0036
  Pseudo loss: -1.04055
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.267 seconds.
  Mean final reward: -0.2559
  Mean return: -4.1098
  Mean final entropy: 0.0008
  Max final entropy: 0.0039
  Pseudo loss: -3.24550
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.273 seconds.
  Mean final reward: -0.2114
  Mean return: -2.7992
  Mean final entropy: 0.0007
  Max final entropy: 0.0054
  Pseudo loss: -0.64166
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.0582
  Mean return: -2.6460
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: 0.00690
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.273 seconds.
  Mean final reward: -0.0612
  Mean return: -2.6490
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: 0.00819
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Testing with greedy policy:
  Start entropy: [0.56762266 0.44321266 0.2413387  0.08882944 0.57742155 0.6648437
 0.08155325 0.18159935]
  Mean final entropy: 0.000254
  Max final entropy: 0.001593
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [2.5331142e-06 6.0069034e-08 2.7166493e-07 7.8204044e-07 3.2147003e-05
 7.5230844e-07 4.2602983e-07 7.5764029e-07]
  Max Min Entropy: 3.214700336684473e-05
  Best actions: [array([8, 8, 3, 6, 3, 8, 8, 8]), array([7, 2, 1, 7, 4, 7, 6, 6]), array([8, 4, 4, 6, 3, 8, 4, 7])]

####################################


step: 97
seed: 740409
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.278 seconds.
  Mean final reward: -2.4083
  Mean return: -9.6118
  Mean final entropy: 0.0387
  Max final entropy: 0.1485
  Pseudo loss: -0.01049
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.269 seconds.
  Mean final reward: -1.4021
  Mean return: -6.8785
  Mean final entropy: 0.0241
  Max final entropy: 0.1683
  Pseudo loss: -1.58082
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.277 seconds.
  Mean final reward: -0.4789
  Mean return: -4.3684
  Mean final entropy: 0.0017
  Max final entropy: 0.0084
  Pseudo loss: -2.15727
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.5933
  Mean return: -3.7982
  Mean final entropy: 0.0030
  Max final entropy: 0.0178
  Pseudo loss: -0.68216
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (50/100) took 0.278 seconds.
  Mean final reward: -0.5300
  Mean return: -3.5907
  Mean final entropy: 0.0049
  Max final entropy: 0.0368
  Pseudo loss: -0.99360
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.275 seconds.
  Mean final reward: -0.3168
  Mean return: -3.2982
  Mean final entropy: 0.0012
  Max final entropy: 0.0067
  Pseudo loss: -0.45715
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.308 seconds.
  Mean final reward: -0.2253
  Mean return: -3.0299
  Mean final entropy: 0.0007
  Max final entropy: 0.0032
  Pseudo loss: -0.80493
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.276 seconds.
  Mean final reward: -0.0986
  Mean return: -2.9497
  Mean final entropy: 0.0004
  Max final entropy: 0.0019
  Pseudo loss: -0.38886
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.276 seconds.
  Mean final reward: -0.0194
  Mean return: -2.5320
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: 0.05689
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.278 seconds.
  Mean final reward: -0.0194
  Mean return: -2.5854
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: 0.12967
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.0321643  0.13563824 0.15368134 0.10829367 0.10379599 0.41000342
 0.57064176 0.46525586]
  Mean final entropy: 0.000218
  Max final entropy: 0.001168
  Solved trajectories: 7 / 8
Exaushtive search:
  Minimum entropy: [7.4096408e-07 2.9086078e-07 7.9944186e-07 3.4547222e-06 1.2699023e-06
 6.5441913e-07 4.8086497e-07 3.2889076e-07]
  Max Min Entropy: 3.4547222185210558e-06
  Best actions: [array([4, 4, 1, 4, 0, 3, 8, 7]), array([7, 7, 2, 0, 6, 4, 5, 1]), array([4, 1, 6, 2, 5, 3, 8, 0])]

####################################


step: 98
seed: 765065
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.274 seconds.
  Mean final reward: -2.2129
  Mean return: -8.4244
  Mean final entropy: 0.0260
  Max final entropy: 0.0866
  Pseudo loss: -0.02096
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.274 seconds.
  Mean final reward: -1.2973
  Mean return: -7.1024
  Mean final entropy: 0.0137
  Max final entropy: 0.0705
  Pseudo loss: -1.45769
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.277 seconds.
  Mean final reward: -1.6931
  Mean return: -6.7829
  Mean final entropy: 0.0344
  Max final entropy: 0.2205
  Pseudo loss: -3.10329
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.276 seconds.
  Mean final reward: -0.5486
  Mean return: -4.1239
  Mean final entropy: 0.0020
  Max final entropy: 0.0102
  Pseudo loss: -1.87174
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.272 seconds.
  Mean final reward: -0.2060
  Mean return: -3.8074
  Mean final entropy: 0.0008
  Max final entropy: 0.0043
  Pseudo loss: -1.29972
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.279 seconds.
  Mean final reward: -0.5290
  Mean return: -4.3860
  Mean final entropy: 0.0021
  Max final entropy: 0.0083
  Pseudo loss: -4.22114
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.282 seconds.
  Mean final reward: -0.3869
  Mean return: -4.5598
  Mean final entropy: 0.0029
  Max final entropy: 0.0221
  Pseudo loss: -4.28377
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7806
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.00663
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (90/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7806
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.03463
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (100/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7806
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.11175
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.28889543 0.35734415 0.46509892 0.57365096 0.16390076 0.3365096
 0.39345726 0.4901527 ]
  Mean final entropy: 0.000141
  Max final entropy: 0.000457
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0269453e-04 7.5855125e-05 1.4507870e-06 2.0519525e-07 9.4282382e-07
 3.6810192e-08 4.2458450e-09 3.6477752e-07]
  Max Min Entropy: 0.00020269452943466604
  Best actions: [array([6, 4, 5, 0, 0, 8, 0, 3]), array([1, 1, 8, 2, 4, 7, 1, 6]), array([2, 4, 5, 0, 7, 0, 5, 1])]

####################################


step: 99
seed: 790269
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.269 seconds.
  Mean final reward: -1.6418
  Mean return: -9.0428
  Mean final entropy: 0.0114
  Max final entropy: 0.0386
  Pseudo loss: -0.51066
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.275 seconds.
  Mean final reward: -0.0890
  Mean return: -3.9559
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.95806
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0278
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.06196
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0278
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.10268
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -3.5386
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.10488
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0278
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.71948
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.268 seconds.
  Mean final reward: -0.1775
  Mean return: -4.8074
  Mean final entropy: 0.0007
  Max final entropy: 0.0041
  Pseudo loss: -1.54170
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0278
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.26716
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -3.5386
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.03025
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.5386
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.02645
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Testing with greedy policy:
  Start entropy: [0.5619099  0.529983   0.41682076 0.08303098 0.46556336 0.41286337
 0.43107253 0.43291837]
  Mean final entropy: 0.000121
  Max final entropy: 0.000436
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0720186e-07 6.7861788e-07 4.5696393e-07 1.0341612e-06 5.7336660e-05
 7.3676492e-06 1.2376017e-06 2.4173240e-04]
  Max Min Entropy: 0.0002417323994450271
  Best actions: [array([0, 3, 3, 1, 8, 3, 0, 6]), array([1, 6, 6, 7, 7, 0, 2, 3]), array([8, 1, 7, 1, 8, 3, 8, 6])]

####################################


step: 100
seed: 816027
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [64]

Using device: cuda

Episode (10/100) took 0.273 seconds.
  Mean final reward: -1.2718
  Mean return: -6.9890
  Mean final entropy: 0.0420
  Max final entropy: 0.2526
  Pseudo loss: -3.52223
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.273 seconds.
  Mean final reward: -0.7558
  Mean return: -4.8674
  Mean final entropy: 0.0301
  Max final entropy: 0.2371
  Pseudo loss: -1.23882
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.273 seconds.
  Mean final reward: -1.3245
  Mean return: -6.2278
  Mean final entropy: 0.0387
  Max final entropy: 0.1605
  Pseudo loss: -5.77563
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.270 seconds.
  Mean final reward: -0.6382
  Mean return: -4.2766
  Mean final entropy: 0.0046
  Max final entropy: 0.0301
  Pseudo loss: -2.83641
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.264 seconds.
  Mean final reward: -0.3883
  Mean return: -3.4037
  Mean final entropy: 0.0031
  Max final entropy: 0.0223
  Pseudo loss: 0.19249
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.291 seconds.
  Mean final reward: -0.6431
  Mean return: -4.2678
  Mean final entropy: 0.0042
  Max final entropy: 0.0267
  Pseudo loss: -1.95934
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (70/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0919
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.32975
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.263 seconds.
  Mean final reward: -0.3480
  Mean return: -3.4746
  Mean final entropy: 0.0021
  Max final entropy: 0.0162
  Pseudo loss: -1.12335
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1789
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.32321
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0871
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.52147
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.35883415 0.2716547  0.6079421  0.04067008 0.24888304 0.46900183
 0.1487752  0.4561959 ]
  Mean final entropy: 0.000192
  Max final entropy: 0.000673
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.4416714e-06 2.5964016e-07 1.3390631e-06 4.2017192e-07 5.7766012e-08
 4.4409730e-04 2.9650910e-06 2.4385711e-06]
  Max Min Entropy: 0.0004440972988959402
  Best actions: [array([2, 5, 2, 3, 8, 0, 2, 8]), array([0, 7, 0, 6, 5, 6, 3, 2]), array([2, 1, 6, 2, 2, 0, 4, 8])]

####################################

Training took 4768.252 seconds
Solved trajectories: 739 / 800
Max entropy at final step: 0.08310
