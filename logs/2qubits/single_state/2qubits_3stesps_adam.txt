
step: 1
seed: 12345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.259 seconds.
  Mean final reward: -1.5411
  Mean return: -7.2416
  Mean final entropy: 0.0399
  Max final entropy: 0.2534
  Pseudo loss: -0.88697
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.318 seconds.
  Mean final reward: -1.7589
  Mean return: -6.8265
  Mean final entropy: 0.0192
  Max final entropy: 0.0929
  Pseudo loss: -3.93102
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.350 seconds.
  Mean final reward: -0.3995
  Mean return: -1.4275
  Mean final entropy: 0.0021
  Max final entropy: 0.0125
  Pseudo loss: -0.45305
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.279 seconds.
  Mean final reward: -0.6965
  Mean return: -2.9529
  Mean final entropy: 0.0046
  Max final entropy: 0.0250
  Pseudo loss: -4.40165
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.288 seconds.
  Mean final reward: -0.0501
  Mean return: -0.4375
  Mean final entropy: 0.0005
  Max final entropy: 0.0011
  Pseudo loss: 0.33675
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.289 seconds.
  Mean final reward: -0.0504
  Mean return: -0.6275
  Mean final entropy: 0.0006
  Max final entropy: 0.0013
  Pseudo loss: 0.17890
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (35/100) took 0.302 seconds.
  Mean final reward: -0.1285
  Mean return: -0.6625
  Mean final entropy: 0.0009
  Max final entropy: 0.0013
  Pseudo loss: 0.18052
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.303 seconds.
  Mean final reward: -0.0504
  Mean return: -0.2037
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: 0.25044
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.4601
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.11532
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.384 seconds.
  Mean final reward: -0.0337
  Mean return: -0.3404
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.17499
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (55/100) took 0.304 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1871
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.08411
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.250
Episode (60/100) took 0.304 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.04683
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.335 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0767
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.04393
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (70/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0767
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.06260
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (75/100) took 0.317 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.302 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.18152
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.38503304 0.38503304 0.38503304 0.38503304 0.38503304 0.38503304
 0.38503304 0.38503304]
  Mean final entropy: 0.000004
  Max final entropy: 0.000004
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07
 4.3489302e-07 4.3489302e-07 4.3489302e-07]
  Max Min Entropy: 4.3489302470334223e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 2
seed: 12329
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1318
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.12334
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0432
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.05995
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0231
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: -0.05763
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (25/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0216
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.08856
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (40/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.321 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.306 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.00239316 0.00239316 0.00239316 0.00239316 0.00239316 0.00239316
 0.00239316 0.00239316]
  Mean final entropy: 0.000416
  Max final entropy: 0.000416
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.729065e-08 7.729065e-08 7.729065e-08 7.729065e-08 7.729065e-08
 7.729065e-08 7.729065e-08 7.729065e-08]
  Max Min Entropy: 7.729065032435756e-08
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 3
seed: 12285
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.286 seconds.
  Mean final reward: -1.0215
  Mean return: -6.8029
  Mean final entropy: 0.0046
  Max final entropy: 0.0174
  Pseudo loss: -0.62139
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.264 seconds.
  Mean final reward: -1.1207
  Mean return: -7.8978
  Mean final entropy: 0.0090
  Max final entropy: 0.0429
  Pseudo loss: 0.03141
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.280 seconds.
  Mean final reward: -2.0765
  Mean return: -8.4583
  Mean final entropy: 0.0316
  Max final entropy: 0.1311
  Pseudo loss: -1.62648
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.437 seconds.
  Mean final reward: -0.8061
  Mean return: -7.1687
  Mean final entropy: 0.0035
  Max final entropy: 0.0131
  Pseudo loss: -0.58473
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.274 seconds.
  Mean final reward: -1.0106
  Mean return: -6.4317
  Mean final entropy: 0.0085
  Max final entropy: 0.0507
  Pseudo loss: -1.57419
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.301 seconds.
  Mean final reward: -0.4617
  Mean return: -6.2505
  Mean final entropy: 0.0018
  Max final entropy: 0.0039
  Pseudo loss: -0.46647
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (35/100) took 0.281 seconds.
  Mean final reward: -1.0097
  Mean return: -6.1260
  Mean final entropy: 0.0060
  Max final entropy: 0.0303
  Pseudo loss: -0.65402
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (40/100) took 0.303 seconds.
  Mean final reward: -0.8280
  Mean return: -6.2654
  Mean final entropy: 0.0061
  Max final entropy: 0.0303
  Pseudo loss: -4.41686
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (45/100) took 0.282 seconds.
  Mean final reward: -0.7312
  Mean return: -4.8218
  Mean final entropy: 0.0025
  Max final entropy: 0.0070
  Pseudo loss: -0.48148
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.281 seconds.
  Mean final reward: -0.4116
  Mean return: -4.9020
  Mean final entropy: 0.0016
  Max final entropy: 0.0106
  Pseudo loss: -1.51990
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (55/100) took 0.280 seconds.
  Mean final reward: -0.1612
  Mean return: -3.8278
  Mean final entropy: 0.0007
  Max final entropy: 0.0036
  Pseudo loss: -0.01266
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (60/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2826
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.97751
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (65/100) took 0.282 seconds.
  Mean final reward: -0.2961
  Mean return: -3.5699
  Mean final entropy: 0.0011
  Max final entropy: 0.0026
  Pseudo loss: 1.31508
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.262 seconds.
  Mean final reward: -0.3548
  Mean return: -3.9105
  Mean final entropy: 0.0011
  Max final entropy: 0.0034
  Pseudo loss: 0.13175
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2537
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: 0.53646
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (80/100) took 0.274 seconds.
  Mean final reward: -0.3785
  Mean return: -3.7966
  Mean final entropy: 0.0012
  Max final entropy: 0.0036
  Pseudo loss: -1.63832
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0856
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.06976
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.264 seconds.
  Mean final reward: -0.1124
  Mean return: -2.8824
  Mean final entropy: 0.0003
  Max final entropy: 0.0025
  Pseudo loss: -0.52767
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6156
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.42696
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.33193147 0.33193147 0.33193147 0.33193147 0.33193147 0.33193147
 0.33193147 0.33193147]
  Mean final entropy: 0.000006
  Max final entropy: 0.000006
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.382591e-07 5.382591e-07 5.382591e-07 5.382591e-07 5.382591e-07
 5.382591e-07 5.382591e-07 5.382591e-07]
  Max Min Entropy: 5.382590870794957e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [6, 6, 6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 4
seed: 12219
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.297 seconds.
  Mean final reward: -0.9741
  Mean return: -5.2925
  Mean final entropy: 0.0080
  Max final entropy: 0.0408
  Pseudo loss: -2.53551
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.305 seconds.
  Mean final reward: -0.9701
  Mean return: -4.2049
  Mean final entropy: 0.0144
  Max final entropy: 0.0881
  Pseudo loss: -4.84509
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.286 seconds.
  Mean final reward: -0.5103
  Mean return: -2.0472
  Mean final entropy: 0.0031
  Max final entropy: 0.0215
  Pseudo loss: -2.63211
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.400 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.395 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.398 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.364 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.310 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.5825697 0.5825697 0.5825697 0.5825697 0.5825697 0.5825697 0.5825697
 0.5825697]
  Mean final entropy: 0.000053
  Max final entropy: 0.000053
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.73917e-07 3.73917e-07 3.73917e-07 3.73917e-07 3.73917e-07 3.73917e-07
 3.73917e-07 3.73917e-07]
  Max Min Entropy: 3.739170040262252e-07
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 5
seed: 12137
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.287 seconds.
  Mean final reward: -1.7409
  Mean return: -8.8593
  Mean final entropy: 0.0073
  Max final entropy: 0.0177
  Pseudo loss: -0.48711
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (10/100) took 0.295 seconds.
  Mean final reward: -1.0940
  Mean return: -5.4387
  Mean final entropy: 0.0046
  Max final entropy: 0.0133
  Pseudo loss: 0.63881
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.289 seconds.
  Mean final reward: -1.2330
  Mean return: -6.2154
  Mean final entropy: 0.0080
  Max final entropy: 0.0386
  Pseudo loss: -2.05752
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.289 seconds.
  Mean final reward: -1.5286
  Mean return: -6.6948
  Mean final entropy: 0.0069
  Max final entropy: 0.0143
  Pseudo loss: 1.11182
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (25/100) took 0.321 seconds.
  Mean final reward: -0.7843
  Mean return: -4.9715
  Mean final entropy: 0.0024
  Max final entropy: 0.0064
  Pseudo loss: -0.70503
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (30/100) took 0.308 seconds.
  Mean final reward: -0.3280
  Mean return: -3.5841
  Mean final entropy: 0.0016
  Max final entropy: 0.0105
  Pseudo loss: 1.02728
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.314 seconds.
  Mean final reward: -1.6855
  Mean return: -7.0077
  Mean final entropy: 0.0138
  Max final entropy: 0.0514
  Pseudo loss: -5.74371
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.295 seconds.
  Mean final reward: -0.2898
  Mean return: -3.4711
  Mean final entropy: 0.0014
  Max final entropy: 0.0102
  Pseudo loss: -1.62771
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (45/100) took 0.340 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.306 seconds.
  Mean final reward: -0.0654
  Mean return: -3.2467
  Mean final entropy: 0.0003
  Max final entropy: 0.0017
  Pseudo loss: -1.76419
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (55/100) took 0.313 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9669
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: -0.35725
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.313 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.330 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0663
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.08597
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2575
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.70293
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8852
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.32000387 0.32000387 0.32000387 0.32000387 0.32000387 0.32000387
 0.32000387 0.32000387]
  Mean final entropy: 0.000004
  Max final entropy: 0.000004
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.565885e-07 5.565885e-07 5.565885e-07 5.565885e-07 5.565885e-07
 5.565885e-07 5.565885e-07 5.565885e-07]
  Max Min Entropy: 5.565884748648386e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 6
seed: 12045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.279 seconds.
  Mean final reward: -2.0747
  Mean return: -9.7495
  Mean final entropy: 0.0332
  Max final entropy: 0.1476
  Pseudo loss: -1.30526
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.285 seconds.
  Mean final reward: -2.4434
  Mean return: -10.0625
  Mean final entropy: 0.0157
  Max final entropy: 0.0458
  Pseudo loss: -1.94687
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.270 seconds.
  Mean final reward: -1.3222
  Mean return: -7.9817
  Mean final entropy: 0.0094
  Max final entropy: 0.0514
  Pseudo loss: -2.76295
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.282 seconds.
  Mean final reward: -0.5834
  Mean return: -5.1528
  Mean final entropy: 0.0021
  Max final entropy: 0.0049
  Pseudo loss: -0.55085
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.278 seconds.
  Mean final reward: -0.6215
  Mean return: -5.0016
  Mean final entropy: 0.0023
  Max final entropy: 0.0050
  Pseudo loss: 0.15488
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.6455
  Mean return: -4.8156
  Mean final entropy: 0.0023
  Max final entropy: 0.0052
  Pseudo loss: -0.38902
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.318 seconds.
  Mean final reward: -0.9633
  Mean return: -5.7574
  Mean final entropy: 0.0030
  Max final entropy: 0.0050
  Pseudo loss: -0.52428
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (40/100) took 0.285 seconds.
  Mean final reward: -0.4537
  Mean return: -4.2045
  Mean final entropy: 0.0025
  Max final entropy: 0.0120
  Pseudo loss: -2.66335
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (45/100) took 0.284 seconds.
  Mean final reward: -0.1091
  Mean return: -3.8967
  Mean final entropy: 0.0009
  Max final entropy: 0.0019
  Pseudo loss: -0.84487
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.5760
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: -0.46704
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.317 seconds.
  Mean final reward: -0.4072
  Mean return: -4.1580
  Mean final entropy: 0.0025
  Max final entropy: 0.0147
  Pseudo loss: -3.02511
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (60/100) took 0.296 seconds.
  Mean final reward: -0.0715
  Mean return: -3.6347
  Mean final entropy: 0.0006
  Max final entropy: 0.0018
  Pseudo loss: -0.33368
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (65/100) took 0.307 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.292 seconds.
  Mean final reward: -0.1758
  Mean return: -3.6715
  Mean final entropy: 0.0008
  Max final entropy: 0.0020
  Pseudo loss: -0.43312
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.289 seconds.
  Mean final reward: -0.0715
  Mean return: -3.5672
  Mean final entropy: 0.0005
  Max final entropy: 0.0018
  Pseudo loss: -0.14392
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (95/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.2932917 0.2932917 0.2932917 0.2932917 0.2932917 0.2932917 0.2932917
 0.2932917]
  Mean final entropy: 0.000239
  Max final entropy: 0.000239
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.9962024e-05 2.9962024e-05 2.9962024e-05 2.9962024e-05 2.9962024e-05
 2.9962024e-05 2.9962024e-05 2.9962024e-05]
  Max Min Entropy: 2.9962024200358428e-05
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [4, 4, 4, 4, 4, 4, 4, 4], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 7
seed: 11949
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.271 seconds.
  Mean final reward: -3.3113
  Mean return: -13.5114
  Mean final entropy: 0.0834
  Max final entropy: 0.3188
  Pseudo loss: -0.25686
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.285 seconds.
  Mean final reward: -1.8511
  Mean return: -9.0535
  Mean final entropy: 0.0167
  Max final entropy: 0.0652
  Pseudo loss: 0.14623
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.271 seconds.
  Mean final reward: -1.8062
  Mean return: -8.2650
  Mean final entropy: 0.0132
  Max final entropy: 0.0485
  Pseudo loss: -0.03602
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.295 seconds.
  Mean final reward: -1.5113
  Mean return: -9.5110
  Mean final entropy: 0.0217
  Max final entropy: 0.1183
  Pseudo loss: -1.20090
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.280 seconds.
  Mean final reward: -1.2455
  Mean return: -6.0925
  Mean final entropy: 0.0119
  Max final entropy: 0.0597
  Pseudo loss: -2.30539
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.293 seconds.
  Mean final reward: -0.2432
  Mean return: -3.4280
  Mean final entropy: 0.0012
  Max final entropy: 0.0052
  Pseudo loss: -2.06585
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.296 seconds.
  Mean final reward: -0.4469
  Mean return: -3.4924
  Mean final entropy: 0.0039
  Max final entropy: 0.0282
  Pseudo loss: -5.66187
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5368
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -0.09145
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (45/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4509
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.02092
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (50/100) took 0.287 seconds.
  Mean final reward: -0.0065
  Mean return: -1.5109
  Mean final entropy: 0.0005
  Max final entropy: 0.0011
  Pseudo loss: -0.09387
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (55/100) took 0.294 seconds.
  Mean final reward: -0.1105
  Mean return: -1.6829
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -0.56736
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (60/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4897
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -0.03612
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (65/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4638
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4573
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.01868
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (75/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5026
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: -0.10274
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.283 seconds.
  Mean final reward: -0.1681
  Mean return: -1.7907
  Mean final entropy: 0.0009
  Max final entropy: 0.0035
  Pseudo loss: -1.17255
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (85/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4638
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.357 seconds.
  Mean final reward: -0.0065
  Mean return: -1.4638
  Mean final entropy: 0.0005
  Max final entropy: 0.0011
  Pseudo loss: 0.01541
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (95/100) took 0.316 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4638
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.328 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4638
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.47997645 0.47997645 0.47997645 0.47997645 0.47997645 0.47997645
 0.47997645 0.47997645]
  Mean final entropy: 0.000239
  Max final entropy: 0.000239
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.2287949e-07 2.2287949e-07 2.2287949e-07 2.2287949e-07 2.2287949e-07
 2.2287949e-07 2.2287949e-07 2.2287949e-07]
  Max Min Entropy: 2.2287949263954943e-07
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 8
seed: 11855
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.289 seconds.
  Mean final reward: -1.8819
  Mean return: -7.2547
  Mean final entropy: 0.0164
  Max final entropy: 0.0830
  Pseudo loss: -3.64734
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.282 seconds.
  Mean final reward: -0.7700
  Mean return: -4.9000
  Mean final entropy: 0.0027
  Max final entropy: 0.0080
  Pseudo loss: -2.92987
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.301 seconds.
  Mean final reward: -0.9137
  Mean return: -4.9390
  Mean final entropy: 0.0053
  Max final entropy: 0.0261
  Pseudo loss: -5.30216
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.3302
  Mean return: -2.9296
  Mean final entropy: 0.0010
  Max final entropy: 0.0026
  Pseudo loss: -0.49673
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.314 seconds.
  Mean final reward: -0.0662
  Mean return: -2.3784
  Mean final entropy: 0.0002
  Max final entropy: 0.0017
  Pseudo loss: -0.87486
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.339 seconds.
  Mean final reward: -0.2019
  Mean return: -2.8902
  Mean final entropy: 0.0007
  Max final entropy: 0.0050
  Pseudo loss: -3.21957
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1135
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.61204
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.331 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.314 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4264
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -1.73514
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.310 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0107
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.27111
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.22726473 0.22726473 0.22726473 0.22726473 0.22726473 0.22726473
 0.22726473 0.22726473]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.6317294e-06 1.6317294e-06 1.6317294e-06 1.6317294e-06 1.6317294e-06
 1.6317294e-06 1.6317294e-06 1.6317294e-06]
  Max Min Entropy: 1.6317294466716703e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 9
seed: 11769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.275 seconds.
  Mean final reward: -1.7186
  Mean return: -8.5186
  Mean final entropy: 0.0134
  Max final entropy: 0.0440
  Pseudo loss: 0.99642
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.262 seconds.
  Mean final reward: -0.4060
  Mean return: -4.4629
  Mean final entropy: 0.0029
  Max final entropy: 0.0211
  Pseudo loss: -0.93424
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (15/100) took 0.298 seconds.
  Mean final reward: -0.1808
  Mean return: -2.3079
  Mean final entropy: 0.0008
  Max final entropy: 0.0028
  Pseudo loss: -0.09066
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.280 seconds.
  Mean final reward: -0.1680
  Mean return: -2.3915
  Mean final entropy: 0.0006
  Max final entropy: 0.0019
  Pseudo loss: -1.56467
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.280 seconds.
  Mean final reward: -0.2718
  Mean return: -2.5724
  Mean final entropy: 0.0009
  Max final entropy: 0.0038
  Pseudo loss: -1.44785
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.336 seconds.
  Mean final reward: -0.2241
  Mean return: -2.3287
  Mean final entropy: 0.0008
  Max final entropy: 0.0026
  Pseudo loss: 0.41036
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.304 seconds.
  Mean final reward: -0.3655
  Mean return: -2.0737
  Mean final entropy: 0.0011
  Max final entropy: 0.0035
  Pseudo loss: -0.50283
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.306 seconds.
  Mean final reward: -0.1289
  Mean return: -1.4292
  Mean final entropy: 0.0005
  Max final entropy: 0.0017
  Pseudo loss: 0.24104
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (45/100) took 0.307 seconds.
  Mean final reward: -0.3666
  Mean return: -2.1171
  Mean final entropy: 0.0012
  Max final entropy: 0.0019
  Pseudo loss: -0.38008
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (50/100) took 0.316 seconds.
  Mean final reward: -0.1251
  Mean return: -1.4185
  Mean final entropy: 0.0006
  Max final entropy: 0.0017
  Pseudo loss: -0.01407
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (55/100) took 0.304 seconds.
  Mean final reward: -0.4344
  Mean return: -1.8610
  Mean final entropy: 0.0014
  Max final entropy: 0.0038
  Pseudo loss: -0.83655
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.279 seconds.
  Mean final reward: -0.0645
  Mean return: -1.1390
  Mean final entropy: 0.0003
  Max final entropy: 0.0017
  Pseudo loss: 0.16664
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (65/100) took 0.280 seconds.
  Mean final reward: -0.1926
  Mean return: -1.3255
  Mean final entropy: 0.0006
  Max final entropy: 0.0017
  Pseudo loss: 0.05903
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.279 seconds.
  Mean final reward: -0.0598
  Mean return: -0.9658
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: 0.05398
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.304 seconds.
  Mean final reward: -0.1951
  Mean return: -1.8196
  Mean final entropy: 0.0008
  Max final entropy: 0.0017
  Pseudo loss: -2.34910
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.284 seconds.
  Mean final reward: -0.1204
  Mean return: -0.8375
  Mean final entropy: 0.0005
  Max final entropy: 0.0016
  Pseudo loss: -0.24935
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (85/100) took 0.308 seconds.
  Mean final reward: 0.0000
  Mean return: -0.6547
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: -0.08635
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.329 seconds.
  Mean final reward: -0.0514
  Mean return: -0.8375
  Mean final entropy: 0.0002
  Max final entropy: 0.0015
  Pseudo loss: -0.40936
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (95/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.6547
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: -0.18655
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -0.5854
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.1356557 0.1356557 0.1356557 0.1356557 0.1356557 0.1356557 0.1356557
 0.1356557]
  Mean final entropy: 0.000086
  Max final entropy: 0.000086
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.09362844e-07 1.09362844e-07 1.09362844e-07 1.09362844e-07
 1.09362844e-07 1.09362844e-07 1.09362844e-07 1.09362844e-07]
  Max Min Entropy: 1.0936284411400266e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 10
seed: 11697
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.262 seconds.
  Mean final reward: -0.4520
  Mean return: -5.5454
  Mean final entropy: 0.0024
  Max final entropy: 0.0133
  Pseudo loss: -1.13450
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.278 seconds.
  Mean final reward: -0.3880
  Mean return: -5.8586
  Mean final entropy: 0.0015
  Max final entropy: 0.0050
  Pseudo loss: -1.16688
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.264 seconds.
  Mean final reward: -1.1592
  Mean return: -5.7417
  Mean final entropy: 0.0111
  Max final entropy: 0.0520
  Pseudo loss: -3.50067
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.266 seconds.
  Mean final reward: -0.9346
  Mean return: -5.4152
  Mean final entropy: 0.0197
  Max final entropy: 0.1466
  Pseudo loss: -5.03328
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.254 seconds.
  Mean final reward: -1.4773
  Mean return: -5.9897
  Mean final entropy: 0.0120
  Max final entropy: 0.0695
  Pseudo loss: -4.79022
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (30/100) took 0.254 seconds.
  Mean final reward: -0.0416
  Mean return: -2.0661
  Mean final entropy: 0.0007
  Max final entropy: 0.0014
  Pseudo loss: 0.07976
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (35/100) took 0.253 seconds.
  Mean final reward: -0.1372
  Mean return: -2.6001
  Mean final entropy: 0.0007
  Max final entropy: 0.0023
  Pseudo loss: -2.21086
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.252 seconds.
  Mean final reward: -0.1025
  Mean return: -2.1045
  Mean final entropy: 0.0009
  Max final entropy: 0.0023
  Pseudo loss: 0.16344
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (45/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8616
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.19863
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (50/100) took 0.255 seconds.
  Mean final reward: -0.1748
  Mean return: -2.2677
  Mean final entropy: 0.0009
  Max final entropy: 0.0023
  Pseudo loss: -0.35820
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (55/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7897
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.23937
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.257 seconds.
  Mean final reward: -0.1628
  Mean return: -1.9669
  Mean final entropy: 0.0007
  Max final entropy: 0.0037
  Pseudo loss: -1.11351
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.258 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6987
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -0.07406
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.258 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.3289355 0.3289355 0.3289355 0.3289355 0.3289355 0.3289355 0.3289355
 0.3289355]
  Mean final entropy: 0.000086
  Max final entropy: 0.000086
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.3486309e-06 1.3486309e-06 1.3486309e-06 1.3486309e-06 1.3486309e-06
 1.3486309e-06 1.3486309e-06 1.3486309e-06]
  Max Min Entropy: 1.3486309171639732e-06
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [2, 2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 11
seed: 11645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.271 seconds.
  Mean final reward: -0.8459
  Mean return: -7.6014
  Mean final entropy: 0.0031
  Max final entropy: 0.0088
  Pseudo loss: -0.27049
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.1554
  Mean return: -6.8713
  Mean final entropy: 0.0067
  Max final entropy: 0.0234
  Pseudo loss: -0.04486
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.274 seconds.
  Mean final reward: -1.1515
  Mean return: -6.7125
  Mean final entropy: 0.0067
  Max final entropy: 0.0205
  Pseudo loss: -0.42256
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.264 seconds.
  Mean final reward: -0.8115
  Mean return: -5.9546
  Mean final entropy: 0.0032
  Max final entropy: 0.0127
  Pseudo loss: -0.61894
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (25/100) took 0.283 seconds.
  Mean final reward: -1.0848
  Mean return: -6.0794
  Mean final entropy: 0.0136
  Max final entropy: 0.0892
  Pseudo loss: -1.99295
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.268 seconds.
  Mean final reward: -0.1971
  Mean return: -5.0970
  Mean final entropy: 0.0010
  Max final entropy: 0.0040
  Pseudo loss: -0.03621
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.275 seconds.
  Mean final reward: -0.6986
  Mean return: -5.4938
  Mean final entropy: 0.0026
  Max final entropy: 0.0100
  Pseudo loss: -0.87755
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.267 seconds.
  Mean final reward: -0.2324
  Mean return: -4.9216
  Mean final entropy: 0.0009
  Max final entropy: 0.0064
  Pseudo loss: -0.89352
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (45/100) took 0.268 seconds.
  Mean final reward: -0.3830
  Mean return: -4.6196
  Mean final entropy: 0.0029
  Max final entropy: 0.0214
  Pseudo loss: -0.64342
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.274 seconds.
  Mean final reward: -0.8208
  Mean return: -5.5802
  Mean final entropy: 0.0058
  Max final entropy: 0.0373
  Pseudo loss: -2.97483
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (55/100) took 0.282 seconds.
  Mean final reward: -0.5278
  Mean return: -4.9595
  Mean final entropy: 0.0027
  Max final entropy: 0.0171
  Pseudo loss: -1.94099
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (60/100) took 0.269 seconds.
  Mean final reward: -0.0943
  Mean return: -4.1249
  Mean final entropy: 0.0003
  Max final entropy: 0.0021
  Pseudo loss: -2.09677
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9338
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.49343
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8173
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.56675
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.278 seconds.
  Mean final reward: -0.4893
  Mean return: -5.1324
  Mean final entropy: 0.0063
  Max final entropy: 0.0501
  Pseudo loss: -5.73679
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1759
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -1.55486
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.271 seconds.
  Mean final reward: -0.1354
  Mean return: -4.1452
  Mean final entropy: 0.0004
  Max final entropy: 0.0030
  Pseudo loss: -1.74951
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (90/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7858
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.10283
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6459
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.24302
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6459
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.20796
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.10031906 0.10031906 0.10031906 0.10031906 0.10031906 0.10031906
 0.10031906 0.10031906]
  Mean final entropy: 0.000009
  Max final entropy: 0.000009
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.788516e-07 4.788516e-07 4.788516e-07 4.788516e-07 4.788516e-07
 4.788516e-07 4.788516e-07 4.788516e-07]
  Max Min Entropy: 4.788515752807143e-07
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [5, 5, 5, 5, 5, 5, 5, 5], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 12
seed: 11619
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.256 seconds.
  Mean final reward: -1.2782
  Mean return: -7.8921
  Mean final entropy: 0.0469
  Max final entropy: 0.3365
  Pseudo loss: -2.33088
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.272 seconds.
  Mean final reward: -1.2631
  Mean return: -7.7514
  Mean final entropy: 0.0058
  Max final entropy: 0.0160
  Pseudo loss: -0.88825
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.262 seconds.
  Mean final reward: -0.9860
  Mean return: -7.4154
  Mean final entropy: 0.0044
  Max final entropy: 0.0214
  Pseudo loss: -1.59942
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.273 seconds.
  Mean final reward: -0.4785
  Mean return: -6.6075
  Mean final entropy: 0.0014
  Max final entropy: 0.0036
  Pseudo loss: -2.95539
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.274 seconds.
  Mean final reward: -0.7459
  Mean return: -6.4858
  Mean final entropy: 0.0027
  Max final entropy: 0.0122
  Pseudo loss: -2.10633
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.270 seconds.
  Mean final reward: -0.3878
  Mean return: -5.8162
  Mean final entropy: 0.0013
  Max final entropy: 0.0050
  Pseudo loss: -3.55308
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.272 seconds.
  Mean final reward: -0.4553
  Mean return: -5.1757
  Mean final entropy: 0.0013
  Max final entropy: 0.0036
  Pseudo loss: -0.90697
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.252 seconds.
  Mean final reward: -0.0857
  Mean return: -4.3525
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -0.13827
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (45/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.261 seconds.
  Mean final reward: -0.1502
  Mean return: -4.4886
  Mean final entropy: 0.0004
  Max final entropy: 0.0033
  Pseudo loss: -0.81999
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (55/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.251 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3872
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.89387
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.256 seconds.
  Mean final reward: -0.0429
  Mean return: -4.2696
  Mean final entropy: 0.0002
  Max final entropy: 0.0014
  Pseudo loss: -0.25975
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.253 seconds.
  Mean final reward: -0.0429
  Mean return: -4.2696
  Mean final entropy: 0.0002
  Max final entropy: 0.0014
  Pseudo loss: -0.28168
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.251 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.35815555 0.35815555 0.35815555 0.35815555 0.35815555 0.35815555
 0.35815555 0.35815555]
  Mean final entropy: 0.000017
  Max final entropy: 0.000017
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.2591973e-05 1.2591973e-05 1.2591973e-05 1.2591973e-05 1.2591973e-05
 1.2591973e-05 1.2591973e-05 1.2591973e-05]
  Max Min Entropy: 1.2591973245434929e-05
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 13
seed: 11625
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.268 seconds.
  Mean final reward: -2.2414
  Mean return: -9.6551
  Mean final entropy: 0.0237
  Max final entropy: 0.1147
  Pseudo loss: -0.33843
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.279 seconds.
  Mean final reward: -1.1830
  Mean return: -8.4263
  Mean final entropy: 0.0241
  Max final entropy: 0.1727
  Pseudo loss: -1.10979
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.274 seconds.
  Mean final reward: -2.3571
  Mean return: -10.8476
  Mean final entropy: 0.0266
  Max final entropy: 0.1317
  Pseudo loss: -0.61250
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.276 seconds.
  Mean final reward: -1.8192
  Mean return: -9.5853
  Mean final entropy: 0.0160
  Max final entropy: 0.0618
  Pseudo loss: -1.85985
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (25/100) took 0.267 seconds.
  Mean final reward: -1.2335
  Mean return: -8.1066
  Mean final entropy: 0.0183
  Max final entropy: 0.1169
  Pseudo loss: -0.63531
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.258 seconds.
  Mean final reward: -1.3204
  Mean return: -7.9603
  Mean final entropy: 0.0066
  Max final entropy: 0.0227
  Pseudo loss: -0.88590
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (35/100) took 0.268 seconds.
  Mean final reward: -1.6489
  Mean return: -9.3240
  Mean final entropy: 0.0182
  Max final entropy: 0.0902
  Pseudo loss: -2.52395
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.275 seconds.
  Mean final reward: -0.9694
  Mean return: -6.9446
  Mean final entropy: 0.0038
  Max final entropy: 0.0100
  Pseudo loss: -1.48423
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (45/100) took 0.291 seconds.
  Mean final reward: -0.4731
  Mean return: -6.5203
  Mean final entropy: 0.0022
  Max final entropy: 0.0116
  Pseudo loss: -2.81131
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.290 seconds.
  Mean final reward: -0.4703
  Mean return: -6.2145
  Mean final entropy: 0.0023
  Max final entropy: 0.0135
  Pseudo loss: -2.69655
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (55/100) took 0.286 seconds.
  Mean final reward: -1.3506
  Mean return: -7.7750
  Mean final entropy: 0.0119
  Max final entropy: 0.0584
  Pseudo loss: -8.20022
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0251
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.09705
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -5.1968
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: -0.54823
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0475
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.07076
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0699
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.291 seconds.
  Mean final reward: -0.3498
  Mean return: -6.0231
  Mean final entropy: 0.0023
  Max final entropy: 0.0164
  Pseudo loss: -6.03054
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0699
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0475
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.07645
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0699
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0699
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.25989905 0.25989905 0.25989905 0.25989905 0.25989905 0.25989905
 0.25989905 0.25989905]
  Mean final entropy: 0.000366
  Max final entropy: 0.000366
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [0.00243049 0.00243049 0.00243049 0.00243049 0.00243049 0.00243049
 0.00243049 0.00243049]
  Max Min Entropy: 0.002430494176223874
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 14
seed: 11669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.272 seconds.
  Mean final reward: -0.6420
  Mean return: -4.5127
  Mean final entropy: 0.0140
  Max final entropy: 0.1091
  Pseudo loss: -2.03501
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (10/100) took 0.261 seconds.
  Mean final reward: -0.3339
  Mean return: -2.0063
  Mean final entropy: 0.0012
  Max final entropy: 0.0034
  Pseudo loss: -0.99016
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (15/100) took 0.267 seconds.
  Mean final reward: -0.1193
  Mean return: -1.4231
  Mean final entropy: 0.0006
  Max final entropy: 0.0026
  Pseudo loss: -2.01624
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.265 seconds.
  Mean final reward: -0.0406
  Mean return: -0.3917
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.08086
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (25/100) took 0.260 seconds.
  Mean final reward: -0.0391
  Mean return: -0.3579
  Mean final entropy: 0.0005
  Max final entropy: 0.0013
  Pseudo loss: 0.00578
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.258 seconds.
  Mean final reward: -0.0849
  Mean return: -0.4380
  Mean final entropy: 0.0006
  Max final entropy: 0.0013
  Pseudo loss: -0.06564
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2899
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.01436
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (40/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -0.3373
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.07177
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (45/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2639
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.01557
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2730
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.03853
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.265 seconds.
  Mean final reward: -0.0268
  Mean return: -0.2998
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -0.09097
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (65/100) took 0.265 seconds.
  Mean final reward: -0.0158
  Mean return: -0.2758
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.03841
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2667
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.08646
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2509
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.03141
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2509
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.03360
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2667
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.09086
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (95/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.25187474 0.25187474 0.25187474 0.25187474 0.25187474 0.25187474
 0.25187474 0.25187474]
  Mean final entropy: 0.000015
  Max final entropy: 0.000015
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.9582785e-08 5.9582785e-08 5.9582785e-08 5.9582785e-08 5.9582785e-08
 5.9582785e-08 5.9582785e-08 5.9582785e-08]
  Max Min Entropy: 5.9582784928124966e-08
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [3, 3, 3, 3, 3, 3, 3, 3], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 15
seed: 11757
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.270 seconds.
  Mean final reward: -2.1127
  Mean return: -10.3122
  Mean final entropy: 0.0630
  Max final entropy: 0.3179
  Pseudo loss: -1.82580
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.262 seconds.
  Mean final reward: -1.0500
  Mean return: -7.0767
  Mean final entropy: 0.0103
  Max final entropy: 0.0686
  Pseudo loss: -1.06615
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.269 seconds.
  Mean final reward: -1.0213
  Mean return: -6.1337
  Mean final entropy: 0.0156
  Max final entropy: 0.1116
  Pseudo loss: -2.96471
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.276 seconds.
  Mean final reward: -1.1691
  Mean return: -7.7025
  Mean final entropy: 0.0202
  Max final entropy: 0.1411
  Pseudo loss: -3.10262
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.272 seconds.
  Mean final reward: -0.6812
  Mean return: -6.4637
  Mean final entropy: 0.0037
  Max final entropy: 0.0211
  Pseudo loss: -2.99706
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.278 seconds.
  Mean final reward: -0.4487
  Mean return: -4.6283
  Mean final entropy: 0.0022
  Max final entropy: 0.0120
  Pseudo loss: -2.15517
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.280 seconds.
  Mean final reward: -0.2839
  Mean return: -3.5777
  Mean final entropy: 0.0011
  Max final entropy: 0.0032
  Pseudo loss: -0.24836
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.0562
  Mean return: -3.1858
  Mean final entropy: 0.0005
  Max final entropy: 0.0014
  Pseudo loss: 0.03748
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (45/100) took 0.280 seconds.
  Mean final reward: -0.1991
  Mean return: -3.2919
  Mean final entropy: 0.0008
  Max final entropy: 0.0016
  Pseudo loss: -0.13290
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.274 seconds.
  Mean final reward: -0.1004
  Mean return: -3.2228
  Mean final entropy: 0.0008
  Max final entropy: 0.0014
  Pseudo loss: 0.25960
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (55/100) took 0.276 seconds.
  Mean final reward: -0.1560
  Mean return: -3.8158
  Mean final entropy: 0.0007
  Max final entropy: 0.0032
  Pseudo loss: -2.23025
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1418
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.269 seconds.
  Mean final reward: -0.0460
  Mean return: -3.1756
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -0.00522
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (70/100) took 0.284 seconds.
  Mean final reward: -0.0102
  Mean return: -3.1521
  Mean final entropy: 0.0005
  Max final entropy: 0.0011
  Pseudo loss: -0.02044
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (75/100) took 0.280 seconds.
  Mean final reward: -0.0460
  Mean return: -3.1756
  Mean final entropy: 0.0002
  Max final entropy: 0.0014
  Pseudo loss: -0.04829
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (80/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1418
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.272 seconds.
  Mean final reward: -0.1125
  Mean return: -3.2298
  Mean final entropy: 0.0007
  Max final entropy: 0.0014
  Pseudo loss: -0.04110
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1418
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.280 seconds.
  Mean final reward: -0.0920
  Mean return: -3.2094
  Mean final entropy: 0.0005
  Max final entropy: 0.0014
  Pseudo loss: -0.10273
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.303 seconds.
  Mean final reward: -0.0562
  Mean return: -3.1858
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -0.07484
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.41695437 0.41695437 0.41695437 0.41695437 0.41695437 0.41695437
 0.41695437 0.41695437]
  Mean final entropy: 0.000071
  Max final entropy: 0.000071
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.7722635e-07 2.7722635e-07 2.7722635e-07 2.7722635e-07 2.7722635e-07
 2.7722635e-07 2.7722635e-07 2.7722635e-07]
  Max Min Entropy: 2.7722634854399075e-07
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 16
seed: 11895
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.282 seconds.
  Mean final reward: -1.2417
  Mean return: -6.9747
  Mean final entropy: 0.0065
  Max final entropy: 0.0222
  Pseudo loss: -0.72894
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.393 seconds.
  Mean final reward: -1.2736
  Mean return: -6.0000
  Mean final entropy: 0.0070
  Max final entropy: 0.0179
  Pseudo loss: -2.51082
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (15/100) took 0.320 seconds.
  Mean final reward: -0.7175
  Mean return: -5.1278
  Mean final entropy: 0.0035
  Max final entropy: 0.0186
  Pseudo loss: -3.42657
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.290 seconds.
  Mean final reward: -0.1806
  Mean return: -3.9915
  Mean final entropy: 0.0009
  Max final entropy: 0.0027
  Pseudo loss: -1.66935
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.458 seconds.
  Mean final reward: -0.3712
  Mean return: -4.1117
  Mean final entropy: 0.0021
  Max final entropy: 0.0128
  Pseudo loss: -6.32170
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.302 seconds.
  Mean final reward: -0.5542
  Mean return: -4.0323
  Mean final entropy: 0.0035
  Max final entropy: 0.0200
  Pseudo loss: -3.51789
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (35/100) took 0.308 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6789
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.33727
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (40/100) took 0.340 seconds.
  Mean final reward: -0.1676
  Mean return: -2.4657
  Mean final entropy: 0.0007
  Max final entropy: 0.0025
  Pseudo loss: 0.02890
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.344 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5471
  Mean final entropy: 0.0004
  Max final entropy: 0.0007
  Pseudo loss: -0.17701
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (50/100) took 0.528 seconds.
  Mean final reward: -0.1634
  Mean return: -2.0016
  Mean final entropy: 0.0008
  Max final entropy: 0.0037
  Pseudo loss: -0.87542
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (55/100) took 0.509 seconds.
  Mean final reward: -0.0960
  Mean return: -2.2510
  Mean final entropy: 0.0007
  Max final entropy: 0.0022
  Pseudo loss: -2.31911
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.511 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8509
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: -0.42508
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.324 seconds.
  Mean final reward: -0.0529
  Mean return: -1.8630
  Mean final entropy: 0.0006
  Max final entropy: 0.0015
  Pseudo loss: -0.70632
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.333 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8509
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: -0.72442
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.291 seconds.
  Mean final reward: -0.0529
  Mean return: -2.0393
  Mean final entropy: 0.0005
  Max final entropy: 0.0015
  Pseudo loss: -1.33267
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0004
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.17790876 0.17790876 0.17790876 0.17790876 0.17790876 0.17790876
 0.17790876 0.17790876]
  Mean final entropy: 0.000156
  Max final entropy: 0.000156
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.1524484e-05 1.1524484e-05 1.1524484e-05 1.1524484e-05 1.1524484e-05
 1.1524484e-05 1.1524484e-05 1.1524484e-05]
  Max Min Entropy: 1.1524483852554113e-05
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 17
seed: 12089
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.276 seconds.
  Mean final reward: -1.8961
  Mean return: -8.7361
  Mean final entropy: 0.0107
  Max final entropy: 0.0275
  Pseudo loss: -0.95314
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.281 seconds.
  Mean final reward: -1.3725
  Mean return: -7.5829
  Mean final entropy: 0.0094
  Max final entropy: 0.0493
  Pseudo loss: -0.39950
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.277 seconds.
  Mean final reward: -1.1892
  Mean return: -7.5512
  Mean final entropy: 0.0165
  Max final entropy: 0.0782
  Pseudo loss: -4.05665
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.270 seconds.
  Mean final reward: -0.9745
  Mean return: -5.9131
  Mean final entropy: 0.0124
  Max final entropy: 0.0524
  Pseudo loss: -7.11114
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.269 seconds.
  Mean final reward: -0.5257
  Mean return: -5.3904
  Mean final entropy: 0.0021
  Max final entropy: 0.0085
  Pseudo loss: -4.59097
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.267 seconds.
  Mean final reward: -0.4401
  Mean return: -4.1511
  Mean final entropy: 0.0016
  Max final entropy: 0.0085
  Pseudo loss: -3.07637
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8047
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.32121
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.341 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5180683 0.5180683 0.5180683 0.5180683 0.5180683 0.5180683 0.5180683
 0.5180683]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.973639e-06 1.973639e-06 1.973639e-06 1.973639e-06 1.973639e-06
 1.973639e-06 1.973639e-06 1.973639e-06]
  Max Min Entropy: 1.9736389731406234e-06
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [3, 3, 3, 3, 3, 3, 3, 3], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 18
seed: 12345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.259 seconds.
  Mean final reward: -1.5411
  Mean return: -7.2416
  Mean final entropy: 0.0399
  Max final entropy: 0.2534
  Pseudo loss: -0.88697
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.278 seconds.
  Mean final reward: -1.7589
  Mean return: -6.8265
  Mean final entropy: 0.0192
  Max final entropy: 0.0929
  Pseudo loss: -3.93102
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.291 seconds.
  Mean final reward: -0.3995
  Mean return: -1.4275
  Mean final entropy: 0.0021
  Max final entropy: 0.0125
  Pseudo loss: -0.45305
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.6965
  Mean return: -2.9529
  Mean final entropy: 0.0046
  Max final entropy: 0.0250
  Pseudo loss: -4.40165
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.298 seconds.
  Mean final reward: -0.0501
  Mean return: -0.4375
  Mean final entropy: 0.0005
  Max final entropy: 0.0011
  Pseudo loss: 0.33675
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.296 seconds.
  Mean final reward: -0.0504
  Mean return: -0.6275
  Mean final entropy: 0.0006
  Max final entropy: 0.0013
  Pseudo loss: 0.17890
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (35/100) took 0.297 seconds.
  Mean final reward: -0.1285
  Mean return: -0.6625
  Mean final entropy: 0.0009
  Max final entropy: 0.0013
  Pseudo loss: 0.18052
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (40/100) took 0.309 seconds.
  Mean final reward: -0.0504
  Mean return: -0.2037
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: 0.25044
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -0.4601
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.11532
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.297 seconds.
  Mean final reward: -0.0337
  Mean return: -0.3404
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.17499
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.750
Episode (55/100) took 0.295 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1871
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.08411
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.250
Episode (60/100) took 0.299 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.04683
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0767
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.04393
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (70/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0767
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.06260
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (75/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.296 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.18152
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.308 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.38503304 0.38503304 0.38503304 0.38503304 0.38503304 0.38503304
 0.38503304 0.38503304]
  Mean final entropy: 0.000004
  Max final entropy: 0.000004
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07
 4.3489302e-07 4.3489302e-07 4.3489302e-07]
  Max Min Entropy: 4.3489302470334223e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 19
seed: 12669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.275 seconds.
  Mean final reward: -1.2761
  Mean return: -6.9621
  Mean final entropy: 0.0205
  Max final entropy: 0.1400
  Pseudo loss: -1.16352
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.256 seconds.
  Mean final reward: -1.3777
  Mean return: -5.8233
  Mean final entropy: 0.0085
  Max final entropy: 0.0226
  Pseudo loss: -1.20804
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.258 seconds.
  Mean final reward: -0.7117
  Mean return: -3.7152
  Mean final entropy: 0.0053
  Max final entropy: 0.0318
  Pseudo loss: -2.84765
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.272 seconds.
  Mean final reward: -1.2459
  Mean return: -5.6142
  Mean final entropy: 0.0083
  Max final entropy: 0.0409
  Pseudo loss: -5.35621
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.264 seconds.
  Mean final reward: -1.1116
  Mean return: -4.7429
  Mean final entropy: 0.0086
  Max final entropy: 0.0403
  Pseudo loss: -7.71299
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.125
Episode (30/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.251 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.31995574 0.31995574 0.31995574 0.31995574 0.31995574 0.31995574
 0.31995574 0.31995574]
  Mean final entropy: 0.000493
  Max final entropy: 0.000493
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.629032e-07 5.629032e-07 5.629032e-07 5.629032e-07 5.629032e-07
 5.629032e-07 5.629032e-07 5.629032e-07]
  Max Min Entropy: 5.629032102660858e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [2, 2, 2, 2, 2, 2, 2, 2], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 20
seed: 13067
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.257 seconds.
  Mean final reward: -1.1873
  Mean return: -9.3619
  Mean final entropy: 0.0047
  Max final entropy: 0.0120
  Pseudo loss: -0.25106
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.276 seconds.
  Mean final reward: -2.0394
  Mean return: -10.1767
  Mean final entropy: 0.0381
  Max final entropy: 0.2317
  Pseudo loss: -1.77163
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.281 seconds.
  Mean final reward: -2.1929
  Mean return: -10.4962
  Mean final entropy: 0.0280
  Max final entropy: 0.1348
  Pseudo loss: -4.68395
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.265 seconds.
  Mean final reward: -0.8067
  Mean return: -7.8155
  Mean final entropy: 0.0032
  Max final entropy: 0.0114
  Pseudo loss: -2.75503
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (25/100) took 0.274 seconds.
  Mean final reward: -1.1065
  Mean return: -7.5467
  Mean final entropy: 0.0094
  Max final entropy: 0.0452
  Pseudo loss: -3.55398
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.277 seconds.
  Mean final reward: -0.3224
  Mean return: -5.4061
  Mean final entropy: 0.0011
  Max final entropy: 0.0041
  Pseudo loss: -0.14165
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.267 seconds.
  Mean final reward: -0.2620
  Mean return: -5.6207
  Mean final entropy: 0.0008
  Max final entropy: 0.0041
  Pseudo loss: -0.81615
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.261 seconds.
  Mean final reward: -0.0545
  Mean return: -5.5470
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: -0.25781
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (45/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9842
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.07406
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (55/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -4.7371
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.12252
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (60/100) took 0.270 seconds.
  Mean final reward: -0.2794
  Mean return: -5.4282
  Mean final entropy: 0.0010
  Max final entropy: 0.0041
  Pseudo loss: -1.10084
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (65/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9644
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.13194
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (70/100) took 0.270 seconds.
  Mean final reward: -0.4872
  Mean return: -5.8424
  Mean final entropy: 0.0062
  Max final entropy: 0.0493
  Pseudo loss: -4.38029
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.270 seconds.
  Mean final reward: -0.0844
  Mean return: -5.2174
  Mean final entropy: 0.0003
  Max final entropy: 0.0020
  Pseudo loss: -1.38013
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9105
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.11703
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (85/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8108
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.04226
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9042
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.05065
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (95/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8108
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.02567
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -4.7515
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.09932
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.49351934 0.49351934 0.49351934 0.49351934 0.49351934 0.49351934
 0.49351934 0.49351934]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.4960123e-07 4.4960123e-07 4.4960123e-07 4.4960123e-07 4.4960123e-07
 4.4960123e-07 4.4960123e-07 4.4960123e-07]
  Max Min Entropy: 4.496012309118669e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 21
seed: 13545
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.262 seconds.
  Mean final reward: -0.4172
  Mean return: -6.8010
  Mean final entropy: 0.0015
  Max final entropy: 0.0057
  Pseudo loss: -0.72701
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.291 seconds.
  Mean final reward: -0.7209
  Mean return: -6.5617
  Mean final entropy: 0.0073
  Max final entropy: 0.0512
  Pseudo loss: -2.31506
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (15/100) took 0.292 seconds.
  Mean final reward: -0.8208
  Mean return: -7.9584
  Mean final entropy: 0.0069
  Max final entropy: 0.0446
  Pseudo loss: -0.93349
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.285 seconds.
  Mean final reward: -0.8602
  Mean return: -6.3221
  Mean final entropy: 0.0040
  Max final entropy: 0.0148
  Pseudo loss: -1.55962
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.292 seconds.
  Mean final reward: -0.3706
  Mean return: -4.6204
  Mean final entropy: 0.0020
  Max final entropy: 0.0109
  Pseudo loss: -1.19010
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.6035
  Mean return: -4.7124
  Mean final entropy: 0.0020
  Max final entropy: 0.0079
  Pseudo loss: -0.17948
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.288 seconds.
  Mean final reward: -0.9042
  Mean return: -5.0083
  Mean final entropy: 0.0071
  Max final entropy: 0.0456
  Pseudo loss: -4.22087
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.292 seconds.
  Mean final reward: -0.2780
  Mean return: -3.4874
  Mean final entropy: 0.0009
  Max final entropy: 0.0031
  Pseudo loss: 0.21567
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (45/100) took 0.289 seconds.
  Mean final reward: -0.0613
  Mean return: -4.2898
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -6.07584
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.294 seconds.
  Mean final reward: -0.2883
  Mean return: -3.5752
  Mean final entropy: 0.0012
  Max final entropy: 0.0061
  Pseudo loss: -1.03765
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.289 seconds.
  Mean final reward: -0.2864
  Mean return: -3.6875
  Mean final entropy: 0.0012
  Max final entropy: 0.0071
  Pseudo loss: -1.59203
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.287 seconds.
  Mean final reward: -0.0712
  Mean return: -2.7305
  Mean final entropy: 0.0002
  Max final entropy: 0.0018
  Pseudo loss: -0.22476
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.284 seconds.
  Mean final reward: -0.1720
  Mean return: -3.9724
  Mean final entropy: 0.0006
  Max final entropy: 0.0040
  Pseudo loss: -6.44245
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.285 seconds.
  Mean final reward: -0.0613
  Mean return: -2.6459
  Mean final entropy: 0.0002
  Max final entropy: 0.0016
  Pseudo loss: -0.36333
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (75/100) took 0.284 seconds.
  Mean final reward: -0.1423
  Mean return: -2.8355
  Mean final entropy: 0.0004
  Max final entropy: 0.0031
  Pseudo loss: -1.80694
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5846
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.30617
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (95/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.6136633 0.6136633 0.6136633 0.6136633 0.6136633 0.6136633 0.6136633
 0.6136633]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.249249e-07 4.249249e-07 4.249249e-07 4.249249e-07 4.249249e-07
 4.249249e-07 4.249249e-07 4.249249e-07]
  Max Min Entropy: 4.249249059284921e-07
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 22
seed: 14109
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.276 seconds.
  Mean final reward: -1.4274
  Mean return: -7.7285
  Mean final entropy: 0.0309
  Max final entropy: 0.2124
  Pseudo loss: -0.18908
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.274 seconds.
  Mean final reward: -0.6393
  Mean return: -6.6609
  Mean final entropy: 0.0027
  Max final entropy: 0.0093
  Pseudo loss: -1.08001
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.288 seconds.
  Mean final reward: -0.3076
  Mean return: -2.6333
  Mean final entropy: 0.0011
  Max final entropy: 0.0039
  Pseudo loss: -1.16663
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.286 seconds.
  Mean final reward: -0.2864
  Mean return: -2.2392
  Mean final entropy: 0.0012
  Max final entropy: 0.0024
  Pseudo loss: -0.98810
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (25/100) took 0.287 seconds.
  Mean final reward: -0.1456
  Mean return: -1.5036
  Mean final entropy: 0.0007
  Max final entropy: 0.0029
  Pseudo loss: -0.99255
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.285 seconds.
  Mean final reward: -0.4541
  Mean return: -1.9609
  Mean final entropy: 0.0015
  Max final entropy: 0.0024
  Pseudo loss: 1.11427
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (35/100) took 0.288 seconds.
  Mean final reward: -0.2953
  Mean return: -1.5369
  Mean final entropy: 0.0012
  Max final entropy: 0.0023
  Pseudo loss: 0.51808
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.295 seconds.
  Mean final reward: -0.7295
  Mean return: -2.3904
  Mean final entropy: 0.0021
  Max final entropy: 0.0024
  Pseudo loss: 0.45896
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (45/100) took 0.290 seconds.
  Mean final reward: -0.4415
  Mean return: -1.7207
  Mean final entropy: 0.0013
  Max final entropy: 0.0024
  Pseudo loss: 1.93532
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.250
Episode (50/100) took 0.283 seconds.
  Mean final reward: -0.3001
  Mean return: -1.7164
  Mean final entropy: 0.0011
  Max final entropy: 0.0024
  Pseudo loss: 0.86160
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.288 seconds.
  Mean final reward: -0.2012
  Mean return: -1.4031
  Mean final entropy: 0.0009
  Max final entropy: 0.0024
  Pseudo loss: 0.36283
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.284 seconds.
  Mean final reward: -0.0669
  Mean return: -1.6344
  Mean final entropy: 0.0007
  Max final entropy: 0.0017
  Pseudo loss: -1.02336
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.282 seconds.
  Mean final reward: -0.0682
  Mean return: -1.4666
  Mean final entropy: 0.0005
  Max final entropy: 0.0017
  Pseudo loss: -3.32920
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8055
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.12452
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (75/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2497
  Mean final entropy: 0.0005
  Max final entropy: 0.0010
  Pseudo loss: -1.05708
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -0.6797
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: 0.15065
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (85/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.6101
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.21898
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0374
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: -5.94418
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.6275
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.30872
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -0.6711
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.22074
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.511224 0.511224 0.511224 0.511224 0.511224 0.511224 0.511224 0.511224]
  Mean final entropy: 0.000981
  Max final entropy: 0.000981
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.881851e-08 9.881851e-08 9.881851e-08 9.881851e-08 9.881851e-08
 9.881851e-08 9.881851e-08 9.881851e-08]
  Max Min Entropy: 9.881851070758785e-08
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 23
seed: 14765
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.269 seconds.
  Mean final reward: -0.6615
  Mean return: -5.7782
  Mean final entropy: 0.0024
  Max final entropy: 0.0075
  Pseudo loss: 0.06433
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.277 seconds.
  Mean final reward: -1.0991
  Mean return: -6.4252
  Mean final entropy: 0.0086
  Max final entropy: 0.0526
  Pseudo loss: -1.08491
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (15/100) took 0.270 seconds.
  Mean final reward: -0.3097
  Mean return: -4.7293
  Mean final entropy: 0.0011
  Max final entropy: 0.0023
  Pseudo loss: -0.18675
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.271 seconds.
  Mean final reward: -1.1277
  Mean return: -7.4394
  Mean final entropy: 0.0082
  Max final entropy: 0.0315
  Pseudo loss: -1.70509
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.270 seconds.
  Mean final reward: -0.7577
  Mean return: -5.1650
  Mean final entropy: 0.0061
  Max final entropy: 0.0360
  Pseudo loss: -1.27547
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.273 seconds.
  Mean final reward: -0.7439
  Mean return: -4.3663
  Mean final entropy: 0.0026
  Max final entropy: 0.0075
  Pseudo loss: -1.00455
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.269 seconds.
  Mean final reward: -0.4220
  Mean return: -4.4608
  Mean final entropy: 0.0038
  Max final entropy: 0.0293
  Pseudo loss: -3.62184
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (40/100) took 0.271 seconds.
  Mean final reward: -0.0487
  Mean return: -2.6395
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: 0.30219
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (45/100) took 0.268 seconds.
  Mean final reward: -0.0487
  Mean return: -2.6281
  Mean final entropy: 0.0004
  Max final entropy: 0.0015
  Pseudo loss: 0.13119
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5676
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -0.17657
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (55/100) took 0.273 seconds.
  Mean final reward: -0.2024
  Mean return: -2.9494
  Mean final entropy: 0.0007
  Max final entropy: 0.0023
  Pseudo loss: -1.24381
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.265 seconds.
  Mean final reward: -0.1248
  Mean return: -2.8757
  Mean final entropy: 0.0006
  Max final entropy: 0.0027
  Pseudo loss: -0.89101
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (65/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1789
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.04159
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.276 seconds.
  Mean final reward: -0.0264
  Mean return: -2.8077
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -1.80112
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.265 seconds.
  Mean final reward: -0.0875
  Mean return: -2.5267
  Mean final entropy: 0.0007
  Max final entropy: 0.0015
  Pseudo loss: -0.11452
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2309
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -0.04631
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (85/100) took 0.281 seconds.
  Mean final reward: -0.0509
  Mean return: -2.2298
  Mean final entropy: 0.0002
  Max final entropy: 0.0015
  Pseudo loss: -0.22732
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1789
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.10970
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (95/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.13400906 0.13400906 0.13400906 0.13400906 0.13400906 0.13400906
 0.13400906 0.13400906]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.129566e-07 3.129566e-07 3.129566e-07 3.129566e-07 3.129566e-07
 3.129566e-07 3.129566e-07 3.129566e-07]
  Max Min Entropy: 3.1295658686758543e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 24
seed: 15519
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.261 seconds.
  Mean final reward: -1.9555
  Mean return: -10.8946
  Mean final entropy: 0.0173
  Max final entropy: 0.0583
  Pseudo loss: 0.70704
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.276 seconds.
  Mean final reward: -1.6596
  Mean return: -8.8093
  Mean final entropy: 0.0126
  Max final entropy: 0.0470
  Pseudo loss: -0.39436
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.275 seconds.
  Mean final reward: -1.5555
  Mean return: -8.0447
  Mean final entropy: 0.0212
  Max final entropy: 0.0949
  Pseudo loss: -1.82583
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.270 seconds.
  Mean final reward: -1.7941
  Mean return: -8.8274
  Mean final entropy: 0.0463
  Max final entropy: 0.3087
  Pseudo loss: -1.49808
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (25/100) took 0.291 seconds.
  Mean final reward: -0.7563
  Mean return: -7.3263
  Mean final entropy: 0.0046
  Max final entropy: 0.0186
  Pseudo loss: 0.09961
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.285 seconds.
  Mean final reward: -1.6544
  Mean return: -9.2115
  Mean final entropy: 0.0163
  Max final entropy: 0.0898
  Pseudo loss: -0.96135
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (35/100) took 0.269 seconds.
  Mean final reward: -1.3243
  Mean return: -8.1858
  Mean final entropy: 0.0100
  Max final entropy: 0.0331
  Pseudo loss: -1.14303
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.274 seconds.
  Mean final reward: -0.5296
  Mean return: -6.3058
  Mean final entropy: 0.0025
  Max final entropy: 0.0119
  Pseudo loss: -2.88632
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (45/100) took 0.274 seconds.
  Mean final reward: -0.8484
  Mean return: -7.6965
  Mean final entropy: 0.0056
  Max final entropy: 0.0257
  Pseudo loss: -2.71564
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.294 seconds.
  Mean final reward: -0.5874
  Mean return: -6.6764
  Mean final entropy: 0.0060
  Max final entropy: 0.0442
  Pseudo loss: -5.63119
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (55/100) took 0.297 seconds.
  Mean final reward: -0.3628
  Mean return: -4.3981
  Mean final entropy: 0.0015
  Max final entropy: 0.0079
  Pseudo loss: -2.85846
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (60/100) took 0.303 seconds.
  Mean final reward: -0.3756
  Mean return: -3.9999
  Mean final entropy: 0.0027
  Max final entropy: 0.0202
  Pseudo loss: -3.26287
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (65/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.292 seconds.
  Mean final reward: -0.3428
  Mean return: -3.9535
  Mean final entropy: 0.0022
  Max final entropy: 0.0155
  Pseudo loss: -4.22664
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (90/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.4300679 0.4300679 0.4300679 0.4300679 0.4300679 0.4300679 0.4300679
 0.4300679]
  Mean final entropy: 0.000345
  Max final entropy: 0.000345
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0489292e-06 2.0489292e-06 2.0489292e-06 2.0489292e-06 2.0489292e-06
 2.0489292e-06 2.0489292e-06 2.0489292e-06]
  Max Min Entropy: 2.048929218290141e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [4, 4, 4, 4, 4, 4, 4, 4], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 25
seed: 16377
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.288 seconds.
  Mean final reward: -1.3829
  Mean return: -7.1376
  Mean final entropy: 0.0197
  Max final entropy: 0.0861
  Pseudo loss: -1.13727
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.283 seconds.
  Mean final reward: -1.4030
  Mean return: -7.4070
  Mean final entropy: 0.0207
  Max final entropy: 0.0984
  Pseudo loss: -2.45210
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (15/100) took 0.288 seconds.
  Mean final reward: -0.9581
  Mean return: -4.8060
  Mean final entropy: 0.0091
  Max final entropy: 0.0625
  Pseudo loss: -1.72617
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.5670
  Mean return: -4.5324
  Mean final entropy: 0.0018
  Max final entropy: 0.0042
  Pseudo loss: -1.17365
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.290 seconds.
  Mean final reward: -0.0942
  Mean return: -2.7708
  Mean final entropy: 0.0006
  Max final entropy: 0.0021
  Pseudo loss: -0.16871
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (30/100) took 0.285 seconds.
  Mean final reward: -0.4116
  Mean return: -2.9394
  Mean final entropy: 0.0038
  Max final entropy: 0.0269
  Pseudo loss: -4.78317
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.297 seconds.
  Mean final reward: -0.1554
  Mean return: -2.1557
  Mean final entropy: 0.0006
  Max final entropy: 0.0035
  Pseudo loss: -0.90551
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.295 seconds.
  Mean final reward: -0.0487
  Mean return: -1.8261
  Mean final entropy: 0.0002
  Max final entropy: 0.0015
  Pseudo loss: -0.45632
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (45/100) took 0.296 seconds.
  Mean final reward: -0.1906
  Mean return: -2.0485
  Mean final entropy: 0.0010
  Max final entropy: 0.0046
  Pseudo loss: -1.13998
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.297 seconds.
  Mean final reward: -0.5517
  Mean return: -3.3150
  Mean final entropy: 0.0056
  Max final entropy: 0.0418
  Pseudo loss: -9.13110
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8272
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: -0.54804
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8312
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -0.85163
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.18161497 0.18161497 0.18161497 0.18161497 0.18161497 0.18161497
 0.18161497 0.18161497]
  Mean final entropy: 0.000861
  Max final entropy: 0.000861
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.4483534e-07 4.4483534e-07 4.4483534e-07 4.4483534e-07 4.4483534e-07
 4.4483534e-07 4.4483534e-07 4.4483534e-07]
  Max Min Entropy: 4.448353365660296e-07
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [7, 7, 7, 7, 7, 7, 7, 7], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 26
seed: 17345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.269 seconds.
  Mean final reward: -2.3172
  Mean return: -11.6862
  Mean final entropy: 0.0236
  Max final entropy: 0.0764
  Pseudo loss: 0.01216
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.258 seconds.
  Mean final reward: -1.5102
  Mean return: -9.1821
  Mean final entropy: 0.0060
  Max final entropy: 0.0168
  Pseudo loss: -0.05672
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.270 seconds.
  Mean final reward: -2.1905
  Mean return: -9.4922
  Mean final entropy: 0.0161
  Max final entropy: 0.0340
  Pseudo loss: 0.46880
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.257 seconds.
  Mean final reward: -1.7516
  Mean return: -9.9486
  Mean final entropy: 0.0165
  Max final entropy: 0.0562
  Pseudo loss: -5.26868
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.250 seconds.
  Mean final reward: -0.6568
  Mean return: -7.7682
  Mean final entropy: 0.0019
  Max final entropy: 0.0049
  Pseudo loss: 1.14373
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.241 seconds.
  Mean final reward: -1.2479
  Mean return: -8.8549
  Mean final entropy: 0.0065
  Max final entropy: 0.0240
  Pseudo loss: -2.54679
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.245 seconds.
  Mean final reward: -0.9628
  Mean return: -8.3293
  Mean final entropy: 0.0055
  Max final entropy: 0.0284
  Pseudo loss: -2.34905
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.252 seconds.
  Mean final reward: -0.9214
  Mean return: -7.7028
  Mean final entropy: 0.0053
  Max final entropy: 0.0284
  Pseudo loss: -1.07641
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.260 seconds.
  Mean final reward: -0.4315
  Mean return: -5.9337
  Mean final entropy: 0.0018
  Max final entropy: 0.0064
  Pseudo loss: 0.84870
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.276 seconds.
  Mean final reward: -0.2975
  Mean return: -4.6463
  Mean final entropy: 0.0020
  Max final entropy: 0.0108
  Pseudo loss: -3.21510
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (55/100) took 0.285 seconds.
  Mean final reward: -0.0719
  Mean return: -3.5724
  Mean final entropy: 0.0009
  Max final entropy: 0.0018
  Pseudo loss: -0.91988
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (60/100) took 0.281 seconds.
  Mean final reward: -0.0972
  Mean return: -3.5661
  Mean final entropy: 0.0010
  Max final entropy: 0.0018
  Pseudo loss: -0.99296
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (65/100) took 0.285 seconds.
  Mean final reward: -0.0719
  Mean return: -3.3565
  Mean final entropy: 0.0009
  Max final entropy: 0.0018
  Pseudo loss: -0.31634
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (70/100) took 0.285 seconds.
  Mean final reward: -0.4184
  Mean return: -4.2536
  Mean final entropy: 0.0043
  Max final entropy: 0.0284
  Pseudo loss: -6.12585
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (75/100) took 0.320 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1956
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.3988
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: -0.87599
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1956
  Mean final entropy: 0.0009
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1956
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1956
  Mean final entropy: 0.0008
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.281 seconds.
  Mean final reward: -0.0719
  Mean return: -3.3565
  Mean final entropy: 0.0010
  Max final entropy: 0.0018
  Pseudo loss: -0.57724
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Testing with greedy policy:
  Start entropy: [0.40393737 0.40393737 0.40393737 0.40393737 0.40393737 0.40393737
 0.40393737 0.40393737]
  Mean final entropy: 0.000860
  Max final entropy: 0.000860
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.1454004e-05 5.1454004e-05 5.1454004e-05 5.1454004e-05 5.1454004e-05
 5.1454004e-05 5.1454004e-05 5.1454004e-05]
  Max Min Entropy: 5.145400427863933e-05
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 27
seed: 18429
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.280 seconds.
  Mean final reward: -1.6162
  Mean return: -7.3045
  Mean final entropy: 0.0140
  Max final entropy: 0.0379
  Pseudo loss: -0.03394
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.261 seconds.
  Mean final reward: -1.4443
  Mean return: -8.9282
  Mean final entropy: 0.0118
  Max final entropy: 0.0541
  Pseudo loss: -1.27678
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.277 seconds.
  Mean final reward: -0.3195
  Mean return: -3.5975
  Mean final entropy: 0.0010
  Max final entropy: 0.0033
  Pseudo loss: -1.32356
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.280 seconds.
  Mean final reward: -0.5726
  Mean return: -3.0742
  Mean final entropy: 0.0100
  Max final entropy: 0.0760
  Pseudo loss: -4.88988
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2782
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -6.44498
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (60/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.5799166 0.5799166 0.5799166 0.5799166 0.5799166 0.5799166 0.5799166
 0.5799166]
  Mean final entropy: 0.000879
  Max final entropy: 0.000879
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.9914105e-07 2.9914105e-07 2.9914105e-07 2.9914105e-07 2.9914105e-07
 2.9914105e-07 2.9914105e-07 2.9914105e-07]
  Max Min Entropy: 2.991410497088509e-07
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 28
seed: 19635
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.266 seconds.
  Mean final reward: -1.9415
  Mean return: -10.1020
  Mean final entropy: 0.0535
  Max final entropy: 0.2746
  Pseudo loss: -0.79022
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.268 seconds.
  Mean final reward: -1.2248
  Mean return: -8.1193
  Mean final entropy: 0.0060
  Max final entropy: 0.0232
  Pseudo loss: -3.56193
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.286 seconds.
  Mean final reward: -1.8252
  Mean return: -8.7048
  Mean final entropy: 0.0160
  Max final entropy: 0.0599
  Pseudo loss: -3.69486
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.284 seconds.
  Mean final reward: -1.4366
  Mean return: -7.3655
  Mean final entropy: 0.0099
  Max final entropy: 0.0326
  Pseudo loss: -7.36673
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.286 seconds.
  Mean final reward: -0.2265
  Mean return: -4.2930
  Mean final entropy: 0.0009
  Max final entropy: 0.0061
  Pseudo loss: -1.77072
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.285 seconds.
  Mean final reward: -0.0859
  Mean return: -4.7933
  Mean final entropy: 0.0003
  Max final entropy: 0.0020
  Pseudo loss: -2.57788
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.288 seconds.
  Mean final reward: -0.4173
  Mean return: -4.6800
  Mean final entropy: 0.0037
  Max final entropy: 0.0282
  Pseudo loss: -4.43913
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (45/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.324 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.4441823 0.4441823 0.4441823 0.4441823 0.4441823 0.4441823 0.4441823
 0.4441823]
  Mean final entropy: 0.000006
  Max final entropy: 0.000006
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.4514486e-05 2.4514486e-05 2.4514486e-05 2.4514486e-05 2.4514486e-05
 2.4514486e-05 2.4514486e-05 2.4514486e-05]
  Max Min Entropy: 2.451448563078884e-05
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 29
seed: 20969
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.263 seconds.
  Mean final reward: -1.8032
  Mean return: -10.1360
  Mean final entropy: 0.0163
  Max final entropy: 0.0871
  Pseudo loss: -0.37865
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.274 seconds.
  Mean final reward: -1.9142
  Mean return: -8.5097
  Mean final entropy: 0.0256
  Max final entropy: 0.1301
  Pseudo loss: -1.02725
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.280 seconds.
  Mean final reward: -0.8949
  Mean return: -6.4343
  Mean final entropy: 0.0055
  Max final entropy: 0.0305
  Pseudo loss: -1.06242
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.269 seconds.
  Mean final reward: -1.2004
  Mean return: -7.3969
  Mean final entropy: 0.0049
  Max final entropy: 0.0165
  Pseudo loss: -0.81589
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.270 seconds.
  Mean final reward: -1.0545
  Mean return: -5.7300
  Mean final entropy: 0.0053
  Max final entropy: 0.0225
  Pseudo loss: -0.20802
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.256 seconds.
  Mean final reward: -0.8398
  Mean return: -6.1637
  Mean final entropy: 0.0048
  Max final entropy: 0.0225
  Pseudo loss: -3.41604
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.264 seconds.
  Mean final reward: -0.4525
  Mean return: -4.8340
  Mean final entropy: 0.0031
  Max final entropy: 0.0225
  Pseudo loss: -1.23377
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (40/100) took 0.280 seconds.
  Mean final reward: -0.8838
  Mean return: -5.8220
  Mean final entropy: 0.0038
  Max final entropy: 0.0157
  Pseudo loss: -3.89466
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (45/100) took 0.269 seconds.
  Mean final reward: -0.2725
  Mean return: -4.6050
  Mean final entropy: 0.0009
  Max final entropy: 0.0034
  Pseudo loss: -1.13022
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0567
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.43736
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (65/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2268
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: -0.22708
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.270 seconds.
  Mean final reward: -0.3890
  Mean return: -4.7704
  Mean final entropy: 0.0030
  Max final entropy: 0.0225
  Pseudo loss: -2.66956
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (75/100) took 0.263 seconds.
  Mean final reward: -0.2811
  Mean return: -4.6765
  Mean final entropy: 0.0014
  Max final entropy: 0.0095
  Pseudo loss: -2.61600
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (80/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.268 seconds.
  Mean final reward: -0.0613
  Mean return: -4.2372
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.41055
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (95/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.0567
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.27683
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (100/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9281
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.33597
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Testing with greedy policy:
  Start entropy: [0.34479892 0.34479892 0.34479892 0.34479892 0.34479892 0.34479892
 0.34479892 0.34479892]
  Mean final entropy: 0.000210
  Max final entropy: 0.000210
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.858189e-07 7.858189e-07 7.858189e-07 7.858189e-07 7.858189e-07
 7.858189e-07 7.858189e-07 7.858189e-07]
  Max Min Entropy: 7.858188837417401e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 30
seed: 22437
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    weight_decay: 0.0
)

Episode (5/100) took 0.269 seconds.
  Mean final reward: -0.2419
  Mean return: -5.7314
  Mean final entropy: 0.0010
  Max final entropy: 0.0069
  Pseudo loss: -0.89314
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (10/100) took 0.256 seconds.
  Mean final reward: -1.3318
  Mean return: -7.5205
  Mean final entropy: 0.0069
  Max final entropy: 0.0281
  Pseudo loss: -3.14594
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.249 seconds.
  Mean final reward: -1.6924
  Mean return: -8.0998
  Mean final entropy: 0.0144
  Max final entropy: 0.0631
  Pseudo loss: -3.70909
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.250 seconds.
  Mean final reward: -1.1047
  Mean return: -5.7298
  Mean final entropy: 0.0072
  Max final entropy: 0.0199
  Pseudo loss: 0.89287
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.270 seconds.
  Mean final reward: -0.4594
  Mean return: -3.9138
  Mean final entropy: 0.0028
  Max final entropy: 0.0199
  Pseudo loss: -0.50502
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.265 seconds.
  Mean final reward: -0.4328
  Mean return: -4.1246
  Mean final entropy: 0.0014
  Max final entropy: 0.0039
  Pseudo loss: -0.91750
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -2.9867
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.71621
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (40/100) took 0.287 seconds.
  Mean final reward: -0.3271
  Mean return: -2.9239
  Mean final entropy: 0.0013
  Max final entropy: 0.0062
  Pseudo loss: -0.63879
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (45/100) took 0.295 seconds.
  Mean final reward: -0.2637
  Mean return: -2.6222
  Mean final entropy: 0.0011
  Max final entropy: 0.0062
  Pseudo loss: -0.55274
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.291 seconds.
  Mean final reward: -0.2134
  Mean return: -2.5355
  Mean final entropy: 0.0007
  Max final entropy: 0.0038
  Pseudo loss: -0.54661
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0692
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.09651
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.295 seconds.
  Mean final reward: -0.0519
  Mean return: -2.1731
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: 0.06860
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.297 seconds.
  Mean final reward: -0.0519
  Mean return: -2.3291
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: -0.06315
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (70/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0692
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.02978
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (75/100) took 0.298 seconds.
  Mean final reward: -0.0994
  Mean return: -2.3246
  Mean final entropy: 0.0004
  Max final entropy: 0.0015
  Pseudo loss: -0.22773
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0172
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.07551
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.33247346 0.33247346 0.33247346 0.33247346 0.33247346 0.33247346
 0.33247346 0.33247346]
  Mean final entropy: 0.000009
  Max final entropy: 0.000009
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.5916063e-06 2.5916063e-06 2.5916063e-06 2.5916063e-06 2.5916063e-06
 2.5916063e-06 2.5916063e-06 2.5916063e-06]
  Max Min Entropy: 2.5916062895703362e-06
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 31
seed: 24045
