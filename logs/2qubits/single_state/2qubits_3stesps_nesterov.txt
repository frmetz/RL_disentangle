
step: 1
seed: 12345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -1.5411
  Mean return: -6.9556
  Mean final entropy: 0.0399
  Max final entropy: 0.2534
  Pseudo loss: -1.63065
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.288 seconds.
  Mean final reward: -0.3321
  Mean return: -1.1357
  Mean final entropy: 0.0019
  Max final entropy: 0.0125
  Pseudo loss: -0.68991
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.296 seconds.
  Mean final reward: -0.1685
  Mean return: -0.5519
  Mean final entropy: 0.0009
  Max final entropy: 0.0013
  Pseudo loss: 0.04896
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.292 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.07715
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.295 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.27341
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.330 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.308 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.38503304 0.38503304 0.38503304 0.38503304 0.38503304 0.38503304
 0.38503304 0.38503304]
  Mean final entropy: 0.000444
  Max final entropy: 0.000444
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07
 4.3489302e-07 4.3489302e-07 4.3489302e-07]
  Max Min Entropy: 4.3489302470334223e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 2
seed: 12329
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2949
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.15235
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1753
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.11581
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.285 seconds.
  Mean final reward: -0.0821
  Mean return: -0.4892
  Mean final entropy: 0.0006
  Max final entropy: 0.0018
  Pseudo loss: -0.19665
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2083
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.16456
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -0.3077
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.38342
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1120
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.10433
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.500
Episode (35/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1303
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.18357
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.375
Episode (40/100) took 0.282 seconds.
  Mean final reward: -0.0005
  Mean return: -0.0880
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.11824
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.250
Episode (45/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0373
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: -0.09056
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (50/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0346
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.07045
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (55/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1402
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.24736
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.500
Episode (70/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0859
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.30784
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (80/100) took 0.277 seconds.
  Mean final reward: -0.0322
  Mean return: -0.1298
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.30078
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.125
Episode (85/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.279 seconds.
  Mean final reward: -0.0100
  Mean return: -0.0557
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -0.12340
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.00239316 0.00239316 0.00239316 0.00239316 0.00239316 0.00239316
 0.00239316 0.00239316]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.729065e-08 7.729065e-08 7.729065e-08 7.729065e-08 7.729065e-08
 7.729065e-08 7.729065e-08 7.729065e-08]
  Max Min Entropy: 7.729065032435756e-08
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 3
seed: 12285
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.255 seconds.
  Mean final reward: -1.0215
  Mean return: -6.8029
  Mean final entropy: 0.0046
  Max final entropy: 0.0174
  Pseudo loss: -0.72172
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.263 seconds.
  Mean final reward: -1.0098
  Mean return: -7.4158
  Mean final entropy: 0.0084
  Max final entropy: 0.0429
  Pseudo loss: -1.80197
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.273 seconds.
  Mean final reward: -1.6220
  Mean return: -7.1610
  Mean final entropy: 0.0138
  Max final entropy: 0.0661
  Pseudo loss: -1.53509
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.269 seconds.
  Mean final reward: -0.6294
  Mean return: -5.2891
  Mean final entropy: 0.0024
  Max final entropy: 0.0106
  Pseudo loss: -0.33804
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.261 seconds.
  Mean final reward: -0.7108
  Mean return: -4.8252
  Mean final entropy: 0.0029
  Max final entropy: 0.0106
  Pseudo loss: -0.30672
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.262 seconds.
  Mean final reward: -0.6170
  Mean return: -4.2377
  Mean final entropy: 0.0023
  Max final entropy: 0.0106
  Pseudo loss: -1.39390
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.265 seconds.
  Mean final reward: -0.8091
  Mean return: -4.7733
  Mean final entropy: 0.0056
  Max final entropy: 0.0303
  Pseudo loss: -6.93182
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6088
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: -0.51335
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4543
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.33193147 0.33193147 0.33193147 0.33193147 0.33193147 0.33193147
 0.33193147 0.33193147]
  Mean final entropy: 0.000006
  Max final entropy: 0.000006
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.382591e-07 5.382591e-07 5.382591e-07 5.382591e-07 5.382591e-07
 5.382591e-07 5.382591e-07 5.382591e-07]
  Max Min Entropy: 5.382590870794957e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [6, 6, 6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 4
seed: 12219
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.291 seconds.
  Mean final reward: -0.5444
  Mean return: -4.1720
  Mean final entropy: 0.0030
  Max final entropy: 0.0195
  Pseudo loss: -5.67254
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.5825697 0.5825697 0.5825697 0.5825697 0.5825697 0.5825697 0.5825697
 0.5825697]
  Mean final entropy: 0.000053
  Max final entropy: 0.000053
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.73917e-07 3.73917e-07 3.73917e-07 3.73917e-07 3.73917e-07 3.73917e-07
 3.73917e-07 3.73917e-07]
  Max Min Entropy: 3.739170040262252e-07
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 5
seed: 12137
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.280 seconds.
  Mean final reward: -1.4169
  Mean return: -8.0355
  Mean final entropy: 0.0056
  Max final entropy: 0.0177
  Pseudo loss: -0.16678
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.288 seconds.
  Mean final reward: -1.6799
  Mean return: -6.5669
  Mean final entropy: 0.0070
  Max final entropy: 0.0133
  Pseudo loss: 1.30747
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.292 seconds.
  Mean final reward: -1.3980
  Mean return: -6.1279
  Mean final entropy: 0.0057
  Max final entropy: 0.0143
  Pseudo loss: 0.22425
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.289 seconds.
  Mean final reward: -1.7473
  Mean return: -7.0172
  Mean final entropy: 0.0083
  Max final entropy: 0.0143
  Pseudo loss: 0.05966
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (25/100) took 0.288 seconds.
  Mean final reward: -0.5036
  Mean return: -3.9931
  Mean final entropy: 0.0016
  Max final entropy: 0.0046
  Pseudo loss: -0.21546
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1986
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.00648
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (35/100) took 0.295 seconds.
  Mean final reward: -0.7159
  Mean return: -3.6787
  Mean final entropy: 0.0023
  Max final entropy: 0.0046
  Pseudo loss: -0.55106
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (40/100) took 0.289 seconds.
  Mean final reward: -0.4925
  Mean return: -3.6405
  Mean final entropy: 0.0066
  Max final entropy: 0.0514
  Pseudo loss: -6.13261
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (45/100) took 0.295 seconds.
  Mean final reward: -0.0401
  Mean return: -2.2729
  Mean final entropy: 0.0004
  Max final entropy: 0.0014
  Pseudo loss: -0.06897
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (50/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1884
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.17100
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (55/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1884
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.06824
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1441
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.03081
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (65/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1098
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.37589
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (70/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9221
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9221
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9665
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.12451
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9665
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.14793
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9221
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9221
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9221
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.32000387 0.32000387 0.32000387 0.32000387 0.32000387 0.32000387
 0.32000387 0.32000387]
  Mean final entropy: 0.000005
  Max final entropy: 0.000005
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.565885e-07 5.565885e-07 5.565885e-07 5.565885e-07 5.565885e-07
 5.565885e-07 5.565885e-07 5.565885e-07]
  Max Min Entropy: 5.565884748648386e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 6
seed: 12045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.275 seconds.
  Mean final reward: -2.0747
  Mean return: -9.7495
  Mean final entropy: 0.0332
  Max final entropy: 0.1476
  Pseudo loss: -1.52573
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.284 seconds.
  Mean final reward: -1.4018
  Mean return: -6.8790
  Mean final entropy: 0.0122
  Max final entropy: 0.0654
  Pseudo loss: -6.85643
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.285 seconds.
  Mean final reward: -1.2172
  Mean return: -5.9242
  Mean final entropy: 0.0054
  Max final entropy: 0.0160
  Pseudo loss: -0.68370
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (25/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (30/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.287 seconds.
  Mean final reward: -0.0879
  Mean return: -3.5836
  Mean final entropy: 0.0005
  Max final entropy: 0.0020
  Pseudo loss: -0.44401
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (40/100) took 0.285 seconds.
  Mean final reward: -0.0715
  Mean return: -3.5672
  Mean final entropy: 0.0004
  Max final entropy: 0.0018
  Pseudo loss: -0.29446
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.4957
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.2932917 0.2932917 0.2932917 0.2932917 0.2932917 0.2932917 0.2932917
 0.2932917]
  Mean final entropy: 0.000239
  Max final entropy: 0.000239
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.9962024e-05 2.9962024e-05 2.9962024e-05 2.9962024e-05 2.9962024e-05
 2.9962024e-05 2.9962024e-05 2.9962024e-05]
  Max Min Entropy: 2.9962024200358428e-05
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [4, 4, 4, 4, 4, 4, 4, 4], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 7
seed: 11949
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.268 seconds.
  Mean final reward: -3.2864
  Mean return: -13.5153
  Mean final entropy: 0.0824
  Max final entropy: 0.3188
  Pseudo loss: -0.13708
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.273 seconds.
  Mean final reward: -1.1995
  Mean return: -7.9512
  Mean final entropy: 0.0122
  Max final entropy: 0.0652
  Pseudo loss: -1.00226
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (15/100) took 0.287 seconds.
  Mean final reward: -0.0618
  Mean return: -1.9444
  Mean final entropy: 0.0007
  Max final entropy: 0.0012
  Pseudo loss: -0.46957
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.286 seconds.
  Mean final reward: -0.0229
  Mean return: -1.5144
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.12525
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (25/100) took 0.286 seconds.
  Mean final reward: -0.0138
  Mean return: -1.4970
  Mean final entropy: 0.0004
  Max final entropy: 0.0011
  Pseudo loss: -0.08288
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4979
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: -0.06752
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (35/100) took 0.319 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4962
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: -0.08199
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5432
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: -0.16660
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4444
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.01546
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (50/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5043
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: -0.10414
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4573
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00665
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (60/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4509
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.01120
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4573
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00619
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4509
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00973
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (75/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4250
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00834
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.285 seconds.
  Mean final reward: -0.0065
  Mean return: -1.4574
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -0.02229
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4380
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00724
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.288 seconds.
  Mean final reward: -0.0065
  Mean return: -1.4509
  Mean final entropy: 0.0004
  Max final entropy: 0.0011
  Pseudo loss: -0.02014
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (95/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4573
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00089
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (100/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4380
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: -0.00050
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.47997645 0.47997645 0.47997645 0.47997645 0.47997645 0.47997645
 0.47997645 0.47997645]
  Mean final entropy: 0.000126
  Max final entropy: 0.000126
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.2287949e-07 2.2287949e-07 2.2287949e-07 2.2287949e-07 2.2287949e-07
 2.2287949e-07 2.2287949e-07 2.2287949e-07]
  Max Min Entropy: 2.2287949263954943e-07
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 8
seed: 11855
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.284 seconds.
  Mean final reward: -1.8819
  Mean return: -7.2547
  Mean final entropy: 0.0164
  Max final entropy: 0.0830
  Pseudo loss: -3.68531
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.287 seconds.
  Mean final reward: -0.4688
  Mean return: -3.8941
  Mean final entropy: 0.0014
  Max final entropy: 0.0034
  Pseudo loss: -1.03938
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.294 seconds.
  Mean final reward: -0.5060
  Mean return: -3.6086
  Mean final entropy: 0.0020
  Max final entropy: 0.0118
  Pseudo loss: -4.60041
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.290 seconds.
  Mean final reward: -0.1698
  Mean return: -2.4557
  Mean final entropy: 0.0005
  Max final entropy: 0.0023
  Pseudo loss: -1.20993
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9537
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.22726473 0.22726473 0.22726473 0.22726473 0.22726473 0.22726473
 0.22726473 0.22726473]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.6317294e-06 1.6317294e-06 1.6317294e-06 1.6317294e-06 1.6317294e-06
 1.6317294e-06 1.6317294e-06 1.6317294e-06]
  Max Min Entropy: 1.6317294466716703e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 9
seed: 11769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.257 seconds.
  Mean final reward: -1.7186
  Mean return: -8.5186
  Mean final entropy: 0.0134
  Max final entropy: 0.0440
  Pseudo loss: 0.89864
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.266 seconds.
  Mean final reward: -0.2473
  Mean return: -3.7994
  Mean final entropy: 0.0009
  Max final entropy: 0.0039
  Pseudo loss: -0.98171
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.282 seconds.
  Mean final reward: -0.3160
  Mean return: -2.2818
  Mean final entropy: 0.0010
  Max final entropy: 0.0028
  Pseudo loss: 0.63680
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.283 seconds.
  Mean final reward: -0.2043
  Mean return: -1.9653
  Mean final entropy: 0.0007
  Max final entropy: 0.0038
  Pseudo loss: 0.16654
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.282 seconds.
  Mean final reward: -0.1665
  Mean return: -1.8479
  Mean final entropy: 0.0006
  Max final entropy: 0.0038
  Pseudo loss: -0.03953
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.282 seconds.
  Mean final reward: -0.4735
  Mean return: -2.4753
  Mean final entropy: 0.0014
  Max final entropy: 0.0038
  Pseudo loss: 0.13884
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5181
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.00725
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.283 seconds.
  Mean final reward: -0.0598
  Mean return: -1.5343
  Mean final entropy: 0.0003
  Max final entropy: 0.0016
  Pseudo loss: 0.00092
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4881
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4446
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.08894
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.284 seconds.
  Mean final reward: -0.0684
  Mean return: -1.5429
  Mean final entropy: 0.0003
  Max final entropy: 0.0017
  Pseudo loss: -0.16650
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4446
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.07265
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4010
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.09883
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4446
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.03864
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3139
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.04616
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4010
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00782
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (85/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2267
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.05790
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (90/100) took 0.275 seconds.
  Mean final reward: -0.0645
  Mean return: -1.2041
  Mean final entropy: 0.0002
  Max final entropy: 0.0017
  Pseudo loss: -0.19824
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (95/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0703
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.19899
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (100/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1396
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.1356557 0.1356557 0.1356557 0.1356557 0.1356557 0.1356557 0.1356557
 0.1356557]
  Mean final entropy: 0.000018
  Max final entropy: 0.000018
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.09362844e-07 1.09362844e-07 1.09362844e-07 1.09362844e-07
 1.09362844e-07 1.09362844e-07 1.09362844e-07 1.09362844e-07]
  Max Min Entropy: 1.0936284411400266e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 10
seed: 11697
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.264 seconds.
  Mean final reward: -0.4520
  Mean return: -5.5454
  Mean final entropy: 0.0024
  Max final entropy: 0.0133
  Pseudo loss: -1.38261
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.269 seconds.
  Mean final reward: -0.3166
  Mean return: -4.4235
  Mean final entropy: 0.0013
  Max final entropy: 0.0045
  Pseudo loss: -2.16083
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (15/100) took 0.252 seconds.
  Mean final reward: -0.4535
  Mean return: -2.8416
  Mean final entropy: 0.0016
  Max final entropy: 0.0036
  Pseudo loss: -0.01022
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.257 seconds.
  Mean final reward: -0.0877
  Mean return: -2.0877
  Mean final entropy: 0.0008
  Max final entropy: 0.0020
  Pseudo loss: 0.08865
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (25/100) took 0.255 seconds.
  Mean final reward: -0.2779
  Mean return: -2.5568
  Mean final entropy: 0.0014
  Max final entropy: 0.0023
  Pseudo loss: -0.74538
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1060
  Mean final entropy: 0.0009
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0245
  Mean final entropy: 0.0008
  Max final entropy: 0.0009
  Pseudo loss: 0.12936
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (40/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0245
  Mean final entropy: 0.0008
  Max final entropy: 0.0009
  Pseudo loss: 0.08375
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (45/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7802
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: -0.00896
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (50/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6173
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.22079
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (55/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6173
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.43391
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (60/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.258 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4544
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.3289355 0.3289355 0.3289355 0.3289355 0.3289355 0.3289355 0.3289355
 0.3289355]
  Mean final entropy: 0.000008
  Max final entropy: 0.000008
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.3486309e-06 1.3486309e-06 1.3486309e-06 1.3486309e-06 1.3486309e-06
 1.3486309e-06 1.3486309e-06 1.3486309e-06]
  Max Min Entropy: 1.3486309171639732e-06
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [2, 2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 11
seed: 11645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.272 seconds.
  Mean final reward: -0.8459
  Mean return: -7.6014
  Mean final entropy: 0.0031
  Max final entropy: 0.0088
  Pseudo loss: -0.19567
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.276 seconds.
  Mean final reward: -1.1554
  Mean return: -6.8713
  Mean final entropy: 0.0067
  Max final entropy: 0.0234
  Pseudo loss: 0.05432
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.274 seconds.
  Mean final reward: -1.9341
  Mean return: -7.9775
  Mean final entropy: 0.0130
  Max final entropy: 0.0362
  Pseudo loss: -0.97688
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.264 seconds.
  Mean final reward: -0.9190
  Mean return: -6.1827
  Mean final entropy: 0.0035
  Max final entropy: 0.0127
  Pseudo loss: -0.79228
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (25/100) took 0.279 seconds.
  Mean final reward: -0.9115
  Mean return: -5.6982
  Mean final entropy: 0.0125
  Max final entropy: 0.0892
  Pseudo loss: -4.15893
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.287 seconds.
  Mean final reward: -0.7224
  Mean return: -4.9323
  Mean final entropy: 0.0045
  Max final entropy: 0.0171
  Pseudo loss: -0.46546
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (35/100) took 0.283 seconds.
  Mean final reward: -0.0630
  Mean return: -3.5467
  Mean final entropy: 0.0003
  Max final entropy: 0.0017
  Pseudo loss: -0.82535
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0823
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.10031906 0.10031906 0.10031906 0.10031906 0.10031906 0.10031906
 0.10031906 0.10031906]
  Mean final entropy: 0.000006
  Max final entropy: 0.000006
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.788516e-07 4.788516e-07 4.788516e-07 4.788516e-07 4.788516e-07
 4.788516e-07 4.788516e-07 4.788516e-07]
  Max Min Entropy: 4.788515752807143e-07
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [5, 5, 5, 5, 5, 5, 5, 5], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 12
seed: 11619
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.259 seconds.
  Mean final reward: -1.3597
  Mean return: -8.1793
  Mean final entropy: 0.0086
  Max final entropy: 0.0348
  Pseudo loss: -1.13905
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.261 seconds.
  Mean final reward: -1.5117
  Mean return: -8.6963
  Mean final entropy: 0.0067
  Max final entropy: 0.0160
  Pseudo loss: -0.10679
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.268 seconds.
  Mean final reward: -0.6215
  Mean return: -6.0918
  Mean final entropy: 0.0018
  Max final entropy: 0.0033
  Pseudo loss: -2.19222
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.266 seconds.
  Mean final reward: -1.0712
  Mean return: -6.8418
  Mean final entropy: 0.0105
  Max final entropy: 0.0651
  Pseudo loss: -6.39883
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.258 seconds.
  Mean final reward: -0.1286
  Mean return: -4.4355
  Mean final entropy: 0.0005
  Max final entropy: 0.0014
  Pseudo loss: -0.12264
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.251 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1866
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.35815555 0.35815555 0.35815555 0.35815555 0.35815555 0.35815555
 0.35815555 0.35815555]
  Mean final entropy: 0.000017
  Max final entropy: 0.000017
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.2591973e-05 1.2591973e-05 1.2591973e-05 1.2591973e-05 1.2591973e-05
 1.2591973e-05 1.2591973e-05 1.2591973e-05]
  Max Min Entropy: 1.2591973245434929e-05
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 13
seed: 11625
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.270 seconds.
  Mean final reward: -2.2414
  Mean return: -9.6551
  Mean final entropy: 0.0237
  Max final entropy: 0.1147
  Pseudo loss: -0.19289
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.266 seconds.
  Mean final reward: -1.2993
  Mean return: -8.8181
  Mean final entropy: 0.0244
  Max final entropy: 0.1727
  Pseudo loss: -0.66150
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.280 seconds.
  Mean final reward: -2.2854
  Mean return: -10.7032
  Mean final entropy: 0.0237
  Max final entropy: 0.1169
  Pseudo loss: -0.58830
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.283 seconds.
  Mean final reward: -1.2146
  Mean return: -8.3772
  Mean final entropy: 0.0124
  Max final entropy: 0.0640
  Pseudo loss: -2.64067
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.278 seconds.
  Mean final reward: -0.8572
  Mean return: -7.0706
  Mean final entropy: 0.0105
  Max final entropy: 0.0732
  Pseudo loss: -2.24684
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.323 seconds.
  Mean final reward: -1.2255
  Mean return: -7.6618
  Mean final entropy: 0.0070
  Max final entropy: 0.0304
  Pseudo loss: -3.84680
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (35/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9803
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.02083
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9579
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.00277
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -5.0027
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.02967
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9131
  Mean final entropy: 0.0000
  Max final entropy: 0.0004
  Pseudo loss: -0.02647
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.239 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.243 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9131
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.04461
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.239 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9355
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.07011
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.243 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9131
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.04358
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.238 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.237 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.239 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.238 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.25989905 0.25989905 0.25989905 0.25989905 0.25989905 0.25989905
 0.25989905 0.25989905]
  Mean final entropy: 0.000004
  Max final entropy: 0.000004
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [0.00243049 0.00243049 0.00243049 0.00243049 0.00243049 0.00243049
 0.00243049 0.00243049]
  Max Min Entropy: 0.002430494176223874
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 14
seed: 11669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.272 seconds.
  Mean final reward: -0.6701
  Mean return: -3.6978
  Mean final entropy: 0.0141
  Max final entropy: 0.1091
  Pseudo loss: -5.83018
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.264 seconds.
  Mean final reward: -0.1145
  Mean return: -0.4814
  Mean final entropy: 0.0008
  Max final entropy: 0.0013
  Pseudo loss: -0.07226
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.265 seconds.
  Mean final reward: -0.0281
  Mean return: -0.3496
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: 0.03138
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.261 seconds.
  Mean final reward: -0.1152
  Mean return: -0.4700
  Mean final entropy: 0.0006
  Max final entropy: 0.0013
  Pseudo loss: 0.03938
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.261 seconds.
  Mean final reward: -0.0281
  Mean return: -0.3468
  Mean final entropy: 0.0006
  Max final entropy: 0.0013
  Pseudo loss: 0.01246
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.264 seconds.
  Mean final reward: -0.0562
  Mean return: -0.3805
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.10843
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2899
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -0.04274
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (40/100) took 0.265 seconds.
  Mean final reward: -0.0562
  Mean return: -0.3647
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.20166
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (45/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2639
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.03467
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2509
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.02063
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2509
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.03164
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2509
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: -0.03915
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.265 seconds.
  Mean final reward: -0.0281
  Mean return: -0.2948
  Mean final entropy: 0.0002
  Max final entropy: 0.0013
  Pseudo loss: -0.23309
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (95/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2379
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.25187474 0.25187474 0.25187474 0.25187474 0.25187474 0.25187474
 0.25187474 0.25187474]
  Mean final entropy: 0.000015
  Max final entropy: 0.000015
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.9582785e-08 5.9582785e-08 5.9582785e-08 5.9582785e-08 5.9582785e-08
 5.9582785e-08 5.9582785e-08 5.9582785e-08]
  Max Min Entropy: 5.9582784928124966e-08
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [3, 3, 3, 3, 3, 3, 3, 3], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 15
seed: 11757
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.267 seconds.
  Mean final reward: -2.1588
  Mean return: -10.3582
  Mean final entropy: 0.0632
  Max final entropy: 0.3179
  Pseudo loss: -3.24322
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.283 seconds.
  Mean final reward: -0.5646
  Mean return: -3.7129
  Mean final entropy: 0.0019
  Max final entropy: 0.0038
  Pseudo loss: -0.62332
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.281 seconds.
  Mean final reward: -0.7125
  Mean return: -4.3601
  Mean final entropy: 0.0028
  Max final entropy: 0.0074
  Pseudo loss: -0.43298
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.282 seconds.
  Mean final reward: -0.4462
  Mean return: -3.5145
  Mean final entropy: 0.0018
  Max final entropy: 0.0070
  Pseudo loss: -0.11606
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (25/100) took 0.285 seconds.
  Mean final reward: -0.5589
  Mean return: -3.6074
  Mean final entropy: 0.0020
  Max final entropy: 0.0062
  Pseudo loss: -0.30608
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (30/100) took 0.286 seconds.
  Mean final reward: -0.4559
  Mean return: -3.5796
  Mean final entropy: 0.0016
  Max final entropy: 0.0037
  Pseudo loss: -0.64786
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (35/100) took 0.285 seconds.
  Mean final reward: -0.3372
  Mean return: -3.2310
  Mean final entropy: 0.0013
  Max final entropy: 0.0016
  Pseudo loss: 0.73595
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (40/100) took 0.286 seconds.
  Mean final reward: -0.3292
  Mean return: -3.3730
  Mean final entropy: 0.0014
  Max final entropy: 0.0016
  Pseudo loss: 0.04133
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.286 seconds.
  Mean final reward: -0.3562
  Mean return: -3.4000
  Mean final entropy: 0.0014
  Max final entropy: 0.0016
  Pseudo loss: 0.00774
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.288 seconds.
  Mean final reward: -0.4375
  Mean return: -3.5612
  Mean final entropy: 0.0016
  Max final entropy: 0.0028
  Pseudo loss: -0.77212
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.287 seconds.
  Mean final reward: -0.2905
  Mean return: -2.8884
  Mean final entropy: 0.0013
  Max final entropy: 0.0063
  Pseudo loss: -1.40154
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4423
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.290 seconds.
  Mean final reward: -0.0675
  Mean return: -2.4351
  Mean final entropy: 0.0006
  Max final entropy: 0.0017
  Pseudo loss: -0.03169
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (70/100) took 0.286 seconds.
  Mean final reward: -0.0675
  Mean return: -2.4351
  Mean final entropy: 0.0006
  Max final entropy: 0.0017
  Pseudo loss: -0.05936
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (75/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3676
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.08525
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (80/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1434
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: -0.00239
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (85/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9192
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.11744
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8445
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8445
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8445
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.41695437 0.41695437 0.41695437 0.41695437 0.41695437 0.41695437
 0.41695437 0.41695437]
  Mean final entropy: 0.000015
  Max final entropy: 0.000015
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.7722635e-07 2.7722635e-07 2.7722635e-07 2.7722635e-07 2.7722635e-07
 2.7722635e-07 2.7722635e-07 2.7722635e-07]
  Max Min Entropy: 2.7722634854399075e-07
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 16
seed: 11895
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.278 seconds.
  Mean final reward: -1.2417
  Mean return: -6.9747
  Mean final entropy: 0.0065
  Max final entropy: 0.0222
  Pseudo loss: -0.62808
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.292 seconds.
  Mean final reward: -0.6900
  Mean return: -4.3160
  Mean final entropy: 0.0032
  Max final entropy: 0.0147
  Pseudo loss: -1.38864
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.293 seconds.
  Mean final reward: -0.7942
  Mean return: -4.2129
  Mean final entropy: 0.0037
  Max final entropy: 0.0186
  Pseudo loss: -5.76017
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.293 seconds.
  Mean final reward: -0.2976
  Mean return: -2.7473
  Mean final entropy: 0.0010
  Max final entropy: 0.0027
  Pseudo loss: -0.66883
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.295 seconds.
  Mean final reward: -0.0529
  Mean return: -2.0393
  Mean final entropy: 0.0003
  Max final entropy: 0.0015
  Pseudo loss: -0.67314
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.333 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6746
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.17790876 0.17790876 0.17790876 0.17790876 0.17790876 0.17790876
 0.17790876 0.17790876]
  Mean final entropy: 0.000059
  Max final entropy: 0.000059
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.1524484e-05 1.1524484e-05 1.1524484e-05 1.1524484e-05 1.1524484e-05
 1.1524484e-05 1.1524484e-05 1.1524484e-05]
  Max Min Entropy: 1.1524483852554113e-05
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 17
seed: 12089
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -1.7541
  Mean return: -7.9461
  Mean final entropy: 0.0090
  Max final entropy: 0.0275
  Pseudo loss: -0.58303
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.279 seconds.
  Mean final reward: -1.1477
  Mean return: -7.2938
  Mean final entropy: 0.0086
  Max final entropy: 0.0493
  Pseudo loss: -0.22743
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.266 seconds.
  Mean final reward: -0.8847
  Mean return: -6.3201
  Mean final entropy: 0.0076
  Max final entropy: 0.0486
  Pseudo loss: -5.19123
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.353 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7511
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5180683 0.5180683 0.5180683 0.5180683 0.5180683 0.5180683 0.5180683
 0.5180683]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.973639e-06 1.973639e-06 1.973639e-06 1.973639e-06 1.973639e-06
 1.973639e-06 1.973639e-06 1.973639e-06]
  Max Min Entropy: 1.9736389731406234e-06
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [3, 3, 3, 3, 3, 3, 3, 3], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 18
seed: 12345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.263 seconds.
  Mean final reward: -1.5411
  Mean return: -6.9556
  Mean final entropy: 0.0399
  Max final entropy: 0.2534
  Pseudo loss: -1.63065
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.291 seconds.
  Mean final reward: -0.3321
  Mean return: -1.1357
  Mean final entropy: 0.0019
  Max final entropy: 0.0125
  Pseudo loss: -0.68991
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.291 seconds.
  Mean final reward: -0.1685
  Mean return: -0.5519
  Mean final entropy: 0.0009
  Max final entropy: 0.0013
  Pseudo loss: 0.04896
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.295 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: -0.07715
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.293 seconds.
  Mean final reward: -0.0337
  Mean return: -0.1104
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.27341
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.38503304 0.38503304 0.38503304 0.38503304 0.38503304 0.38503304
 0.38503304 0.38503304]
  Mean final entropy: 0.000444
  Max final entropy: 0.000444
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07 4.3489302e-07
 4.3489302e-07 4.3489302e-07 4.3489302e-07]
  Max Min Entropy: 4.3489302470334223e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 19
seed: 12669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -0.9580
  Mean return: -6.0318
  Mean final entropy: 0.0043
  Max final entropy: 0.0141
  Pseudo loss: -0.30439
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.258 seconds.
  Mean final reward: -0.5919
  Mean return: -2.7965
  Mean final entropy: 0.0030
  Max final entropy: 0.0131
  Pseudo loss: -3.13267
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.31995574 0.31995574 0.31995574 0.31995574 0.31995574 0.31995574
 0.31995574 0.31995574]
  Mean final entropy: 0.000633
  Max final entropy: 0.000633
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.629032e-07 5.629032e-07 5.629032e-07 5.629032e-07 5.629032e-07
 5.629032e-07 5.629032e-07 5.629032e-07]
  Max Min Entropy: 5.629032102660858e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [2, 2, 2, 2, 2, 2, 2, 2], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 20
seed: 13067
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.261 seconds.
  Mean final reward: -0.9761
  Mean return: -8.6334
  Mean final entropy: 0.0041
  Max final entropy: 0.0120
  Pseudo loss: -0.94652
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.278 seconds.
  Mean final reward: -1.5041
  Mean return: -8.8862
  Mean final entropy: 0.0135
  Max final entropy: 0.0502
  Pseudo loss: -2.75054
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.269 seconds.
  Mean final reward: -0.9383
  Mean return: -9.0567
  Mean final entropy: 0.0045
  Max final entropy: 0.0150
  Pseudo loss: -3.27035
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.268 seconds.
  Mean final reward: -0.1104
  Mean return: -5.3597
  Mean final entropy: 0.0004
  Max final entropy: 0.0024
  Pseudo loss: -0.09132
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (25/100) took 0.262 seconds.
  Mean final reward: -0.1776
  Mean return: -5.2079
  Mean final entropy: 0.0005
  Max final entropy: 0.0041
  Pseudo loss: -0.92235
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8449
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.18921
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.256 seconds.
  Mean final reward: -0.0065
  Mean return: -5.4347
  Mean final entropy: 0.0001
  Max final entropy: 0.0011
  Pseudo loss: -3.78497
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8252
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.49351934 0.49351934 0.49351934 0.49351934 0.49351934 0.49351934
 0.49351934 0.49351934]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.4960123e-07 4.4960123e-07 4.4960123e-07 4.4960123e-07 4.4960123e-07
 4.4960123e-07 4.4960123e-07 4.4960123e-07]
  Max Min Entropy: 4.496012309118669e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 21
seed: 13545
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.256 seconds.
  Mean final reward: -0.4172
  Mean return: -6.8645
  Mean final entropy: 0.0014
  Max final entropy: 0.0057
  Pseudo loss: -1.03919
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.286 seconds.
  Mean final reward: -1.3402
  Mean return: -7.7460
  Mean final entropy: 0.0133
  Max final entropy: 0.0512
  Pseudo loss: -3.64425
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.288 seconds.
  Mean final reward: -0.6522
  Mean return: -4.6511
  Mean final entropy: 0.0028
  Max final entropy: 0.0109
  Pseudo loss: -0.11858
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.287 seconds.
  Mean final reward: -0.1447
  Mean return: -3.3202
  Mean final entropy: 0.0007
  Max final entropy: 0.0016
  Pseudo loss: -0.02441
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7340
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.10096
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5515
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.22399
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5846
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.40731
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5099
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.6136633 0.6136633 0.6136633 0.6136633 0.6136633 0.6136633 0.6136633
 0.6136633]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.249249e-07 4.249249e-07 4.249249e-07 4.249249e-07 4.249249e-07
 4.249249e-07 4.249249e-07 4.249249e-07]
  Max Min Entropy: 4.249249059284921e-07
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 22
seed: 14109
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.275 seconds.
  Mean final reward: -1.0681
  Mean return: -7.3692
  Mean final entropy: 0.0058
  Max final entropy: 0.0260
  Pseudo loss: -0.53311
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.287 seconds.
  Mean final reward: -0.1072
  Mean return: -1.3540
  Mean final entropy: 0.0006
  Max final entropy: 0.0021
  Pseudo loss: 0.33554
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (15/100) took 0.282 seconds.
  Mean final reward: -0.3938
  Mean return: -1.9733
  Mean final entropy: 0.0015
  Max final entropy: 0.0024
  Pseudo loss: 0.11118
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.286 seconds.
  Mean final reward: -0.3300
  Mean return: -1.8507
  Mean final entropy: 0.0012
  Max final entropy: 0.0024
  Pseudo loss: 0.17301
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.284 seconds.
  Mean final reward: -0.1022
  Mean return: -1.4704
  Mean final entropy: 0.0010
  Max final entropy: 0.0023
  Pseudo loss: -0.05265
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (30/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2247
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: -0.03817
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1378
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.03289
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (40/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2466
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: -0.26506
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1598
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: -0.08957
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1598
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: -0.17184
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1299
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: -0.10534
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.511224 0.511224 0.511224 0.511224 0.511224 0.511224 0.511224 0.511224]
  Mean final entropy: 0.000526
  Max final entropy: 0.000526
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.881851e-08 9.881851e-08 9.881851e-08 9.881851e-08 9.881851e-08
 9.881851e-08 9.881851e-08 9.881851e-08]
  Max Min Entropy: 9.881851070758785e-08
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 23
seed: 14765
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.264 seconds.
  Mean final reward: -0.4102
  Mean return: -5.5270
  Mean final entropy: 0.0015
  Max final entropy: 0.0075
  Pseudo loss: -0.02536
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.275 seconds.
  Mean final reward: -0.8936
  Mean return: -5.7370
  Mean final entropy: 0.0079
  Max final entropy: 0.0526
  Pseudo loss: -1.82183
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.276 seconds.
  Mean final reward: -1.3510
  Mean return: -5.8970
  Mean final entropy: 0.0084
  Max final entropy: 0.0419
  Pseudo loss: -1.93660
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.270 seconds.
  Mean final reward: -0.4820
  Mean return: -5.7461
  Mean final entropy: 0.0016
  Max final entropy: 0.0073
  Pseudo loss: -1.53710
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.288 seconds.
  Mean final reward: -0.3642
  Mean return: -3.3173
  Mean final entropy: 0.0012
  Max final entropy: 0.0052
  Pseudo loss: -1.22508
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.289 seconds.
  Mean final reward: -0.2484
  Mean return: -2.7377
  Mean final entropy: 0.0010
  Max final entropy: 0.0073
  Pseudo loss: -0.98897
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7133
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -1.83813
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2351
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: -0.42369
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (45/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2351
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: -0.64647
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0748
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.13400906 0.13400906 0.13400906 0.13400906 0.13400906 0.13400906
 0.13400906 0.13400906]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.129566e-07 3.129566e-07 3.129566e-07 3.129566e-07 3.129566e-07
 3.129566e-07 3.129566e-07 3.129566e-07]
  Max Min Entropy: 3.1295658686758543e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 24
seed: 15519
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.269 seconds.
  Mean final reward: -2.3386
  Mean return: -11.2779
  Mean final entropy: 0.0538
  Max final entropy: 0.3063
  Pseudo loss: 0.95353
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.281 seconds.
  Mean final reward: -1.6596
  Mean return: -8.8093
  Mean final entropy: 0.0126
  Max final entropy: 0.0470
  Pseudo loss: -0.38284
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.275 seconds.
  Mean final reward: -1.6295
  Mean return: -8.1869
  Mean final entropy: 0.0222
  Max final entropy: 0.0949
  Pseudo loss: -2.88989
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.268 seconds.
  Mean final reward: -1.7098
  Mean return: -8.9426
  Mean final entropy: 0.0431
  Max final entropy: 0.3087
  Pseudo loss: -1.83387
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (25/100) took 0.289 seconds.
  Mean final reward: -0.4569
  Mean return: -6.0765
  Mean final entropy: 0.0025
  Max final entropy: 0.0155
  Pseudo loss: -0.65800
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.291 seconds.
  Mean final reward: -0.1071
  Mean return: -3.6133
  Mean final entropy: 0.0006
  Max final entropy: 0.0024
  Pseudo loss: -1.65545
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (35/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6716
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: -1.17656
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6716
  Mean final entropy: 0.0004
  Max final entropy: 0.0007
  Pseudo loss: -2.08570
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2469
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.4300679 0.4300679 0.4300679 0.4300679 0.4300679 0.4300679 0.4300679
 0.4300679]
  Mean final entropy: 0.000345
  Max final entropy: 0.000345
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0489292e-06 2.0489292e-06 2.0489292e-06 2.0489292e-06 2.0489292e-06
 2.0489292e-06 2.0489292e-06 2.0489292e-06]
  Max Min Entropy: 2.048929218290141e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [4, 4, 4, 4, 4, 4, 4, 4], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 25
seed: 16377
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.284 seconds.
  Mean final reward: -1.7719
  Mean return: -7.6159
  Mean final entropy: 0.0237
  Max final entropy: 0.0861
  Pseudo loss: -0.35347
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.5586
  Mean return: -6.6448
  Mean final entropy: 0.0190
  Max final entropy: 0.0984
  Pseudo loss: -4.62888
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.291 seconds.
  Mean final reward: -0.2647
  Mean return: -2.4301
  Mean final entropy: 0.0009
  Max final entropy: 0.0029
  Pseudo loss: 0.33725
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8272
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: -0.34834
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (25/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6582
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.18161497 0.18161497 0.18161497 0.18161497 0.18161497 0.18161497
 0.18161497 0.18161497]
  Mean final entropy: 0.000157
  Max final entropy: 0.000157
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.4483534e-07 4.4483534e-07 4.4483534e-07 4.4483534e-07 4.4483534e-07
 4.4483534e-07 4.4483534e-07 4.4483534e-07]
  Max Min Entropy: 4.448353365660296e-07
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [7, 7, 7, 7, 7, 7, 7, 7], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 26
seed: 17345
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.276 seconds.
  Mean final reward: -2.6136
  Mean return: -11.6288
  Mean final entropy: 0.0313
  Max final entropy: 0.0764
  Pseudo loss: 0.50364
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.248 seconds.
  Mean final reward: -1.7877
  Mean return: -10.2216
  Mean final entropy: 0.0184
  Max final entropy: 0.1077
  Pseudo loss: -1.28733
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.261 seconds.
  Mean final reward: -1.2029
  Mean return: -8.6884
  Mean final entropy: 0.0074
  Max final entropy: 0.0240
  Pseudo loss: -0.78607
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.255 seconds.
  Mean final reward: -0.5878
  Mean return: -7.9495
  Mean final entropy: 0.0021
  Max final entropy: 0.0053
  Pseudo loss: -1.54824
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.271 seconds.
  Mean final reward: -0.5292
  Mean return: -7.4682
  Mean final entropy: 0.0029
  Max final entropy: 0.0167
  Pseudo loss: -0.41822
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.5487
  Mean return: -7.2555
  Mean final entropy: 0.0104
  Max final entropy: 0.0806
  Pseudo loss: -2.74261
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (35/100) took 0.287 seconds.
  Mean final reward: -0.2645
  Mean return: -6.0720
  Mean final entropy: 0.0012
  Max final entropy: 0.0083
  Pseudo loss: -1.05598
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.306 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.306 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.319 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -4.4088
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.40393737 0.40393737 0.40393737 0.40393737 0.40393737 0.40393737
 0.40393737 0.40393737]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.1454004e-05 5.1454004e-05 5.1454004e-05 5.1454004e-05 5.1454004e-05
 5.1454004e-05 5.1454004e-05 5.1454004e-05]
  Max Min Entropy: 5.145400427863933e-05
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 27
seed: 18429
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.280 seconds.
  Mean final reward: -1.6162
  Mean return: -7.3045
  Mean final entropy: 0.0140
  Max final entropy: 0.0379
  Pseudo loss: -0.37187
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.284 seconds.
  Mean final reward: -0.4989
  Mean return: -2.8661
  Mean final entropy: 0.0071
  Max final entropy: 0.0541
  Pseudo loss: -5.82291
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.5799166 0.5799166 0.5799166 0.5799166 0.5799166 0.5799166 0.5799166
 0.5799166]
  Mean final entropy: 0.000879
  Max final entropy: 0.000879
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.9914105e-07 2.9914105e-07 2.9914105e-07 2.9914105e-07 2.9914105e-07
 2.9914105e-07 2.9914105e-07 2.9914105e-07]
  Max Min Entropy: 2.991410497088509e-07
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 28
seed: 19635
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -1.5148
  Mean return: -7.9334
  Mean final entropy: 0.0296
  Max final entropy: 0.1461
  Pseudo loss: -5.66596
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -3.8161
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.4441823 0.4441823 0.4441823 0.4441823 0.4441823 0.4441823 0.4441823
 0.4441823]
  Mean final entropy: 0.000006
  Max final entropy: 0.000006
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.4514486e-05 2.4514486e-05 2.4514486e-05 2.4514486e-05 2.4514486e-05
 2.4514486e-05 2.4514486e-05 2.4514486e-05]
  Max Min Entropy: 2.451448563078884e-05
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 29
seed: 20969
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.270 seconds.
  Mean final reward: -1.9374
  Mean return: -10.2702
  Mean final entropy: 0.0168
  Max final entropy: 0.0871
  Pseudo loss: -0.57696
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.276 seconds.
  Mean final reward: -1.1846
  Mean return: -6.9529
  Mean final entropy: 0.0194
  Max final entropy: 0.1301
  Pseudo loss: -2.22798
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.282 seconds.
  Mean final reward: -0.9589
  Mean return: -6.0421
  Mean final entropy: 0.0050
  Max final entropy: 0.0249
  Pseudo loss: -1.57904
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.273 seconds.
  Mean final reward: -0.9875
  Mean return: -5.8284
  Mean final entropy: 0.0057
  Max final entropy: 0.0196
  Pseudo loss: -2.57934
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (30/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2268
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: -0.25411
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.1854
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.34479892 0.34479892 0.34479892 0.34479892 0.34479892 0.34479892
 0.34479892 0.34479892]
  Mean final entropy: 0.000210
  Max final entropy: 0.000210
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.858189e-07 7.858189e-07 7.858189e-07 7.858189e-07 7.858189e-07
 7.858189e-07 7.858189e-07 7.858189e-07]
  Max Min Entropy: 7.858188837417401e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 30
seed: 22437
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.278 seconds.
  Mean final reward: -0.5362
  Mean return: -5.6240
  Mean final entropy: 0.0029
  Max final entropy: 0.0186
  Pseudo loss: -0.87707
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (10/100) took 0.266 seconds.
  Mean final reward: -0.9795
  Mean return: -5.7444
  Mean final entropy: 0.0049
  Max final entropy: 0.0199
  Pseudo loss: -0.29884
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.297 seconds.
  Mean final reward: -0.4588
  Mean return: -2.9584
  Mean final entropy: 0.0016
  Max final entropy: 0.0064
  Pseudo loss: -0.08378
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.4104
  Mean return: -3.2453
  Mean final entropy: 0.0015
  Max final entropy: 0.0071
  Pseudo loss: 0.18489
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (25/100) took 0.298 seconds.
  Mean final reward: -0.2189
  Mean return: -2.6989
  Mean final entropy: 0.0007
  Max final entropy: 0.0024
  Pseudo loss: -0.50458
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.298 seconds.
  Mean final reward: -0.1708
  Mean return: -2.6340
  Mean final entropy: 0.0005
  Max final entropy: 0.0039
  Pseudo loss: -0.98751
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.296 seconds.
  Mean final reward: -0.1216
  Mean return: -2.5925
  Mean final entropy: 0.0004
  Max final entropy: 0.0026
  Pseudo loss: -1.00098
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0172
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.05775
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.301 seconds.
  Mean final reward: -0.1659
  Mean return: -2.3320
  Mean final entropy: 0.0005
  Max final entropy: 0.0038
  Pseudo loss: -1.66470
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (55/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9652
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.33247346 0.33247346 0.33247346 0.33247346 0.33247346 0.33247346
 0.33247346 0.33247346]
  Mean final entropy: 0.000009
  Max final entropy: 0.000009
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.5916063e-06 2.5916063e-06 2.5916063e-06 2.5916063e-06 2.5916063e-06
 2.5916063e-06 2.5916063e-06 2.5916063e-06]
  Max Min Entropy: 2.5916062895703362e-06
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 31
seed: 24045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.265 seconds.
  Mean final reward: -2.6835
  Mean return: -11.7398
  Mean final entropy: 0.0686
  Max final entropy: 0.2829
  Pseudo loss: -0.97880
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.264 seconds.
  Mean final reward: -0.5712
  Mean return: -6.2544
  Mean final entropy: 0.0029
  Max final entropy: 0.0121
  Pseudo loss: -2.22796
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9237
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.34641
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8436
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.53988
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.8436
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.73156
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -4.6502
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.67555535 0.67555535 0.67555535 0.67555535 0.67555535 0.67555535
 0.67555535 0.67555535]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.144562e-07 1.144562e-07 1.144562e-07 1.144562e-07 1.144562e-07
 1.144562e-07 1.144562e-07 1.144562e-07]
  Max Min Entropy: 1.1445619918504235e-07
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 32
seed: 25799
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.285 seconds.
  Mean final reward: -0.8604
  Mean return: -6.2677
  Mean final entropy: 0.0359
  Max final entropy: 0.2826
  Pseudo loss: -1.41748
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.288 seconds.
  Mean final reward: -0.7848
  Mean return: -3.4494
  Mean final entropy: 0.0059
  Max final entropy: 0.0288
  Pseudo loss: -5.29791
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.6034973 0.6034973 0.6034973 0.6034973 0.6034973 0.6034973 0.6034973
 0.6034973]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.2240704e-07 7.2240704e-07 7.2240704e-07 7.2240704e-07 7.2240704e-07
 7.2240704e-07 7.2240704e-07 7.2240704e-07]
  Max Min Entropy: 7.224070373013092e-07
  Best actions: None

####################################


step: 33
seed: 27705
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.281 seconds.
  Mean final reward: -1.2033
  Mean return: -5.7265
  Mean final entropy: 0.0158
  Max final entropy: 0.0998
  Pseudo loss: -4.15289
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.285 seconds.
  Mean final reward: -0.5898
  Mean return: -4.2108
  Mean final entropy: 0.0029
  Max final entropy: 0.0108
  Pseudo loss: 0.21939
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.7297
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.57848
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.284 seconds.
  Mean final reward: -0.3019
  Mean return: -3.0316
  Mean final entropy: 0.0017
  Max final entropy: 0.0112
  Pseudo loss: -1.23027
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4271
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.11558147 0.11558147 0.11558147 0.11558147 0.11558147 0.11558147
 0.11558147 0.11558147]
  Mean final entropy: 0.000100
  Max final entropy: 0.000100
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.2882e-07 5.2882e-07 5.2882e-07 5.2882e-07 5.2882e-07 5.2882e-07
 5.2882e-07 5.2882e-07]
  Max Min Entropy: 5.28820010003983e-07
  Best actions: None

####################################


step: 34
seed: 29769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.264 seconds.
  Mean final reward: -2.2277
  Mean return: -10.0754
  Mean final entropy: 0.0431
  Max final entropy: 0.2276
  Pseudo loss: -0.46553
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1405
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.00254
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (15/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1380
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.00558
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1380
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.00886
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1380
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -0.00941
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.284 seconds.
  Mean final reward: -0.0063
  Mean return: -0.1468
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: -0.05303
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1405
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.01733
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1380
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: -0.01027
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1342
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.3668035 0.3668035 0.3668035 0.3668035 0.3668035 0.3668035 0.3668035
 0.3668035]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.9763209e-07 1.9763209e-07 1.9763209e-07 1.9763209e-07 1.9763209e-07
 1.9763209e-07 1.9763209e-07 1.9763209e-07]
  Max Min Entropy: 1.9763209024858952e-07
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [7, 7, 7, 7, 7, 7, 7, 7], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 35
seed: 31997
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -2.8156
  Mean return: -11.1828
  Mean final entropy: 0.0553
  Max final entropy: 0.1670
  Pseudo loss: 1.15451
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.274 seconds.
  Mean final reward: -2.0215
  Mean return: -9.1189
  Mean final entropy: 0.0177
  Max final entropy: 0.0591
  Pseudo loss: 1.04200
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.270 seconds.
  Mean final reward: -2.4567
  Mean return: -11.0017
  Mean final entropy: 0.0183
  Max final entropy: 0.0403
  Pseudo loss: 0.44093
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.266 seconds.
  Mean final reward: -0.8544
  Mean return: -7.8173
  Mean final entropy: 0.0064
  Max final entropy: 0.0339
  Pseudo loss: 0.13904
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.278 seconds.
  Mean final reward: -0.6523
  Mean return: -7.1515
  Mean final entropy: 0.0031
  Max final entropy: 0.0120
  Pseudo loss: -1.30978
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.282 seconds.
  Mean final reward: -1.1148
  Mean return: -7.1879
  Mean final entropy: 0.0047
  Max final entropy: 0.0191
  Pseudo loss: -1.88790
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -6.0125
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: -1.35649
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6680
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.65364
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.282 seconds.
  Mean final reward: -0.0315
  Mean return: -5.6995
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.38405
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Testing with greedy policy:
  Start entropy: [0.3115207 0.3115207 0.3115207 0.3115207 0.3115207 0.3115207 0.3115207
 0.3115207]
  Mean final entropy: 0.000169
  Max final entropy: 0.000169
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.132602e-05 2.132602e-05 2.132602e-05 2.132602e-05 2.132602e-05
 2.132602e-05 2.132602e-05 2.132602e-05]
  Max Min Entropy: 2.1326019123080187e-05
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [7, 7, 7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 36
seed: 34395
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.266 seconds.
  Mean final reward: -1.3526
  Mean return: -7.4341
  Mean final entropy: 0.0060
  Max final entropy: 0.0167
  Pseudo loss: 0.27892
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.274 seconds.
  Mean final reward: -0.7342
  Mean return: -5.1890
  Mean final entropy: 0.0025
  Max final entropy: 0.0060
  Pseudo loss: -1.01930
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.284 seconds.
  Mean final reward: -0.7972
  Mean return: -4.2351
  Mean final entropy: 0.0027
  Max final entropy: 0.0061
  Pseudo loss: -1.88135
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.285 seconds.
  Mean final reward: -0.1451
  Mean return: -2.3632
  Mean final entropy: 0.0006
  Max final entropy: 0.0032
  Pseudo loss: -0.22513
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1278
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -0.05522
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.8324
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.4040296 0.4040296 0.4040296 0.4040296 0.4040296 0.4040296 0.4040296
 0.4040296]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.4227547e-07 1.4227547e-07 1.4227547e-07 1.4227547e-07 1.4227547e-07
 1.4227547e-07 1.4227547e-07 1.4227547e-07]
  Max Min Entropy: 1.422754678515048e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [6, 6, 6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 37
seed: 36969
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.257 seconds.
  Mean final reward: -0.4436
  Mean return: -3.0935
  Mean final entropy: 0.0017
  Max final entropy: 0.0096
  Pseudo loss: -1.94432
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.17406163 0.17406163 0.17406163 0.17406163 0.17406163 0.17406163
 0.17406163 0.17406163]
  Mean final entropy: 0.000087
  Max final entropy: 0.000087
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.4421298e-10 2.4421298e-10 2.4421298e-10 2.4421298e-10 2.4421298e-10
 2.4421298e-10 2.4421298e-10 2.4421298e-10]
  Max Min Entropy: 2.442129831692341e-10
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 38
seed: 39725
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.284 seconds.
  Mean final reward: -0.6586
  Mean return: -4.5878
  Mean final entropy: 0.0038
  Max final entropy: 0.0205
  Pseudo loss: -0.50850
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9887
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -1.86256
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (15/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7476
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: -1.76123
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (20/100) took 0.280 seconds.
  Mean final reward: -0.1073
  Mean return: -0.5628
  Mean final entropy: 0.0007
  Max final entropy: 0.0012
  Pseudo loss: 0.18879
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.275 seconds.
  Mean final reward: -0.0644
  Mean return: -0.4615
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: 0.16520
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.276 seconds.
  Mean final reward: -0.0429
  Mean return: -0.3858
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: 0.08012
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.280 seconds.
  Mean final reward: -0.0587
  Mean return: -0.4587
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -0.00042
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (40/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.278 seconds.
  Mean final reward: -0.0286
  Mean return: -0.3452
  Mean final entropy: 0.0002
  Max final entropy: 0.0013
  Pseudo loss: -0.14777
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.280 seconds.
  Mean final reward: -0.0429
  Mean return: -0.3858
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.18922
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (65/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.280 seconds.
  Mean final reward: -0.0158
  Mean return: -0.3254
  Mean final entropy: 0.0001
  Max final entropy: 0.0011
  Pseudo loss: -0.11278
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.279 seconds.
  Mean final reward: -0.0158
  Mean return: -0.3254
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.11888
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.279 seconds.
  Mean final reward: -0.0286
  Mean return: -0.3452
  Mean final entropy: 0.0002
  Max final entropy: 0.0013
  Pseudo loss: -0.16098
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (95/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2845
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.43265915 0.43265915 0.43265915 0.43265915 0.43265915 0.43265915
 0.43265915 0.43265915]
  Mean final entropy: 0.000029
  Max final entropy: 0.000029
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.0056303e-07 5.0056303e-07 5.0056303e-07 5.0056303e-07 5.0056303e-07
 5.0056303e-07 5.0056303e-07 5.0056303e-07]
  Max Min Entropy: 5.005630328014377e-07
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 39
seed: 42669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.249 seconds.
  Mean final reward: -2.7356
  Mean return: -11.5897
  Mean final entropy: 0.0552
  Max final entropy: 0.1590
  Pseudo loss: -1.02540
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.359 seconds.
  Mean final reward: -1.8848
  Mean return: -9.3328
  Mean final entropy: 0.0544
  Max final entropy: 0.2582
  Pseudo loss: -8.26550
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.242 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5737
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.22778
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.229 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.232 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.230 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.231 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.232 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.232 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.232 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.230 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.229 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.231 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.229 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.231 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.230 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.232 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.231 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.231 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.239 seconds.
  Mean final reward: 0.0000
  Mean return: -5.5244
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.37509584 0.37509584 0.37509584 0.37509584 0.37509584 0.37509584
 0.37509584 0.37509584]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [0.00013786 0.00013786 0.00013786 0.00013786 0.00013786 0.00013786
 0.00013786 0.00013786]
  Max Min Entropy: 0.00013785679766442627
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 40
seed: 45807
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.267 seconds.
  Mean final reward: -1.7870
  Mean return: -10.0954
  Mean final entropy: 0.0134
  Max final entropy: 0.0503
  Pseudo loss: -1.15014
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.274 seconds.
  Mean final reward: -0.5159
  Mean return: -4.1260
  Mean final entropy: 0.0017
  Max final entropy: 0.0069
  Pseudo loss: -0.07146
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.287 seconds.
  Mean final reward: -0.8141
  Mean return: -4.0489
  Mean final entropy: 0.0027
  Max final entropy: 0.0073
  Pseudo loss: -0.59920
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.281 seconds.
  Mean final reward: -0.0117
  Mean return: -2.2365
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.30820
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.286 seconds.
  Mean final reward: -0.1336
  Mean return: -2.3983
  Mean final entropy: 0.0005
  Max final entropy: 0.0029
  Pseudo loss: -0.76099
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.290 seconds.
  Mean final reward: -0.1163
  Mean return: -2.5851
  Mean final entropy: 0.0003
  Max final entropy: 0.0025
  Pseudo loss: -3.09816
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.375 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0435
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.26599616 0.26599616 0.26599616 0.26599616 0.26599616 0.26599616
 0.26599616 0.26599616]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.7531987e-06 1.7531987e-06 1.7531987e-06 1.7531987e-06 1.7531987e-06
 1.7531987e-06 1.7531987e-06 1.7531987e-06]
  Max Min Entropy: 1.7531987168695196e-06
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [1, 1, 1, 1, 1, 1, 1, 1], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 41
seed: 49145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.4189
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.18246
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8391
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: -1.47191
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -0.4193
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: -1.37337
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (35/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0008
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.02893182 0.02893182 0.02893182 0.02893182 0.02893182 0.02893182
 0.02893182 0.02893182]
  Mean final entropy: 0.000764
  Max final entropy: 0.000764
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [-4.6105115e-09 -4.6105115e-09 -4.6105115e-09 -4.6105115e-09
 -4.6105115e-09 -4.6105115e-09 -4.6105115e-09 -4.6105115e-09]
  Max Min Entropy: -4.610511528113648e-09
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 42
seed: 52689
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.281 seconds.
  Mean final reward: -1.9859
  Mean return: -7.3711
  Mean final entropy: 0.0269
  Max final entropy: 0.1371
  Pseudo loss: -1.65770
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.000
Episode (10/100) took 0.286 seconds.
  Mean final reward: -0.1196
  Mean return: -0.4303
  Mean final entropy: 0.0007
  Max final entropy: 0.0026
  Pseudo loss: -0.39176
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0008
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0008
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0007
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0008
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.15813865 0.15813865 0.15813865 0.15813865 0.15813865 0.15813865
 0.15813865 0.15813865]
  Mean final entropy: 0.000817
  Max final entropy: 0.000817
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.06134515e-07 1.06134515e-07 1.06134515e-07 1.06134515e-07
 1.06134515e-07 1.06134515e-07 1.06134515e-07 1.06134515e-07]
  Max Min Entropy: 1.0613451451035871e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 43
seed: 56445
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.289 seconds.
  Mean final reward: -0.4698
  Mean return: -4.0780
  Mean final entropy: 0.0014
  Max final entropy: 0.0061
  Pseudo loss: -1.99093
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.279 seconds.
  Mean final reward: -0.3288
  Mean return: -2.6297
  Mean final entropy: 0.0011
  Max final entropy: 0.0030
  Pseudo loss: -4.93031
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.285 seconds.
  Mean final reward: -0.1501
  Mean return: -1.6932
  Mean final entropy: 0.0007
  Max final entropy: 0.0018
  Pseudo loss: 0.15994
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.288 seconds.
  Mean final reward: -0.1329
  Mean return: -1.4306
  Mean final entropy: 0.0006
  Max final entropy: 0.0018
  Pseudo loss: -0.02787
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.287 seconds.
  Mean final reward: -0.2533
  Mean return: -1.6052
  Mean final entropy: 0.0009
  Max final entropy: 0.0028
  Pseudo loss: -0.30522
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.287 seconds.
  Mean final reward: -0.1736
  Mean return: -1.5702
  Mean final entropy: 0.0008
  Max final entropy: 0.0016
  Pseudo loss: -0.34580
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (35/100) took 0.283 seconds.
  Mean final reward: -0.0750
  Mean return: -1.2515
  Mean final entropy: 0.0003
  Max final entropy: 0.0018
  Pseudo loss: -0.08870
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.287 seconds.
  Mean final reward: -0.1266
  Mean return: -1.3397
  Mean final entropy: 0.0005
  Max final entropy: 0.0028
  Pseudo loss: -0.30162
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0884
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.01402
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1090
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.01326
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2075
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.20345
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (60/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1043
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.01595
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (65/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1193
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00733
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (70/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0781
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00495
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (75/100) took 0.286 seconds.
  Mean final reward: -0.0750
  Mean return: -1.2666
  Mean final entropy: 0.0004
  Max final entropy: 0.0018
  Pseudo loss: -0.46716
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0884
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00924
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (85/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0931
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00270
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (90/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1137
  Mean final entropy: 0.0003
  Max final entropy: 0.0004
  Pseudo loss: 0.00349
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (95/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2337
  Mean final entropy: 0.0003
  Max final entropy: 0.0007
  Pseudo loss: -0.41632
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (100/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0884
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00205
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Testing with greedy policy:
  Start entropy: [0.17904934 0.17904934 0.17904934 0.17904934 0.17904934 0.17904934
 0.17904934 0.17904934]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.8679835e-06 2.8679835e-06 2.8679835e-06 2.8679835e-06 2.8679835e-06
 2.8679835e-06 2.8679835e-06 2.8679835e-06]
  Max Min Entropy: 2.8679835395450937e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 44
seed: 60419
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.281 seconds.
  Mean final reward: -0.0424
  Mean return: -6.0166
  Mean final entropy: 0.0003
  Max final entropy: 0.0014
  Pseudo loss: -0.61926
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (10/100) took 0.294 seconds.
  Mean final reward: -0.6502
  Mean return: -3.9163
  Mean final entropy: 0.0086
  Max final entropy: 0.0647
  Pseudo loss: -7.08283
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.27218127 0.27218127 0.27218127 0.27218127 0.27218127 0.27218127
 0.27218127 0.27218127]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.5470843e-07 2.5470843e-07 2.5470843e-07 2.5470843e-07 2.5470843e-07
 2.5470843e-07 2.5470843e-07 2.5470843e-07]
  Max Min Entropy: 2.547084250181797e-07
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 45
seed: 64617
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.277 seconds.
  Mean final reward: -0.2928
  Mean return: -4.6979
  Mean final entropy: 0.0010
  Max final entropy: 0.0050
  Pseudo loss: -0.09326
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.277 seconds.
  Mean final reward: -1.4155
  Mean return: -6.6026
  Mean final entropy: 0.0135
  Max final entropy: 0.0611
  Pseudo loss: -1.68147
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.273 seconds.
  Mean final reward: -0.8913
  Mean return: -5.8989
  Mean final entropy: 0.0047
  Max final entropy: 0.0216
  Pseudo loss: -0.77893
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.284 seconds.
  Mean final reward: -0.3957
  Mean return: -3.8660
  Mean final entropy: 0.0013
  Max final entropy: 0.0053
  Pseudo loss: -1.32039
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.280 seconds.
  Mean final reward: -0.1874
  Mean return: -3.1924
  Mean final entropy: 0.0006
  Max final entropy: 0.0045
  Pseudo loss: -4.70606
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (55/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4871
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.08181128 0.08181128 0.08181128 0.08181128 0.08181128 0.08181128
 0.08181128 0.08181128]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.6152952e-07 3.6152952e-07 3.6152952e-07 3.6152952e-07 3.6152952e-07
 3.6152952e-07 3.6152952e-07 3.6152952e-07]
  Max Min Entropy: 3.615295156578213e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 46
seed: 69045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.272 seconds.
  Mean final reward: -1.9508
  Mean return: -8.9771
  Mean final entropy: 0.0210
  Max final entropy: 0.0969
  Pseudo loss: -3.12953
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.274 seconds.
  Mean final reward: -1.1386
  Mean return: -3.7889
  Mean final entropy: 0.0239
  Max final entropy: 0.0951
  Pseudo loss: -11.50222
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.280 seconds.
  Mean final reward: -0.0081
  Mean return: -0.1964
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.02877
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.278 seconds.
  Mean final reward: -0.0081
  Mean return: -0.1783
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: -0.02677
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.278 seconds.
  Mean final reward: -0.0040
  Mean return: -0.1615
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.02002
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.0040
  Mean return: -0.1615
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.02368
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.279 seconds.
  Mean final reward: -0.0178
  Mean return: -0.1806
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -0.09364
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.280 seconds.
  Mean final reward: -0.0040
  Mean return: -0.1615
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.02627
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (55/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.280 seconds.
  Mean final reward: -0.0040
  Mean return: -0.1615
  Mean final entropy: 0.0002
  Max final entropy: 0.0010
  Pseudo loss: -0.03032
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.292 seconds.
  Mean final reward: -0.0040
  Mean return: -0.1615
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.03594
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1447
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.63256085 0.63256085 0.63256085 0.63256085 0.63256085 0.63256085
 0.63256085 0.63256085]
  Mean final entropy: 0.000158
  Max final entropy: 0.000158
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.813444e-08 9.813444e-08 9.813444e-08 9.813444e-08 9.813444e-08
 9.813444e-08 9.813444e-08 9.813444e-08]
  Max Min Entropy: 9.813444279416217e-08
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 47
seed: 73709
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.276 seconds.
  Mean final reward: -1.2974
  Mean return: -8.2783
  Mean final entropy: 0.0134
  Max final entropy: 0.0781
  Pseudo loss: 0.14343
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.266 seconds.
  Mean final reward: -1.2925
  Mean return: -6.9979
  Mean final entropy: 0.0076
  Max final entropy: 0.0309
  Pseudo loss: -1.04307
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.292 seconds.
  Mean final reward: -0.5017
  Mean return: -4.4595
  Mean final entropy: 0.0017
  Max final entropy: 0.0033
  Pseudo loss: -0.24354
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.281 seconds.
  Mean final reward: -0.9959
  Mean return: -5.0915
  Mean final entropy: 0.0033
  Max final entropy: 0.0078
  Pseudo loss: -0.70323
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (25/100) took 0.291 seconds.
  Mean final reward: -1.0321
  Mean return: -4.7185
  Mean final entropy: 0.0030
  Max final entropy: 0.0062
  Pseudo loss: 0.21701
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (30/100) took 0.294 seconds.
  Mean final reward: -0.6663
  Mean return: -4.0828
  Mean final entropy: 0.0020
  Max final entropy: 0.0036
  Pseudo loss: -0.68935
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (35/100) took 0.287 seconds.
  Mean final reward: -0.0960
  Mean return: -2.5591
  Mean final entropy: 0.0003
  Max final entropy: 0.0022
  Pseudo loss: -0.98366
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.285 seconds.
  Mean final reward: -0.2282
  Mean return: -2.7469
  Mean final entropy: 0.0008
  Max final entropy: 0.0062
  Pseudo loss: -3.98197
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.306 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2628
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.20007896 0.20007896 0.20007896 0.20007896 0.20007896 0.20007896
 0.20007896 0.20007896]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.4892945e-07 5.4892945e-07 5.4892945e-07 5.4892945e-07 5.4892945e-07
 5.4892945e-07 5.4892945e-07 5.4892945e-07]
  Max Min Entropy: 5.489294494509522e-07
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [3, 3, 3, 3, 3, 3, 3, 3], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 48
seed: 78615
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.277 seconds.
  Mean final reward: -1.8357
  Mean return: -10.7850
  Mean final entropy: 0.0321
  Max final entropy: 0.1251
  Pseudo loss: -0.75596
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.282 seconds.
  Mean final reward: -2.1003
  Mean return: -7.7412
  Mean final entropy: 0.0318
  Max final entropy: 0.1084
  Pseudo loss: -4.01768
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.270 seconds.
  Mean final reward: -0.6687
  Mean return: -2.9539
  Mean final entropy: 0.0019
  Max final entropy: 0.0037
  Pseudo loss: 0.07185
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.269 seconds.
  Mean final reward: -0.3674
  Mean return: -2.5308
  Mean final entropy: 0.0010
  Max final entropy: 0.0037
  Pseudo loss: -0.23949
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.278 seconds.
  Mean final reward: -0.2590
  Mean return: -2.0946
  Mean final entropy: 0.0009
  Max final entropy: 0.0037
  Pseudo loss: 0.00672
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.268 seconds.
  Mean final reward: -0.1680
  Mean return: -1.8686
  Mean final entropy: 0.0005
  Max final entropy: 0.0038
  Pseudo loss: -0.15018
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.267 seconds.
  Mean final reward: -0.1680
  Mean return: -1.6992
  Mean final entropy: 0.0005
  Max final entropy: 0.0038
  Pseudo loss: -0.65172
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.269 seconds.
  Mean final reward: -0.1652
  Mean return: -2.0008
  Mean final entropy: 0.0007
  Max final entropy: 0.0037
  Pseudo loss: -0.83644
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (45/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4304
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: -0.13203
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (50/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4304
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: -0.18906
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.269 seconds.
  Mean final reward: -0.0888
  Mean return: -1.7461
  Mean final entropy: 0.0003
  Max final entropy: 0.0020
  Pseudo loss: -0.92093
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3618
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5312
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.78903
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.6145026 0.6145026 0.6145026 0.6145026 0.6145026 0.6145026 0.6145026
 0.6145026]
  Mean final entropy: 0.000005
  Max final entropy: 0.000005
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [8.359958e-07 8.359958e-07 8.359958e-07 8.359958e-07 8.359958e-07
 8.359958e-07 8.359958e-07 8.359958e-07]
  Max Min Entropy: 8.359958201253903e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 49
seed: 83769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.276 seconds.
  Mean final reward: -1.3034
  Mean return: -8.3862
  Mean final entropy: 0.0087
  Max final entropy: 0.0438
  Pseudo loss: 0.33635
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.294 seconds.
  Mean final reward: -0.4717
  Mean return: -4.5331
  Mean final entropy: 0.0023
  Max final entropy: 0.0140
  Pseudo loss: -1.75049
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.16633151 0.16633151 0.16633151 0.16633151 0.16633151 0.16633151
 0.16633151 0.16633151]
  Mean final entropy: 0.000135
  Max final entropy: 0.000135
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.3338555e-06 1.3338555e-06 1.3338555e-06 1.3338555e-06 1.3338555e-06
 1.3338555e-06 1.3338555e-06 1.3338555e-06]
  Max Min Entropy: 1.3338554936126457e-06
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [1, 1, 1, 1, 1, 1, 1, 1], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 50
seed: 89177
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.266 seconds.
  Mean final reward: -1.3437
  Mean return: -9.0370
  Mean final entropy: 0.0294
  Max final entropy: 0.2032
  Pseudo loss: -0.08183
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.263 seconds.
  Mean final reward: -1.9678
  Mean return: -10.9040
  Mean final entropy: 0.0319
  Max final entropy: 0.2025
  Pseudo loss: -0.93724
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.307 seconds.
  Mean final reward: -2.6107
  Mean return: -11.9954
  Mean final entropy: 0.0447
  Max final entropy: 0.1512
  Pseudo loss: 2.45576
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.264 seconds.
  Mean final reward: -1.2754
  Mean return: -8.7995
  Mean final entropy: 0.0097
  Max final entropy: 0.0508
  Pseudo loss: -1.61334
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.277 seconds.
  Mean final reward: -2.1700
  Mean return: -10.4469
  Mean final entropy: 0.0366
  Max final entropy: 0.1644
  Pseudo loss: -1.96554
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (30/100) took 0.275 seconds.
  Mean final reward: -0.8499
  Mean return: -8.2623
  Mean final entropy: 0.0094
  Max final entropy: 0.0599
  Pseudo loss: -3.92548
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9811
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.06906
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9811
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.06924
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.244 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7637
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00607
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8071
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.07130
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8071
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.13826
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7637
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.11233
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7637
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.13612
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.244 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.249 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7202
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.6451175 0.6451175 0.6451175 0.6451175 0.6451175 0.6451175 0.6451175
 0.6451175]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.668084e-05 5.668084e-05 5.668084e-05 5.668084e-05 5.668084e-05
 5.668084e-05 5.668084e-05 5.668084e-05]
  Max Min Entropy: 5.668084122589789e-05
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 51
seed: 94845
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.272 seconds.
  Mean final reward: -2.4429
  Mean return: -10.3487
  Mean final entropy: 0.0391
  Max final entropy: 0.1841
  Pseudo loss: -0.73002
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.266 seconds.
  Mean final reward: -0.6825
  Mean return: -6.7041
  Mean final entropy: 0.0030
  Max final entropy: 0.0144
  Pseudo loss: -3.14511
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.261 seconds.
  Mean final reward: -1.3305
  Mean return: -5.3379
  Mean final entropy: 0.0072
  Max final entropy: 0.0144
  Pseudo loss: -1.31446
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.259 seconds.
  Mean final reward: -0.3334
  Mean return: -3.3403
  Mean final entropy: 0.0018
  Max final entropy: 0.0144
  Pseudo loss: -1.19973
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6731
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5827787 0.5827787 0.5827787 0.5827787 0.5827787 0.5827787 0.5827787
 0.5827787]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0369191e-07 2.0369191e-07 2.0369191e-07 2.0369191e-07 2.0369191e-07
 2.0369191e-07 2.0369191e-07 2.0369191e-07]
  Max Min Entropy: 2.036919113379554e-07
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 52
seed: 100779
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.272 seconds.
  Mean final reward: -1.0159
  Mean return: -6.9716
  Mean final entropy: 0.0163
  Max final entropy: 0.0921
  Pseudo loss: -2.28970
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -2.3008
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -5.77436
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.4332286 0.4332286 0.4332286 0.4332286 0.4332286 0.4332286 0.4332286
 0.4332286]
  Mean final entropy: 0.000026
  Max final entropy: 0.000026
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [-2.5029772e-08 -2.5029772e-08 -2.5029772e-08 -2.5029772e-08
 -2.5029772e-08 -2.5029772e-08 -2.5029772e-08 -2.5029772e-08]
  Max Min Entropy: -2.5029772032780784e-08
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 53
seed: 106985
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.256 seconds.
  Mean final reward: -0.6576
  Mean return: -4.0284
  Mean final entropy: 0.0040
  Max final entropy: 0.0230
  Pseudo loss: -1.63479
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.246 seconds.
  Mean final reward: -0.0456
  Mean return: -0.3207
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.01408
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.245 seconds.
  Mean final reward: -0.0456
  Mean return: -0.3207
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -0.03351
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.247 seconds.
  Mean final reward: -0.0422
  Mean return: -0.3207
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.18556
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.247 seconds.
  Mean final reward: -0.0255
  Mean return: -0.2770
  Mean final entropy: 0.0004
  Max final entropy: 0.0012
  Pseudo loss: -0.05876
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.249 seconds.
  Mean final reward: -0.0456
  Mean return: -0.3207
  Mean final entropy: 0.0007
  Max final entropy: 0.0012
  Pseudo loss: -0.22580
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.248 seconds.
  Mean final reward: -0.0201
  Mean return: -0.2682
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.11671
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.245 seconds.
  Mean final reward: -0.0401
  Mean return: -0.3119
  Mean final entropy: 0.0007
  Max final entropy: 0.0012
  Pseudo loss: -0.26266
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (55/100) took 0.247 seconds.
  Mean final reward: -0.0255
  Mean return: -0.2770
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.20145
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0005
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.250 seconds.
  Mean final reward: -0.0255
  Mean return: -0.2770
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -0.20694
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.244 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2244
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.245 seconds.
  Mean final reward: -0.0201
  Mean return: -0.2682
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.19197
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.33081377 0.33081377 0.33081377 0.33081377 0.33081377 0.33081377
 0.33081377 0.33081377]
  Mean final entropy: 0.000494
  Max final entropy: 0.000494
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.038217e-07 3.038217e-07 3.038217e-07 3.038217e-07 3.038217e-07
 3.038217e-07 3.038217e-07 3.038217e-07]
  Max Min Entropy: 3.0382170734810643e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 54
seed: 113469
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.273 seconds.
  Mean final reward: -1.0021
  Mean return: -7.0448
  Mean final entropy: 0.0041
  Max final entropy: 0.0107
  Pseudo loss: -0.33974
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.282 seconds.
  Mean final reward: -0.3802
  Mean return: -5.8026
  Mean final entropy: 0.0016
  Max final entropy: 0.0098
  Pseudo loss: -1.10196
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.270 seconds.
  Mean final reward: -0.5597
  Mean return: -5.5252
  Mean final entropy: 0.0044
  Max final entropy: 0.0312
  Pseudo loss: -0.80167
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.270 seconds.
  Mean final reward: -0.4182
  Mean return: -5.1625
  Mean final entropy: 0.0036
  Max final entropy: 0.0284
  Pseudo loss: -1.47489
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.276 seconds.
  Mean final reward: -0.4247
  Mean return: -4.9829
  Mean final entropy: 0.0038
  Max final entropy: 0.0299
  Pseudo loss: -2.86218
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2494
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.25551
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2078
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: -0.04472
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (45/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2986
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -4.2412
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.36442
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.10937814 0.10937814 0.10937814 0.10937814 0.10937814 0.10937814
 0.10937814 0.10937814]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.2459938e-06 4.2459938e-06 4.2459938e-06 4.2459938e-06 4.2459938e-06
 4.2459938e-06 4.2459938e-06 4.2459938e-06]
  Max Min Entropy: 4.2459937503736e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [7, 7, 7, 7, 7, 7, 7, 7], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 55
seed: 120237
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.270 seconds.
  Mean final reward: -0.5583
  Mean return: -5.2166
  Mean final entropy: 0.0018
  Max final entropy: 0.0053
  Pseudo loss: -0.60822
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.258 seconds.
  Mean final reward: -0.8909
  Mean return: -4.0746
  Mean final entropy: 0.0103
  Max final entropy: 0.0721
  Pseudo loss: -3.17191
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.260 seconds.
  Mean final reward: -0.0977
  Mean return: -1.3116
  Mean final entropy: 0.0004
  Max final entropy: 0.0022
  Pseudo loss: -0.45238
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.261 seconds.
  Mean final reward: -0.1152
  Mean return: -1.3136
  Mean final entropy: 0.0006
  Max final entropy: 0.0022
  Pseudo loss: -0.98292
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8639
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.16298
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8933
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.51400
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8088
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: -0.08204
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (45/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8639
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: -0.34496
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8142
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.11618
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (75/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7872
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.02331
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (80/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7818
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.119811 0.119811 0.119811 0.119811 0.119811 0.119811 0.119811 0.119811]
  Mean final entropy: 0.000008
  Max final entropy: 0.000008
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.4464215e-06 1.4464215e-06 1.4464215e-06 1.4464215e-06 1.4464215e-06
 1.4464215e-06 1.4464215e-06 1.4464215e-06]
  Max Min Entropy: 1.4464214928011643e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [7, 7, 7, 7, 7, 7, 7, 7], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 56
seed: 127295
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.266 seconds.
  Mean final reward: -1.1090
  Mean return: -6.9187
  Mean final entropy: 0.0079
  Max final entropy: 0.0472
  Pseudo loss: -0.11386
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.283 seconds.
  Mean final reward: -1.5939
  Mean return: -8.9121
  Mean final entropy: 0.0120
  Max final entropy: 0.0593
  Pseudo loss: -0.77851
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.279 seconds.
  Mean final reward: -1.2028
  Mean return: -7.3266
  Mean final entropy: 0.0249
  Max final entropy: 0.1422
  Pseudo loss: -2.55038
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -4.9815
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.42645
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.282 seconds.
  Mean final reward: -0.4845
  Mean return: -5.7920
  Mean final entropy: 0.0062
  Max final entropy: 0.0482
  Pseudo loss: -1.67471
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.236 seconds.
  Mean final reward: -0.3330
  Mean return: -5.4845
  Mean final entropy: 0.0018
  Max final entropy: 0.0144
  Pseudo loss: -2.82330
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.235 seconds.
  Mean final reward: 0.0000
  Mean return: -4.7425
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.03519
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9493
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.18961
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -3.9493
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.73588
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7907
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.19466315 0.19466315 0.19466315 0.19466315 0.19466315 0.19466315
 0.19466315 0.19466315]
  Mean final entropy: 0.000006
  Max final entropy: 0.000006
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.802027e-05 6.802027e-05 6.802027e-05 6.802027e-05 6.802027e-05
 6.802027e-05 6.802027e-05 6.802027e-05]
  Max Min Entropy: 6.802027201047167e-05
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 57
seed: 134649
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.255 seconds.
  Mean final reward: -1.5607
  Mean return: -9.0065
  Mean final entropy: 0.0144
  Max final entropy: 0.0498
  Pseudo loss: -0.72709
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.256 seconds.
  Mean final reward: -1.6577
  Mean return: -7.5411
  Mean final entropy: 0.0115
  Max final entropy: 0.0325
  Pseudo loss: -0.26394
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.256 seconds.
  Mean final reward: -0.4020
  Mean return: -4.4136
  Mean final entropy: 0.0015
  Max final entropy: 0.0064
  Pseudo loss: -0.70041
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (20/100) took 0.285 seconds.
  Mean final reward: -0.4309
  Mean return: -2.1303
  Mean final entropy: 0.0018
  Max final entropy: 0.0087
  Pseudo loss: -0.87207
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9884
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00713
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (30/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9884
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: -0.04614
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (35/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8746
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.01472
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8940
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.01190
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (45/100) took 0.285 seconds.
  Mean final reward: -0.0619
  Mean return: -1.0386
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.26804
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8940
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: -0.02516
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8551
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.02605
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8357
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8746
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.03729
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8357
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.283 seconds.
  Mean final reward: -0.0025
  Mean return: -0.8577
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: -0.05235
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8551
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.05071
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8357
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8357
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8551
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.05855
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8357
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.17210087 0.17210087 0.17210087 0.17210087 0.17210087 0.17210087
 0.17210087 0.17210087]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.76024e-07 5.76024e-07 5.76024e-07 5.76024e-07 5.76024e-07 5.76024e-07
 5.76024e-07 5.76024e-07]
  Max Min Entropy: 5.760239787377941e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 58
seed: 142305
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.272 seconds.
  Mean final reward: -0.9682
  Mean return: -5.6532
  Mean final entropy: 0.0099
  Max final entropy: 0.0538
  Pseudo loss: -0.61125
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.270 seconds.
  Mean final reward: -0.8429
  Mean return: -4.3844
  Mean final entropy: 0.0043
  Max final entropy: 0.0165
  Pseudo loss: -3.92389
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.07692181 0.07692181 0.07692181 0.07692181 0.07692181 0.07692181
 0.07692181 0.07692181]
  Mean final entropy: 0.000251
  Max final entropy: 0.000251
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.291831e-07 9.291831e-07 9.291831e-07 9.291831e-07 9.291831e-07
 9.291831e-07 9.291831e-07 9.291831e-07]
  Max Min Entropy: 9.291831020163954e-07
  Best actions: None

####################################


step: 59
seed: 150269
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.266 seconds.
  Mean final reward: -0.5806
  Mean return: -4.0104
  Mean final entropy: 0.0048
  Max final entropy: 0.0348
  Pseudo loss: 0.13048
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.272 seconds.
  Mean final reward: -0.2855
  Mean return: -3.3532
  Mean final entropy: 0.0009
  Max final entropy: 0.0043
  Pseudo loss: -1.18333
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.266 seconds.
  Mean final reward: -0.5371
  Mean return: -3.3945
  Mean final entropy: 0.0022
  Max final entropy: 0.0088
  Pseudo loss: -2.23356
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.07091026 0.07091026 0.07091026 0.07091026 0.07091026 0.07091026
 0.07091026 0.07091026]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0251231e-07 2.0251231e-07 2.0251231e-07 2.0251231e-07 2.0251231e-07
 2.0251231e-07 2.0251231e-07 2.0251231e-07]
  Max Min Entropy: 2.0251231092061062e-07
  Best actions: None

####################################


step: 60
seed: 158547
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.262 seconds.
  Mean final reward: -2.5511
  Mean return: -11.5195
  Mean final entropy: 0.0425
  Max final entropy: 0.1242
  Pseudo loss: -1.12653
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.259 seconds.
  Mean final reward: -2.3902
  Mean return: -10.5848
  Mean final entropy: 0.0463
  Max final entropy: 0.1209
  Pseudo loss: -2.23138
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.260 seconds.
  Mean final reward: -0.6919
  Mean return: -5.8866
  Mean final entropy: 0.0048
  Max final entropy: 0.0297
  Pseudo loss: -0.14413
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.257 seconds.
  Mean final reward: -0.0101
  Mean return: -4.8365
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.71986
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (25/100) took 0.247 seconds.
  Mean final reward: -0.2424
  Mean return: -5.0210
  Mean final entropy: 0.0009
  Max final entropy: 0.0070
  Pseudo loss: -2.70457
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (30/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0002
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.237 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.237 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0001
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.237 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.240 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.240 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.239 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.241 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.241 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.237 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.242 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.239 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.238 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.240 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.238 seconds.
  Mean final reward: 0.0000
  Mean return: -4.3531
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.39378446 0.39378446 0.39378446 0.39378446 0.39378446 0.39378446
 0.39378446 0.39378446]
  Mean final entropy: 0.000023
  Max final entropy: 0.000023
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.6497427e-06 3.6497427e-06 3.6497427e-06 3.6497427e-06 3.6497427e-06
 3.6497427e-06 3.6497427e-06 3.6497427e-06]
  Max Min Entropy: 3.649742666311795e-06
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8, 8, 8], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 61
seed: 167145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.280 seconds.
  Mean final reward: -1.0773
  Mean return: -8.7546
  Mean final entropy: 0.0182
  Max final entropy: 0.1224
  Pseudo loss: -0.09513
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.280 seconds.
  Mean final reward: -1.2502
  Mean return: -8.4265
  Mean final entropy: 0.0155
  Max final entropy: 0.0648
  Pseudo loss: -4.33668
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -5.7966
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: 0.32656
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8142
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.65383
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.306 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0002
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -5.9248
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5155014 0.5155014 0.5155014 0.5155014 0.5155014 0.5155014 0.5155014
 0.5155014]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.3562518e-06 2.3562518e-06 2.3562518e-06 2.3562518e-06 2.3562518e-06
 2.3562518e-06 2.3562518e-06 2.3562518e-06]
  Max Min Entropy: 2.3562517981190467e-06
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 62
seed: 176069
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.263 seconds.
  Mean final reward: -2.2355
  Mean return: -11.1178
  Mean final entropy: 0.0492
  Max final entropy: 0.2740
  Pseudo loss: 0.02880
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.272 seconds.
  Mean final reward: -0.6717
  Mean return: -4.4205
  Mean final entropy: 0.0080
  Max final entropy: 0.0592
  Pseudo loss: -8.17912
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.280 seconds.
  Mean final reward: -0.1321
  Mean return: -1.7336
  Mean final entropy: 0.0006
  Max final entropy: 0.0020
  Pseudo loss: -0.15795
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.278 seconds.
  Mean final reward: -0.1627
  Mean return: -1.8606
  Mean final entropy: 0.0007
  Max final entropy: 0.0020
  Pseudo loss: -0.35865
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.279 seconds.
  Mean final reward: -0.0114
  Mean return: -1.4267
  Mean final entropy: 0.0002
  Max final entropy: 0.0011
  Pseudo loss: -0.13423
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.279 seconds.
  Mean final reward: -0.0114
  Mean return: -1.4061
  Mean final entropy: 0.0001
  Max final entropy: 0.0011
  Pseudo loss: -0.19854
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3271
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.09154
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (55/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3271
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: -0.09977
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (75/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3066
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.56107634 0.56107634 0.56107634 0.56107634 0.56107634 0.56107634
 0.56107634 0.56107634]
  Mean final entropy: 0.000011
  Max final entropy: 0.000011
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.034029e-06 9.034029e-06 9.034029e-06 9.034029e-06 9.034029e-06
 9.034029e-06 9.034029e-06 9.034029e-06]
  Max Min Entropy: 9.034029062604532e-06
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 63
seed: 185325
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.282 seconds.
  Mean final reward: -0.4023
  Mean return: -3.4058
  Mean final entropy: 0.0033
  Max final entropy: 0.0250
  Pseudo loss: -2.23277
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.5915993 0.5915993 0.5915993 0.5915993 0.5915993 0.5915993 0.5915993
 0.5915993]
  Mean final entropy: 0.000543
  Max final entropy: 0.000543
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.848585e-08 6.848585e-08 6.848585e-08 6.848585e-08 6.848585e-08
 6.848585e-08 6.848585e-08 6.848585e-08]
  Max Min Entropy: 6.848585343277591e-08
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [3, 3, 3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 64
seed: 194919
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.266 seconds.
  Mean final reward: -1.7167
  Mean return: -7.9823
  Mean final entropy: 0.0105
  Max final entropy: 0.0393
  Pseudo loss: 0.88609
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.000
Episode (10/100) took 0.267 seconds.
  Mean final reward: -1.5556
  Mean return: -7.8161
  Mean final entropy: 0.0097
  Max final entropy: 0.0240
  Pseudo loss: -0.81606
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.260 seconds.
  Mean final reward: -0.8975
  Mean return: -4.1159
  Mean final entropy: 0.0121
  Max final entropy: 0.0800
  Pseudo loss: -10.99255
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.39248866 0.39248866 0.39248866 0.39248866 0.39248866 0.39248866
 0.39248866 0.39248866]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.8558163e-07 5.8558163e-07 5.8558163e-07 5.8558163e-07 5.8558163e-07
 5.8558163e-07 5.8558163e-07 5.8558163e-07]
  Max Min Entropy: 5.855816311850504e-07
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 65
seed: 204857
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.281 seconds.
  Mean final reward: -0.0862
  Mean return: -1.7725
  Mean final entropy: 0.0006
  Max final entropy: 0.0015
  Pseudo loss: -0.46427
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.287 seconds.
  Mean final reward: -0.1983
  Mean return: -1.4322
  Mean final entropy: 0.0008
  Max final entropy: 0.0036
  Pseudo loss: -4.25217
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.294 seconds.
  Mean final reward: -0.0191
  Mean return: -0.2164
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: 0.02735
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.296 seconds.
  Mean final reward: -0.0370
  Mean return: -0.2336
  Mean final entropy: 0.0004
  Max final entropy: 0.0012
  Pseudo loss: 0.07986
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.292 seconds.
  Mean final reward: -0.0375
  Mean return: -0.2346
  Mean final entropy: 0.0004
  Max final entropy: 0.0012
  Pseudo loss: 0.03144
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.297 seconds.
  Mean final reward: -0.0381
  Mean return: -0.2355
  Mean final entropy: 0.0004
  Max final entropy: 0.0012
  Pseudo loss: 0.02394
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1588
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1588
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.298 seconds.
  Mean final reward: -0.0375
  Mean return: -0.2346
  Mean final entropy: 0.0004
  Max final entropy: 0.0012
  Pseudo loss: -0.00541
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (50/100) took 0.297 seconds.
  Mean final reward: -0.0375
  Mean return: -0.2346
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -0.00254
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (55/100) took 0.301 seconds.
  Mean final reward: -0.0191
  Mean return: -0.2164
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.00942
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.297 seconds.
  Mean final reward: -0.0566
  Mean return: -0.2729
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.00921
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (65/100) took 0.299 seconds.
  Mean final reward: -0.0375
  Mean return: -0.2346
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -0.05088
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (70/100) took 0.296 seconds.
  Mean final reward: -0.0185
  Mean return: -0.1962
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.00438
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (75/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1588
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.298 seconds.
  Mean final reward: -0.0191
  Mean return: -0.1972
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -0.03609
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1588
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1588
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.298 seconds.
  Mean final reward: -0.0191
  Mean return: -0.1972
  Mean final entropy: 0.0002
  Max final entropy: 0.0012
  Pseudo loss: -0.03406
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.294 seconds.
  Mean final reward: -0.0191
  Mean return: -0.1972
  Mean final entropy: 0.0003
  Max final entropy: 0.0012
  Pseudo loss: -0.04224
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Testing with greedy policy:
  Start entropy: [0.3779992 0.3779992 0.3779992 0.3779992 0.3779992 0.3779992 0.3779992
 0.3779992]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [-1.2811951e-07 -1.2811951e-07 -1.2811951e-07 -1.2811951e-07
 -1.2811951e-07 -1.2811951e-07 -1.2811951e-07 -1.2811951e-07]
  Max Min Entropy: -1.2811950966806762e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 66
seed: 215145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.265 seconds.
  Mean final reward: -1.2822
  Mean return: -8.2938
  Mean final entropy: 0.0088
  Max final entropy: 0.0295
  Pseudo loss: -1.07513
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.325 seconds.
  Mean final reward: -0.7998
  Mean return: -5.6687
  Mean final entropy: 0.0063
  Max final entropy: 0.0365
  Pseudo loss: -2.59977
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -3.7984
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.61409
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.6704
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.33335924 0.33335924 0.33335924 0.33335924 0.33335924 0.33335924
 0.33335924 0.33335924]
  Mean final entropy: 0.000008
  Max final entropy: 0.000008
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.7696974e-06 2.7696974e-06 2.7696974e-06 2.7696974e-06 2.7696974e-06
 2.7696974e-06 2.7696974e-06 2.7696974e-06]
  Max Min Entropy: 2.769697402982274e-06
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 67
seed: 225789
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.264 seconds.
  Mean final reward: -0.2060
  Mean return: -4.0703
  Mean final entropy: 0.0009
  Max final entropy: 0.0052
  Pseudo loss: -0.69771
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.254 seconds.
  Mean final reward: -0.4725
  Mean return: -1.5357
  Mean final entropy: 0.0056
  Max final entropy: 0.0438
  Pseudo loss: -5.09376
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.251 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.250 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.17928982 0.17928982 0.17928982 0.17928982 0.17928982 0.17928982
 0.17928982 0.17928982]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.0254804e-08 2.0254804e-08 2.0254804e-08 2.0254804e-08 2.0254804e-08
 2.0254804e-08 2.0254804e-08 2.0254804e-08]
  Max Min Entropy: 2.025480405620783e-08
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 68
seed: 236795
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.278 seconds.
  Mean final reward: -0.8236
  Mean return: -8.7729
  Mean final entropy: 0.0087
  Max final entropy: 0.0598
  Pseudo loss: -1.27505
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.0184
  Mean return: -8.8656
  Mean final entropy: 0.0085
  Max final entropy: 0.0523
  Pseudo loss: -1.65899
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.259 seconds.
  Mean final reward: -0.7684
  Mean return: -7.7387
  Mean final entropy: 0.0063
  Max final entropy: 0.0426
  Pseudo loss: -3.52740
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.265 seconds.
  Mean final reward: -0.3471
  Mean return: -6.9083
  Mean final entropy: 0.0011
  Max final entropy: 0.0044
  Pseudo loss: -2.70119
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (25/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (30/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1904
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00947
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -6.1920
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.62226015 0.62226015 0.62226015 0.62226015 0.62226015 0.62226015
 0.62226015 0.62226015]
  Mean final entropy: 0.000161
  Max final entropy: 0.000161
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [0.0001408 0.0001408 0.0001408 0.0001408 0.0001408 0.0001408 0.0001408
 0.0001408]
  Max Min Entropy: 0.00014079821994528174
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 69
seed: 248169
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.284 seconds.
  Mean final reward: -2.2231
  Mean return: -10.8836
  Mean final entropy: 0.0305
  Max final entropy: 0.1016
  Pseudo loss: -1.06432
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.247 seconds.
  Mean final reward: -3.3040
  Mean return: -13.2321
  Mean final entropy: 0.0585
  Max final entropy: 0.1311
  Pseudo loss: -0.92659
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.265 seconds.
  Mean final reward: -2.3670
  Mean return: -11.1898
  Mean final entropy: 0.0288
  Max final entropy: 0.0721
  Pseudo loss: -1.74028
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.266 seconds.
  Mean final reward: -1.8216
  Mean return: -9.0767
  Mean final entropy: 0.0191
  Max final entropy: 0.0617
  Pseudo loss: -2.52860
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (25/100) took 0.246 seconds.
  Mean final reward: -1.0934
  Mean return: -8.4419
  Mean final entropy: 0.0090
  Max final entropy: 0.0378
  Pseudo loss: -3.92065
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.250 seconds.
  Mean final reward: -0.0473
  Mean return: -6.1831
  Mean final entropy: 0.0002
  Max final entropy: 0.0015
  Pseudo loss: -1.70950
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (35/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -5.3542
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.14682
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -5.2424
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5176791 0.5176791 0.5176791 0.5176791 0.5176791 0.5176791 0.5176791
 0.5176791]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.5221617e-06 5.5221617e-06 5.5221617e-06 5.5221617e-06 5.5221617e-06
 5.5221617e-06 5.5221617e-06 5.5221617e-06]
  Max Min Entropy: 5.522161700355355e-06
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 70
seed: 259917
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.257 seconds.
  Mean final reward: -3.0771
  Mean return: -11.8089
  Mean final entropy: 0.0683
  Max final entropy: 0.2486
  Pseudo loss: -1.43843
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.275 seconds.
  Mean final reward: -2.8677
  Mean return: -10.8146
  Mean final entropy: 0.0581
  Max final entropy: 0.2021
  Pseudo loss: -4.00779
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.287 seconds.
  Mean final reward: -1.5888
  Mean return: -5.7207
  Mean final entropy: 0.0079
  Max final entropy: 0.0127
  Pseudo loss: 0.28653
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.285 seconds.
  Mean final reward: -0.3176
  Mean return: -3.1782
  Mean final entropy: 0.0016
  Max final entropy: 0.0127
  Pseudo loss: -1.48589
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5429
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5249351 0.5249351 0.5249351 0.5249351 0.5249351 0.5249351 0.5249351
 0.5249351]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [-2.9043047e-08 -2.9043047e-08 -2.9043047e-08 -2.9043047e-08
 -2.9043047e-08 -2.9043047e-08 -2.9043047e-08 -2.9043047e-08]
  Max Min Entropy: -2.9043047078403106e-08
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8, 8, 8], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 71
seed: 272045
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -1.3423
  Mean return: -4.7294
  Mean final entropy: 0.0170
  Max final entropy: 0.0809
  Pseudo loss: -5.55206
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.294 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2473
  Mean final entropy: 0.0008
  Max final entropy: 0.0011
  Pseudo loss: -0.02364
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (15/100) took 0.296 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2483
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: -0.03358
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (20/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2143
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: 0.00464
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (25/100) took 0.293 seconds.
  Mean final reward: -0.0284
  Mean return: -0.2781
  Mean final entropy: 0.0008
  Max final entropy: 0.0011
  Pseudo loss: -0.07038
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.295 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2462
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: -0.03630
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Episode (35/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2143
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: 0.00487
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (40/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2154
  Mean final entropy: 0.0008
  Max final entropy: 0.0009
  Pseudo loss: 0.00230
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (45/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2154
  Mean final entropy: 0.0008
  Max final entropy: 0.0010
  Pseudo loss: 0.00275
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (50/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2154
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00239
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (55/100) took 0.292 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2483
  Mean final entropy: 0.0009
  Max final entropy: 0.0011
  Pseudo loss: -0.09200
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (60/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2154
  Mean final entropy: 0.0008
  Max final entropy: 0.0009
  Pseudo loss: 0.00242
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (65/100) took 0.296 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2473
  Mean final entropy: 0.0008
  Max final entropy: 0.0011
  Pseudo loss: -0.08306
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (70/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2164
  Mean final entropy: 0.0009
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2164
  Mean final entropy: 0.0009
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.295 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2483
  Mean final entropy: 0.0008
  Max final entropy: 0.0011
  Pseudo loss: -0.09253
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (85/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2164
  Mean final entropy: 0.0009
  Max final entropy: 0.0010
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2154
  Mean final entropy: 0.0006
  Max final entropy: 0.0009
  Pseudo loss: 0.00292
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (95/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2123
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: 0.00669
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (100/100) took 0.298 seconds.
  Mean final reward: -0.0142
  Mean return: -0.2462
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: -0.06960
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.500
Testing with greedy policy:
  Start entropy: [0.24246481 0.24246481 0.24246481 0.24246481 0.24246481 0.24246481
 0.24246481 0.24246481]
  Mean final entropy: 0.000856
  Max final entropy: 0.000856
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.2112653e-06 1.2112653e-06 1.2112653e-06 1.2112653e-06 1.2112653e-06
 1.2112653e-06 1.2112653e-06 1.2112653e-06]
  Max Min Entropy: 1.2112652711948613e-06
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5, 5, 5], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 72
seed: 284559
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.281 seconds.
  Mean final reward: -0.3464
  Mean return: -3.1665
  Mean final entropy: 0.0019
  Max final entropy: 0.0129
  Pseudo loss: -0.90570
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.289 seconds.
  Mean final reward: -0.4476
  Mean return: -1.6629
  Mean final entropy: 0.0047
  Max final entropy: 0.0359
  Pseudo loss: -1.08400
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2547
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.22954
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (20/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.20558178 0.20558178 0.20558178 0.20558178 0.20558178 0.20558178
 0.20558178 0.20558178]
  Mean final entropy: 0.000191
  Max final entropy: 0.000191
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.095005e-08 7.095005e-08 7.095005e-08 7.095005e-08 7.095005e-08
 7.095005e-08 7.095005e-08 7.095005e-08]
  Max Min Entropy: 7.095005116752873e-08
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 73
seed: 297465
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.279 seconds.
  Mean final reward: -0.2971
  Mean return: -5.5023
  Mean final entropy: 0.0015
  Max final entropy: 0.0108
  Pseudo loss: -1.66373
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (10/100) took 0.283 seconds.
  Mean final reward: -1.5506
  Mean return: -8.1882
  Mean final entropy: 0.0383
  Max final entropy: 0.2763
  Pseudo loss: -7.08963
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1365
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: -0.06564
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1943
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.13251
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2520
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.18697
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1365
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.09936
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1365
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.16069
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1365
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.23424
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1365
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.26925
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -2.0788
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.29570448 0.29570448 0.29570448 0.29570448 0.29570448 0.29570448
 0.29570448 0.29570448]
  Mean final entropy: 0.000020
  Max final entropy: 0.000020
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.966313e-07 7.966313e-07 7.966313e-07 7.966313e-07 7.966313e-07
 7.966313e-07 7.966313e-07 7.966313e-07]
  Max Min Entropy: 7.966312978169299e-07
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [2, 2, 2, 2, 2, 2, 2, 2], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 74
seed: 310769
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.279 seconds.
  Mean final reward: -1.0772
  Mean return: -7.7634
  Mean final entropy: 0.0156
  Max final entropy: 0.1064
  Pseudo loss: 2.19730
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.278 seconds.
  Mean final reward: -0.9737
  Mean return: -7.5025
  Mean final entropy: 0.0039
  Max final entropy: 0.0092
  Pseudo loss: 0.00672
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (15/100) took 0.288 seconds.
  Mean final reward: -1.0989
  Mean return: -6.3890
  Mean final entropy: 0.0170
  Max final entropy: 0.1178
  Pseudo loss: -0.45113
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.285 seconds.
  Mean final reward: -0.5768
  Mean return: -3.2953
  Mean final entropy: 0.0020
  Max final entropy: 0.0046
  Pseudo loss: 0.83627
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (25/100) took 0.283 seconds.
  Mean final reward: -0.5551
  Mean return: -3.1150
  Mean final entropy: 0.0017
  Max final entropy: 0.0048
  Pseudo loss: -0.26112
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.6979
  Mean return: -3.2541
  Mean final entropy: 0.0020
  Max final entropy: 0.0042
  Pseudo loss: -0.18933
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.288 seconds.
  Mean final reward: -0.5505
  Mean return: -3.0871
  Mean final entropy: 0.0104
  Max final entropy: 0.0818
  Pseudo loss: -8.30905
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5960
  Mean final entropy: 0.0002
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5940286 0.5940286 0.5940286 0.5940286 0.5940286 0.5940286 0.5940286
 0.5940286]
  Mean final entropy: 0.000170
  Max final entropy: 0.000170
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.8036457e-06 1.8036457e-06 1.8036457e-06 1.8036457e-06 1.8036457e-06
 1.8036457e-06 1.8036457e-06 1.8036457e-06]
  Max Min Entropy: 1.8036456594927586e-06
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [8, 8, 8, 8, 8, 8, 8, 8], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 75
seed: 324477
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.277 seconds.
  Mean final reward: -0.9625
  Mean return: -5.4521
  Mean final entropy: 0.0183
  Max final entropy: 0.1273
  Pseudo loss: -1.30909
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.280 seconds.
  Mean final reward: -0.5909
  Mean return: -3.8825
  Mean final entropy: 0.0035
  Max final entropy: 0.0193
  Pseudo loss: 0.56840
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (15/100) took 0.267 seconds.
  Mean final reward: -0.0863
  Mean return: -2.8109
  Mean final entropy: 0.0005
  Max final entropy: 0.0020
  Pseudo loss: -3.91898
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.265 seconds.
  Mean final reward: -0.1726
  Mean return: -1.1622
  Mean final entropy: 0.0009
  Max final entropy: 0.0020
  Pseudo loss: -0.38540
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (25/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7768
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.01620
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (30/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7768
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.01777
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (35/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7827
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.01200
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (40/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7906
  Mean final entropy: 0.0005
  Max final entropy: 0.0007
  Pseudo loss: -0.01557
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (45/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7768
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.02315
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (50/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7886
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7827
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.01907
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (60/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7886
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7827
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.01473
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (70/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7886
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7886
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.263 seconds.
  Mean final reward: -0.0863
  Mean return: -0.9529
  Mean final entropy: 0.0007
  Max final entropy: 0.0020
  Pseudo loss: -0.71320
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (85/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7827
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.01553
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (90/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7886
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7886
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -0.7827
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.01596
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Testing with greedy policy:
  Start entropy: [0.5089732 0.5089732 0.5089732 0.5089732 0.5089732 0.5089732 0.5089732
 0.5089732]
  Mean final entropy: 0.000559
  Max final entropy: 0.000559
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.0572947e-08 5.0572947e-08 5.0572947e-08 5.0572947e-08 5.0572947e-08
 5.0572947e-08 5.0572947e-08 5.0572947e-08]
  Max Min Entropy: 5.057294671928503e-08
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [7, 7, 7, 7, 7, 7, 7, 7], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 76
seed: 338595
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.5267
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.78696
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0527
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: -2.33779
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.06765072 0.06765072 0.06765072 0.06765072 0.06765072 0.06765072
 0.06765072 0.06765072]
  Mean final entropy: 0.000635
  Max final entropy: 0.000635
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.212232e-07 1.212232e-07 1.212232e-07 1.212232e-07 1.212232e-07
 1.212232e-07 1.212232e-07 1.212232e-07]
  Max Min Entropy: 1.2122319503760082e-07
  Best actions: [[5, 5, 5, 5, 5, 5, 5, 5], [2, 2, 2, 2, 2, 2, 2, 2], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 77
seed: 353129
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.258 seconds.
  Mean final reward: -0.3556
  Mean return: -2.3173
  Mean final entropy: 0.0022
  Max final entropy: 0.0150
  Pseudo loss: -1.54876
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.250
Episode (10/100) took 0.262 seconds.
  Mean final reward: -0.0569
  Mean return: -2.0379
  Mean final entropy: 0.0005
  Max final entropy: 0.0012
  Pseudo loss: -4.54445
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.256 seconds.
  Mean final reward: -0.0629
  Mean return: -0.2969
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: 0.03447
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.252 seconds.
  Mean final reward: -0.0382
  Mean return: -0.2392
  Mean final entropy: 0.0004
  Max final entropy: 0.0011
  Pseudo loss: 0.09284
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.258 seconds.
  Mean final reward: -0.0544
  Mean return: -0.2726
  Mean final entropy: 0.0006
  Max final entropy: 0.0011
  Pseudo loss: 0.06599
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (30/100) took 0.255 seconds.
  Mean final reward: -0.0544
  Mean return: -0.2888
  Mean final entropy: 0.0006
  Max final entropy: 0.0011
  Pseudo loss: 0.09785
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (35/100) took 0.257 seconds.
  Mean final reward: -0.0867
  Mean return: -0.3218
  Mean final entropy: 0.0008
  Max final entropy: 0.0011
  Pseudo loss: 0.05016
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.250
Episode (40/100) took 0.255 seconds.
  Mean final reward: -0.0705
  Mean return: -0.3053
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: 0.05127
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (45/100) took 0.252 seconds.
  Mean final reward: -0.0462
  Mean return: -0.2472
  Mean final entropy: 0.0005
  Max final entropy: 0.0011
  Pseudo loss: 0.00668
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (50/100) took 0.253 seconds.
  Mean final reward: -0.0603
  Mean return: -0.2736
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: -0.01244
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (55/100) took 0.256 seconds.
  Mean final reward: -0.0323
  Mean return: -0.2217
  Mean final entropy: 0.0003
  Max final entropy: 0.0011
  Pseudo loss: 0.02129
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (60/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1553
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.00592
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.255 seconds.
  Mean final reward: -0.0280
  Mean return: -0.2075
  Mean final entropy: 0.0004
  Max final entropy: 0.0011
  Pseudo loss: -0.01701
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (70/100) took 0.260 seconds.
  Mean final reward: -0.0647
  Mean return: -0.2707
  Mean final entropy: 0.0006
  Max final entropy: 0.0011
  Pseudo loss: -0.06244
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (75/100) took 0.255 seconds.
  Mean final reward: -0.0384
  Mean return: -0.2225
  Mean final entropy: 0.0004
  Max final entropy: 0.0011
  Pseudo loss: 0.00089
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (80/100) took 0.256 seconds.
  Mean final reward: -0.0059
  Mean return: -0.1566
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: -0.02088
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (85/100) took 0.253 seconds.
  Mean final reward: -0.0119
  Mean return: -0.1744
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.02186
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (90/100) took 0.262 seconds.
  Mean final reward: -0.0119
  Mean return: -0.1744
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.04330
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (95/100) took 0.255 seconds.
  Mean final reward: -0.0401
  Mean return: -0.2429
  Mean final entropy: 0.0007
  Max final entropy: 0.0011
  Pseudo loss: -0.08081
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (100/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -0.1387
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.1143382 0.1143382 0.1143382 0.1143382 0.1143382 0.1143382 0.1143382
 0.1143382]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.8824528e-08 3.8824528e-08 3.8824528e-08 3.8824528e-08 3.8824528e-08
 3.8824528e-08 3.8824528e-08 3.8824528e-08]
  Max Min Entropy: 3.8824527592851155e-08
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 78
seed: 368085
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.259 seconds.
  Mean final reward: -1.6041
  Mean return: -9.1515
  Mean final entropy: 0.0213
  Max final entropy: 0.0850
  Pseudo loss: 0.06547
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.257 seconds.
  Mean final reward: -0.8418
  Mean return: -6.6352
  Mean final entropy: 0.0053
  Max final entropy: 0.0225
  Pseudo loss: -4.15001
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (15/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (20/100) took 0.251 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0001
  Max final entropy: 0.0004
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.252 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.258 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -3.2867
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.4670506 0.4670506 0.4670506 0.4670506 0.4670506 0.4670506 0.4670506
 0.4670506]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.2175326e-06 1.2175326e-06 1.2175326e-06 1.2175326e-06 1.2175326e-06
 1.2175326e-06 1.2175326e-06 1.2175326e-06]
  Max Min Entropy: 1.2175325991847785e-06
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 79
seed: 383469
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.244 seconds.
  Mean final reward: -1.0219
  Mean return: -8.9260
  Mean final entropy: 0.0087
  Max final entropy: 0.0499
  Pseudo loss: -0.44494
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (10/100) took 0.278 seconds.
  Mean final reward: -0.6432
  Mean return: -1.9819
  Mean final entropy: 0.0215
  Max final entropy: 0.1717
  Pseudo loss: -7.28192
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.48247147 0.48247147 0.48247147 0.48247147 0.48247147 0.48247147
 0.48247147 0.48247147]
  Mean final entropy: 0.000033
  Max final entropy: 0.000033
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.417793e-07 6.417793e-07 6.417793e-07 6.417793e-07 6.417793e-07
 6.417793e-07 6.417793e-07 6.417793e-07]
  Max Min Entropy: 6.417793088075996e-07
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 80
seed: 399287
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.283 seconds.
  Mean final reward: -2.1749
  Mean return: -8.9598
  Mean final entropy: 0.0127
  Max final entropy: 0.0311
  Pseudo loss: 0.55171
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.279 seconds.
  Mean final reward: -0.9153
  Mean return: -6.4910
  Mean final entropy: 0.0062
  Max final entropy: 0.0273
  Pseudo loss: 0.52512
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.279 seconds.
  Mean final reward: -1.1810
  Mean return: -7.4763
  Mean final entropy: 0.0044
  Max final entropy: 0.0128
  Pseudo loss: -0.74421
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.286 seconds.
  Mean final reward: -0.9197
  Mean return: -5.8148
  Mean final entropy: 0.0037
  Max final entropy: 0.0131
  Pseudo loss: -0.86363
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.286 seconds.
  Mean final reward: -0.7689
  Mean return: -4.6049
  Mean final entropy: 0.0030
  Max final entropy: 0.0128
  Pseudo loss: -2.98865
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.282 seconds.
  Mean final reward: -0.2832
  Mean return: -3.4514
  Mean final entropy: 0.0014
  Max final entropy: 0.0068
  Pseudo loss: -0.80596
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (35/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4929
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.07887
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (40/100) took 0.281 seconds.
  Mean final reward: -0.1873
  Mean return: -2.8560
  Mean final entropy: 0.0008
  Max final entropy: 0.0032
  Pseudo loss: -0.93605
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (45/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4509
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.25515
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (50/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5769
  Mean final entropy: 0.0003
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4929
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.12280
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (60/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5349
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.06440
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (65/100) took 0.283 seconds.
  Mean final reward: -0.1210
  Mean return: -2.6560
  Mean final entropy: 0.0007
  Max final entropy: 0.0014
  Pseudo loss: -0.15981
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (70/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5349
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.03970
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (75/100) took 0.284 seconds.
  Mean final reward: -0.0421
  Mean return: -2.4510
  Mean final entropy: 0.0003
  Max final entropy: 0.0014
  Pseudo loss: -0.07348
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (80/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4509
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.00880
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.625
Episode (85/100) took 0.287 seconds.
  Mean final reward: -0.1133
  Mean return: -2.5990
  Mean final entropy: 0.0004
  Max final entropy: 0.0025
  Pseudo loss: -1.47495
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2829
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.06191
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (95/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2829
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.08818
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (100/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2829
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: -0.10818
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Testing with greedy policy:
  Start entropy: [0.12740684 0.12740684 0.12740684 0.12740684 0.12740684 0.12740684
 0.12740684 0.12740684]
  Mean final entropy: 0.000052
  Max final entropy: 0.000052
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [5.5577484e-06 5.5577484e-06 5.5577484e-06 5.5577484e-06 5.5577484e-06
 5.5577484e-06 5.5577484e-06 5.5577484e-06]
  Max Min Entropy: 5.557748409046326e-06
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 81
seed: 415545
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.278 seconds.
  Mean final reward: -1.4244
  Mean return: -7.3082
  Mean final entropy: 0.0099
  Max final entropy: 0.0409
  Pseudo loss: -4.40366
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.287 seconds.
  Mean final reward: -0.0829
  Mean return: -1.7882
  Mean final entropy: 0.0007
  Max final entropy: 0.0019
  Pseudo loss: -0.37903
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (15/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -1.7072
  Mean final entropy: 0.0006
  Max final entropy: 0.0007
  Pseudo loss: 0.01431
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (20/100) took 0.286 seconds.
  Mean final reward: -0.0389
  Mean return: -1.6825
  Mean final entropy: 0.0007
  Max final entropy: 0.0014
  Pseudo loss: -0.10776
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (25/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.6754
  Mean final entropy: 0.0006
  Max final entropy: 0.0007
  Pseudo loss: -0.04919
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (30/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5800
  Mean final entropy: 0.0006
  Max final entropy: 0.0007
  Pseudo loss: -0.04111
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5800
  Mean final entropy: 0.0006
  Max final entropy: 0.0007
  Pseudo loss: -0.11811
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.284 seconds.
  Mean final reward: -0.0176
  Mean return: -1.5929
  Mean final entropy: 0.0006
  Max final entropy: 0.0012
  Pseudo loss: -0.31030
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (90/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.5482
  Mean final entropy: 0.0006
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.48751473 0.48751473 0.48751473 0.48751473 0.48751473 0.48751473
 0.48751473 0.48751473]
  Mean final entropy: 0.000573
  Max final entropy: 0.000573
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.4903242e-06 1.4903242e-06 1.4903242e-06 1.4903242e-06 1.4903242e-06
 1.4903242e-06 1.4903242e-06 1.4903242e-06]
  Max Min Entropy: 1.4903241662977962e-06
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 82
seed: 432249
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.269 seconds.
  Mean final reward: -1.5338
  Mean return: -8.7749
  Mean final entropy: 0.0293
  Max final entropy: 0.1712
  Pseudo loss: -1.50432
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.287 seconds.
  Mean final reward: -0.2191
  Mean return: -3.7976
  Mean final entropy: 0.0010
  Max final entropy: 0.0058
  Pseudo loss: -1.09214
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.289 seconds.
  Mean final reward: -0.2730
  Mean return: -1.6549
  Mean final entropy: 0.0014
  Max final entropy: 0.0089
  Pseudo loss: -8.01709
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0004
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.444368 0.444368 0.444368 0.444368 0.444368 0.444368 0.444368 0.444368]
  Mean final entropy: 0.000498
  Max final entropy: 0.000498
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.3338262e-06 1.3338262e-06 1.3338262e-06 1.3338262e-06 1.3338262e-06
 1.3338262e-06 1.3338262e-06 1.3338262e-06]
  Max Min Entropy: 1.3338261624085135e-06
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [5, 5, 5, 5, 5, 5, 5, 5], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 83
seed: 449405
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.265 seconds.
  Mean final reward: -0.3907
  Mean return: -5.1291
  Mean final entropy: 0.0015
  Max final entropy: 0.0053
  Pseudo loss: -0.02189
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.266 seconds.
  Mean final reward: -0.8700
  Mean return: -4.6522
  Mean final entropy: 0.0032
  Max final entropy: 0.0086
  Pseudo loss: -0.00120
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.260 seconds.
  Mean final reward: -1.2694
  Mean return: -4.6386
  Mean final entropy: 0.0108
  Max final entropy: 0.0488
  Pseudo loss: -5.16432
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.259 seconds.
  Mean final reward: -0.1715
  Mean return: -1.5702
  Mean final entropy: 0.0008
  Max final entropy: 0.0024
  Pseudo loss: 0.17112
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.255 seconds.
  Mean final reward: -0.1098
  Mean return: -1.6594
  Mean final entropy: 0.0009
  Max final entropy: 0.0024
  Pseudo loss: -0.14010
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.625
Episode (30/100) took 0.260 seconds.
  Mean final reward: -0.1150
  Mean return: -1.4202
  Mean final entropy: 0.0007
  Max final entropy: 0.0025
  Pseudo loss: -0.02142
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (35/100) took 0.253 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4183
  Mean final entropy: 0.0007
  Max final entropy: 0.0009
  Pseudo loss: 0.00555
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (40/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4027
  Mean final entropy: 0.0008
  Max final entropy: 0.0008
  Pseudo loss: 0.07632
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.256 seconds.
  Mean final reward: -0.1234
  Mean return: -1.5928
  Mean final entropy: 0.0008
  Max final entropy: 0.0016
  Pseudo loss: -0.28891
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (50/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4839
  Mean final entropy: 0.0008
  Max final entropy: 0.0010
  Pseudo loss: -0.19750
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.875
Episode (55/100) took 0.261 seconds.
  Mean final reward: -0.0617
  Mean return: -1.2632
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.06757
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.261 seconds.
  Mean final reward: -0.0617
  Mean return: -1.3720
  Mean final entropy: 0.0006
  Max final entropy: 0.0016
  Pseudo loss: -0.10481
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (65/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3256
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.03537
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (70/100) took 0.258 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2939
  Mean final entropy: 0.0006
  Max final entropy: 0.0008
  Pseudo loss: 0.03972
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.750
Episode (75/100) took 0.267 seconds.
  Mean final reward: -0.0617
  Mean return: -1.3720
  Mean final entropy: 0.0006
  Max final entropy: 0.0016
  Pseudo loss: -0.26248
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2168
  Mean final entropy: 0.0004
  Max final entropy: 0.0008
  Pseudo loss: -0.03997
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (85/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.05125
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0537
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: -0.07309
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (95/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: -0.19065
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9993
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.16001761 0.16001761 0.16001761 0.16001761 0.16001761 0.16001761
 0.16001761 0.16001761]
  Mean final entropy: 0.000001
  Max final entropy: 0.000001
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.6444389e-07 1.6444389e-07 1.6444389e-07 1.6444389e-07 1.6444389e-07
 1.6444389e-07 1.6444389e-07 1.6444389e-07]
  Max Min Entropy: 1.6444388961645018e-07
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [3, 3, 3, 3, 3, 3, 3, 3], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 84
seed: 467019
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.278 seconds.
  Mean final reward: -2.2558
  Mean return: -9.2270
  Mean final entropy: 0.0273
  Max final entropy: 0.0726
  Pseudo loss: -0.16794
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.282 seconds.
  Mean final reward: -1.2177
  Mean return: -7.0476
  Mean final entropy: 0.0058
  Max final entropy: 0.0210
  Pseudo loss: 0.18685
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.272 seconds.
  Mean final reward: -0.5516
  Mean return: -5.7131
  Mean final entropy: 0.0030
  Max final entropy: 0.0188
  Pseudo loss: -0.89223
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.284 seconds.
  Mean final reward: -0.7451
  Mean return: -4.7797
  Mean final entropy: 0.0030
  Max final entropy: 0.0095
  Pseudo loss: -2.03941
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6495
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: -0.19563
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (30/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.5001
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.51628965 0.51628965 0.51628965 0.51628965 0.51628965 0.51628965
 0.51628965 0.51628965]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.218775e-07 1.218775e-07 1.218775e-07 1.218775e-07 1.218775e-07
 1.218775e-07 1.218775e-07 1.218775e-07]
  Max Min Entropy: 1.2187750542125286e-07
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [4, 4, 4, 4, 4, 4, 4, 4], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 85
seed: 485097
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.279 seconds.
  Mean final reward: -2.1509
  Mean return: -10.2317
  Mean final entropy: 0.0437
  Max final entropy: 0.2226
  Pseudo loss: -1.39693
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (10/100) took 0.276 seconds.
  Mean final reward: -1.8765
  Mean return: -9.7210
  Mean final entropy: 0.0177
  Max final entropy: 0.0713
  Pseudo loss: -1.22907
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.375
Episode (15/100) took 0.281 seconds.
  Mean final reward: -2.0004
  Mean return: -7.1851
  Mean final entropy: 0.0114
  Max final entropy: 0.0415
  Pseudo loss: -0.80448
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.287 seconds.
  Mean final reward: -1.3273
  Mean return: -5.8504
  Mean final entropy: 0.0058
  Max final entropy: 0.0113
  Pseudo loss: -0.03575
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (25/100) took 0.287 seconds.
  Mean final reward: -0.2716
  Mean return: -2.9920
  Mean final entropy: 0.0011
  Max final entropy: 0.0088
  Pseudo loss: -1.06843
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (30/100) took 0.294 seconds.
  Mean final reward: -0.1520
  Mean return: -2.8762
  Mean final entropy: 0.0004
  Max final entropy: 0.0034
  Pseudo loss: -1.85776
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0001
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.352 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.316 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.327 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -2.4346
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.49085796 0.49085796 0.49085796 0.49085796 0.49085796 0.49085796
 0.49085796 0.49085796]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.857316e-07 9.857316e-07 9.857316e-07 9.857316e-07 9.857316e-07
 9.857316e-07 9.857316e-07 9.857316e-07]
  Max Min Entropy: 9.857316172201536e-07
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [7, 7, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 86
seed: 503645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.264 seconds.
  Mean final reward: -1.7772
  Mean return: -8.3989
  Mean final entropy: 0.0186
  Max final entropy: 0.0777
  Pseudo loss: -0.40458
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (10/100) took 0.287 seconds.
  Mean final reward: -0.1383
  Mean return: -1.0322
  Mean final entropy: 0.0005
  Max final entropy: 0.0030
  Pseudo loss: -1.95988
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.000
Episode (15/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.294 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.295 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.292 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.21386322 0.21386322 0.21386322 0.21386322 0.21386322 0.21386322
 0.21386322 0.21386322]
  Mean final entropy: 0.000003
  Max final entropy: 0.000003
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.6180467e-07 2.6180467e-07 2.6180467e-07 2.6180467e-07 2.6180467e-07
 2.6180467e-07 2.6180467e-07 2.6180467e-07]
  Max Min Entropy: 2.618046721636347e-07
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 0, 0, 0, 0]]

####################################


step: 87
seed: 522669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.268 seconds.
  Mean final reward: -1.5752
  Mean return: -9.6671
  Mean final entropy: 0.0089
  Max final entropy: 0.0324
  Pseudo loss: -0.42582
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.293 seconds.
  Mean final reward: -1.2058
  Mean return: -5.6357
  Mean final entropy: 0.0093
  Max final entropy: 0.0461
  Pseudo loss: -3.86738
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.295 seconds.
  Mean final reward: -0.0784
  Mean return: -1.3448
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: -0.03598
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (20/100) took 0.296 seconds.
  Mean final reward: -0.0784
  Mean return: -1.1332
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: -0.00244
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (25/100) took 0.295 seconds.
  Mean final reward: -0.1569
  Mean return: -1.3101
  Mean final entropy: 0.0005
  Max final entropy: 0.0019
  Pseudo loss: -0.01210
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0657
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.17618
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.293 seconds.
  Mean final reward: -0.0784
  Mean return: -1.1332
  Mean final entropy: 0.0003
  Max final entropy: 0.0019
  Pseudo loss: -0.23007
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (40/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.296 seconds.
  Mean final reward: -0.1060
  Mean return: -1.1754
  Mean final entropy: 0.0003
  Max final entropy: 0.0023
  Pseudo loss: -0.65742
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (50/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.293 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.301 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.296 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.291 seconds.
  Mean final reward: -0.1060
  Mean return: -1.1754
  Mean final entropy: 0.0003
  Max final entropy: 0.0023
  Pseudo loss: -1.11741
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (95/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -0.9563
  Mean final entropy: 0.0000
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.35098872 0.35098872 0.35098872 0.35098872 0.35098872 0.35098872
 0.35098872 0.35098872]
  Mean final entropy: 0.000021
  Max final entropy: 0.000021
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.8170954e-07 1.8170954e-07 1.8170954e-07 1.8170954e-07 1.8170954e-07
 1.8170954e-07 1.8170954e-07 1.8170954e-07]
  Max Min Entropy: 1.8170953808294144e-07
  Best actions: [[7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 88
seed: 542175
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.266 seconds.
  Mean final reward: -2.0944
  Mean return: -10.9608
  Mean final entropy: 0.0330
  Max final entropy: 0.1120
  Pseudo loss: -0.63740
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.281 seconds.
  Mean final reward: -2.6246
  Mean return: -10.5048
  Mean final entropy: 0.0364
  Max final entropy: 0.1281
  Pseudo loss: -1.43525
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.265 seconds.
  Mean final reward: -2.4074
  Mean return: -11.5966
  Mean final entropy: 0.0365
  Max final entropy: 0.1283
  Pseudo loss: -1.65350
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (20/100) took 0.286 seconds.
  Mean final reward: -1.8011
  Mean return: -9.2041
  Mean final entropy: 0.0240
  Max final entropy: 0.0892
  Pseudo loss: -4.01798
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (25/100) took 0.288 seconds.
  Mean final reward: -0.7151
  Mean return: -7.2564
  Mean final entropy: 0.0028
  Max final entropy: 0.0129
  Pseudo loss: -2.32052
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (30/100) took 0.305 seconds.
  Mean final reward: -0.0983
  Mean return: -6.2447
  Mean final entropy: 0.0003
  Max final entropy: 0.0022
  Pseudo loss: -2.85985
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (35/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.298 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.300 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.304 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.304 seconds.
  Mean final reward: -0.0983
  Mean return: -5.7741
  Mean final entropy: 0.0003
  Max final entropy: 0.0022
  Pseudo loss: -0.64582
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (90/100) took 0.305 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.297 seconds.
  Mean final reward: -0.2448
  Mean return: -6.0750
  Mean final entropy: 0.0009
  Max final entropy: 0.0071
  Pseudo loss: -3.27173
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (100/100) took 0.303 seconds.
  Mean final reward: 0.0000
  Mean return: -5.6758
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.35866925 0.35866925 0.35866925 0.35866925 0.35866925 0.35866925
 0.35866925 0.35866925]
  Mean final entropy: 0.000035
  Max final entropy: 0.000035
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.2036445e-05 4.2036445e-05 4.2036445e-05 4.2036445e-05 4.2036445e-05
 4.2036445e-05 4.2036445e-05 4.2036445e-05]
  Max Min Entropy: 4.203644493827596e-05
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [3, 3, 3, 3, 3, 3, 3, 3]]

####################################


step: 89
seed: 562169
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.269 seconds.
  Mean final reward: -0.8818
  Mean return: -7.5815
  Mean final entropy: 0.0192
  Max final entropy: 0.1455
  Pseudo loss: -0.18595
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (10/100) took 0.272 seconds.
  Mean final reward: -1.4629
  Mean return: -6.7643
  Mean final entropy: 0.0261
  Max final entropy: 0.1439
  Pseudo loss: -4.88640
  Solved trajectories: 2 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.277 seconds.
  Mean final reward: -0.5932
  Mean return: -4.1092
  Mean final entropy: 0.0023
  Max final entropy: 0.0093
  Pseudo loss: -1.01783
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0817
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.12087
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (25/100) took 0.277 seconds.
  Mean final reward: -0.0794
  Mean return: -3.2096
  Mean final entropy: 0.0007
  Max final entropy: 0.0019
  Pseudo loss: -0.23622
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (30/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (35/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0817
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.24521
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.279 seconds.
  Mean final reward: -0.0842
  Mean return: -3.2144
  Mean final entropy: 0.0007
  Max final entropy: 0.0020
  Pseudo loss: -0.45694
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (60/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.279 seconds.
  Mean final reward: -0.0842
  Mean return: -3.2144
  Mean final entropy: 0.0006
  Max final entropy: 0.0020
  Pseudo loss: -0.42979
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.750
Episode (95/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -3.0817
  Mean final entropy: 0.0004
  Max final entropy: 0.0005
  Pseudo loss: 0.26246
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -3.1302
  Mean final entropy: 0.0005
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.38490862 0.38490862 0.38490862 0.38490862 0.38490862 0.38490862
 0.38490862 0.38490862]
  Mean final entropy: 0.000480
  Max final entropy: 0.000480
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [0.00014093 0.00014093 0.00014093 0.00014093 0.00014093 0.00014093
 0.00014093 0.00014093]
  Max Min Entropy: 0.00014092730998527259
  Best actions: [[4, 4, 4, 4, 4, 4, 4, 4], [7, 7, 7, 7, 7, 7, 7, 7], [4, 4, 4, 4, 4, 4, 4, 4]]

####################################


step: 90
seed: 582657
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.276 seconds.
  Mean final reward: -1.4739
  Mean return: -8.8748
  Mean final entropy: 0.0239
  Max final entropy: 0.0903
  Pseudo loss: 0.70891
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (10/100) took 0.269 seconds.
  Mean final reward: -0.9699
  Mean return: -5.3748
  Mean final entropy: 0.0159
  Max final entropy: 0.1058
  Pseudo loss: -6.59461
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.125
Episode (15/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (20/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (25/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (30/100) took 0.281 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (35/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (40/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (45/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (55/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (60/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (65/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0003
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (70/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (75/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (80/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (85/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (90/100) took 0.275 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0002
  Max final entropy: 0.0008
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (95/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Episode (100/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: 0.0000
  Mean final entropy: 0.0001
  Max final entropy: 0.0002
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 1.000
Testing with greedy policy:
  Start entropy: [0.22829494 0.22829494 0.22829494 0.22829494 0.22829494 0.22829494
 0.22829494 0.22829494]
  Mean final entropy: 0.000039
  Max final entropy: 0.000039
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.9776033e-06 2.9776033e-06 2.9776033e-06 2.9776033e-06 2.9776033e-06
 2.9776033e-06 2.9776033e-06 2.9776033e-06]
  Max Min Entropy: 2.9776033443340566e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [8, 8, 8, 8, 8, 8, 8, 8], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 91
seed: 603645
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.280 seconds.
  Mean final reward: -3.7108
  Mean return: -13.9348
  Mean final entropy: 0.0811
  Max final entropy: 0.2865
  Pseudo loss: -0.00194
  Solved trajectories: 0 / 8
  Avg steps to disentangle: 1.000
Episode (10/100) took 0.271 seconds.
  Mean final reward: -2.8600
  Mean return: -11.4215
  Mean final entropy: 0.0383
  Max final entropy: 0.1420
  Pseudo loss: 0.24750
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.250
Episode (15/100) took 0.281 seconds.
  Mean final reward: -2.7368
  Mean return: -10.9651
  Mean final entropy: 0.0223
  Max final entropy: 0.0509
  Pseudo loss: -0.01779
  Solved trajectories: 1 / 8
  Avg steps to disentangle: 1.125
Episode (20/100) took 0.264 seconds.
  Mean final reward: -0.8445
  Mean return: -7.0733
  Mean final entropy: 0.0029
  Max final entropy: 0.0071
  Pseudo loss: -0.09645
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.264 seconds.
  Mean final reward: -0.4032
  Mean return: -6.3788
  Mean final entropy: 0.0012
  Max final entropy: 0.0028
  Pseudo loss: -0.70487
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.261 seconds.
  Mean final reward: -0.1592
  Mean return: -5.9896
  Mean final entropy: 0.0005
  Max final entropy: 0.0028
  Pseudo loss: -0.42074
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.500
Episode (35/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (40/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (45/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (50/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (55/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (60/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (65/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (70/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (75/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (80/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8304
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.09272
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (85/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (90/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (95/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Episode (100/100) took 0.259 seconds.
  Mean final reward: 0.0000
  Mean return: -5.8002
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 3.000
Testing with greedy policy:
  Start entropy: [0.5037743 0.5037743 0.5037743 0.5037743 0.5037743 0.5037743 0.5037743
 0.5037743]
  Mean final entropy: 0.000048
  Max final entropy: 0.000048
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.1989488e-06 2.1989488e-06 2.1989488e-06 2.1989488e-06 2.1989488e-06
 2.1989488e-06 2.1989488e-06 2.1989488e-06]
  Max Min Entropy: 2.1989487777318573e-06
  Best actions: [[8, 8, 8, 8, 8, 8, 8, 8], [6, 6, 6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8, 8, 8]]

####################################


step: 92
seed: 625139
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.265 seconds.
  Mean final reward: -1.5709
  Mean return: -7.3325
  Mean final entropy: 0.0180
  Max final entropy: 0.0595
  Pseudo loss: -0.60763
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (10/100) took 0.247 seconds.
  Mean final reward: -0.7982
  Mean return: -4.3964
  Mean final entropy: 0.0073
  Max final entropy: 0.0419
  Pseudo loss: -3.40082
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.250
Episode (15/100) took 0.246 seconds.
  Mean final reward: -0.3628
  Mean return: -2.4485
  Mean final entropy: 0.0011
  Max final entropy: 0.0029
  Pseudo loss: 0.31978
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.249 seconds.
  Mean final reward: -0.1282
  Mean return: -1.8007
  Mean final entropy: 0.0004
  Max final entropy: 0.0028
  Pseudo loss: -0.35428
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.248 seconds.
  Mean final reward: -0.1282
  Mean return: -1.7247
  Mean final entropy: 0.0005
  Max final entropy: 0.0028
  Pseudo loss: -0.27433
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.247 seconds.
  Mean final reward: -0.1282
  Mean return: -1.7192
  Mean final entropy: 0.0004
  Max final entropy: 0.0028
  Pseudo loss: -1.10653
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (35/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4402
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -0.21551
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.245 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.248 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.247 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.246 seconds.
  Mean final reward: 0.0000
  Mean return: -1.3717
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.20689148 0.20689148 0.20689148 0.20689148 0.20689148 0.20689148
 0.20689148 0.20689148]
  Mean final entropy: 0.000004
  Max final entropy: 0.000004
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.5141441e-06 1.5141441e-06 1.5141441e-06 1.5141441e-06 1.5141441e-06
 1.5141441e-06 1.5141441e-06 1.5141441e-06]
  Max Min Entropy: 1.5141440599109046e-06
  Best actions: [[0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 93
seed: 647145
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.279 seconds.
  Mean final reward: -1.2842
  Mean return: -9.4269
  Mean final entropy: 0.0173
  Max final entropy: 0.1157
  Pseudo loss: -0.97407
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.279 seconds.
  Mean final reward: -0.9865
  Mean return: -7.1557
  Mean final entropy: 0.0052
  Max final entropy: 0.0158
  Pseudo loss: -2.03028
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.291 seconds.
  Mean final reward: -0.6247
  Mean return: -5.1343
  Mean final entropy: 0.0187
  Max final entropy: 0.1481
  Pseudo loss: -5.21968
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8215
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.03269
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.302 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8215
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.04156
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (90/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -2.8135
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.64526355 0.64526355 0.64526355 0.64526355 0.64526355 0.64526355
 0.64526355 0.64526355]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [3.97331e-07 3.97331e-07 3.97331e-07 3.97331e-07 3.97331e-07 3.97331e-07
 3.97331e-07 3.97331e-07]
  Max Min Entropy: 3.97331007206958e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 94
seed: 669669
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.268 seconds.
  Mean final reward: -0.5327
  Mean return: -3.8155
  Mean final entropy: 0.0024
  Max final entropy: 0.0118
  Pseudo loss: -0.82088
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0359
  Mean final entropy: 0.0002
  Max final entropy: 0.0006
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.264 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0474
  Mean final entropy: 0.0005
  Max final entropy: 0.0010
  Pseudo loss: -0.00694
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (20/100) took 0.265 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0374
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.00082
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (25/100) took 0.264 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0482
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: -0.01125
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (30/100) took 0.263 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0374
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.00034
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (35/100) took 0.261 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0459
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.01017
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0389
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: 0.00154
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.500
Episode (45/100) took 0.269 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0482
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: -0.01026
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.375
Episode (50/100) took 0.264 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0474
  Mean final entropy: 0.0005
  Max final entropy: 0.0010
  Pseudo loss: -0.00988
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (55/100) took 0.264 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0467
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: -0.00824
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.125
Episode (60/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0406
  Mean final entropy: 0.0002
  Max final entropy: 0.0005
  Pseudo loss: -0.00322
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (65/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0359
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.262 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0382
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.00033
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (75/100) took 0.261 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0382
  Mean final entropy: 0.0004
  Max final entropy: 0.0010
  Pseudo loss: 0.00063
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (80/100) took 0.260 seconds.
  Mean final reward: 0.0000
  Mean return: -0.0374
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: 0.00024
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.250
Episode (85/100) took 0.261 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0474
  Mean final entropy: 0.0005
  Max final entropy: 0.0010
  Pseudo loss: -0.01005
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (90/100) took 0.265 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0459
  Mean final entropy: 0.0003
  Max final entropy: 0.0010
  Pseudo loss: -0.01129
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.261 seconds.
  Mean final reward: -0.0046
  Mean return: -0.0474
  Mean final entropy: 0.0006
  Max final entropy: 0.0010
  Pseudo loss: -0.01125
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (100/100) took 0.262 seconds.
  Mean final reward: -0.0092
  Mean return: -0.0559
  Mean final entropy: 0.0005
  Max final entropy: 0.0010
  Pseudo loss: -0.01148
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.3166428 0.3166428 0.3166428 0.3166428 0.3166428 0.3166428 0.3166428
 0.3166428]
  Mean final entropy: 0.000966
  Max final entropy: 0.000966
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [7.228584e-07 7.228584e-07 7.228584e-07 7.228584e-07 7.228584e-07
 7.228584e-07 7.228584e-07 7.228584e-07]
  Max Min Entropy: 7.22858374047064e-07
  Best actions: [[2, 2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4, 4], [2, 2, 2, 2, 2, 2, 2, 2]]

####################################


step: 95
seed: 692717
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.282 seconds.
  Mean final reward: -0.3490
  Mean return: -2.3471
  Mean final entropy: 0.0012
  Max final entropy: 0.0068
  Pseudo loss: -2.55018
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.282 seconds.
  Mean final reward: -0.2063
  Mean return: -2.3045
  Mean final entropy: 0.0009
  Max final entropy: 0.0023
  Pseudo loss: -0.70793
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.285 seconds.
  Mean final reward: -0.1081
  Mean return: -2.0757
  Mean final entropy: 0.0006
  Max final entropy: 0.0018
  Pseudo loss: -0.06471
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.375
Episode (20/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.4761
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -0.17740
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.375
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.2310
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: -0.19868
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0004
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0009
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0003
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.291 seconds.
  Mean final reward: 0.0000
  Mean return: -1.1081
  Mean final entropy: 0.0002
  Max final entropy: 0.0003
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.31844008 0.31844008 0.31844008 0.31844008 0.31844008 0.31844008
 0.31844008 0.31844008]
  Mean final entropy: 0.000286
  Max final entropy: 0.000286
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.2289355e-07 9.2289355e-07 9.2289355e-07 9.2289355e-07 9.2289355e-07
 9.2289355e-07 9.2289355e-07 9.2289355e-07]
  Max Min Entropy: 9.228935482497036e-07
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6]]

####################################


step: 96
seed: 716295
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.284 seconds.
  Mean final reward: -0.4324
  Mean return: -5.8366
  Mean final entropy: 0.0017
  Max final entropy: 0.0052
  Pseudo loss: -1.45037
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.276 seconds.
  Mean final reward: -0.7444
  Mean return: -4.6393
  Mean final entropy: 0.0025
  Max final entropy: 0.0084
  Pseudo loss: -1.17361
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.500
Episode (15/100) took 0.280 seconds.
  Mean final reward: -0.3633
  Mean return: -3.0677
  Mean final entropy: 0.0014
  Max final entropy: 0.0084
  Pseudo loss: -0.35042
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2638
  Mean final entropy: 0.0001
  Max final entropy: 0.0007
  Pseudo loss: -0.18539
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (25/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.2638
  Mean final entropy: 0.0002
  Max final entropy: 0.0007
  Pseudo loss: -0.53314
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.278 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.280 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -2.6336
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: -3.00064
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.279 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.277 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.276 seconds.
  Mean final reward: 0.0000
  Mean return: -2.1306
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.5733027 0.5733027 0.5733027 0.5733027 0.5733027 0.5733027 0.5733027
 0.5733027]
  Mean final entropy: 0.000041
  Max final entropy: 0.000041
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [4.6594755e-06 4.6594755e-06 4.6594755e-06 4.6594755e-06 4.6594755e-06
 4.6594755e-06 4.6594755e-06 4.6594755e-06]
  Max Min Entropy: 4.659475507651223e-06
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [7, 7, 7, 7, 7, 7, 7, 7], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 97
seed: 740409
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.274 seconds.
  Mean final reward: -0.6340
  Mean return: -4.9939
  Mean final entropy: 0.0096
  Max final entropy: 0.0729
  Pseudo loss: -1.38905
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.285 seconds.
  Mean final reward: -0.2563
  Mean return: -1.7879
  Mean final entropy: 0.0009
  Max final entropy: 0.0024
  Pseudo loss: -2.92377
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (15/100) took 0.286 seconds.
  Mean final reward: -0.1395
  Mean return: -0.5590
  Mean final entropy: 0.0008
  Max final entropy: 0.0013
  Pseudo loss: 0.13377
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (20/100) took 0.287 seconds.
  Mean final reward: -0.1378
  Mean return: -0.5579
  Mean final entropy: 0.0008
  Max final entropy: 0.0013
  Pseudo loss: 0.04854
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.375
Episode (25/100) took 0.285 seconds.
  Mean final reward: -0.0499
  Mean return: -0.4315
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.07713
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.288 seconds.
  Mean final reward: -0.0539
  Mean return: -0.4413
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: 0.07528
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.288 seconds.
  Mean final reward: -0.0633
  Mean return: -0.4825
  Mean final entropy: 0.0004
  Max final entropy: 0.0013
  Pseudo loss: 0.09676
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.297 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2917
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.02398
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (45/100) took 0.286 seconds.
  Mean final reward: -0.0950
  Mean return: -0.4509
  Mean final entropy: 0.0005
  Max final entropy: 0.0013
  Pseudo loss: 0.04226
  Solved trajectories: 5 / 8
  Avg steps to disentangle: 1.625
Episode (50/100) took 0.285 seconds.
  Mean final reward: -0.0318
  Mean return: -0.3553
  Mean final entropy: 0.0002
  Max final entropy: 0.0013
  Pseudo loss: -0.03629
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.288 seconds.
  Mean final reward: -0.1133
  Mean return: -0.4949
  Mean final entropy: 0.0006
  Max final entropy: 0.0013
  Pseudo loss: -0.12428
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.500
Episode (60/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2599
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2599
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2599
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.287 seconds.
  Mean final reward: -0.0638
  Mean return: -0.4197
  Mean final entropy: 0.0003
  Max final entropy: 0.0013
  Pseudo loss: -0.18971
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.875
Episode (80/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2917
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: -0.02668
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (85/100) took 0.290 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2599
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.282 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2599
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.286 seconds.
  Mean final reward: -0.0182
  Mean return: -0.3042
  Mean final entropy: 0.0001
  Max final entropy: 0.0012
  Pseudo loss: -0.07731
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 1.875
Episode (100/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -0.2599
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.10662384 0.10662384 0.10662384 0.10662384 0.10662384 0.10662384
 0.10662384 0.10662384]
  Mean final entropy: 0.000000
  Max final entropy: 0.000000
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [1.6601743e-06 1.6601743e-06 1.6601743e-06 1.6601743e-06 1.6601743e-06
 1.6601743e-06 1.6601743e-06 1.6601743e-06]
  Max Min Entropy: 1.6601743482169695e-06
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [3, 3, 3, 3, 3, 3, 3, 3], [7, 7, 7, 7, 7, 7, 7, 7]]

####################################


step: 98
seed: 765065
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.287 seconds.
  Mean final reward: -0.5065
  Mean return: -2.7443
  Mean final entropy: 0.0068
  Max final entropy: 0.0508
  Pseudo loss: -3.43813
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.125
Episode (10/100) took 0.286 seconds.
  Mean final reward: -0.0579
  Mean return: -1.2572
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: 0.11282
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.250
Episode (15/100) took 0.286 seconds.
  Mean final reward: -0.0993
  Mean return: -1.2824
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.44760
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (20/100) took 0.291 seconds.
  Mean final reward: -0.1158
  Mean return: -1.3016
  Mean final entropy: 0.0004
  Max final entropy: 0.0016
  Pseudo loss: -0.58551
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 1.750
Episode (25/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.299 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0392
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.15925
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (35/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0392
  Mean final entropy: 0.0001
  Max final entropy: 0.0006
  Pseudo loss: -0.17528
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (40/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.288 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.285 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.284 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.283 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.286 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.287 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.289 seconds.
  Mean final reward: 0.0000
  Mean return: -1.0060
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.42907777 0.42907777 0.42907777 0.42907777 0.42907777 0.42907777
 0.42907777 0.42907777]
  Mean final entropy: 0.000011
  Max final entropy: 0.000011
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [2.3631301e-07 2.3631301e-07 2.3631301e-07 2.3631301e-07 2.3631301e-07
 2.3631301e-07 2.3631301e-07 2.3631301e-07]
  Max Min Entropy: 2.3631301360182988e-07
  Best actions: [[6, 6, 6, 6, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################


step: 99
seed: 790269
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.260 seconds.
  Mean final reward: -1.2552
  Mean return: -6.3582
  Mean final entropy: 0.0367
  Max final entropy: 0.2689
  Pseudo loss: -3.63436
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.875
Episode (10/100) took 0.253 seconds.
  Mean final reward: -0.0917
  Mean return: -1.7916
  Mean final entropy: 0.0005
  Max final entropy: 0.0020
  Pseudo loss: -2.36813
  Solved trajectories: 6 / 8
  Avg steps to disentangle: 2.000
Episode (15/100) took 0.256 seconds.
  Mean final reward: -0.3674
  Mean return: -1.6346
  Mean final entropy: 0.0012
  Max final entropy: 0.0021
  Pseudo loss: -0.39123
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.262 seconds.
  Mean final reward: -0.0871
  Mean return: -1.0536
  Mean final entropy: 0.0004
  Max final entropy: 0.0020
  Pseudo loss: -0.12583
  Solved trajectories: 7 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8725
  Mean final entropy: 0.0001
  Max final entropy: 0.0010
  Pseudo loss: -0.11249
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.125
Episode (30/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.257 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.264 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.254 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.255 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.256 seconds.
  Mean final reward: 0.0000
  Mean return: -0.8155
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.30071625 0.30071625 0.30071625 0.30071625 0.30071625 0.30071625
 0.30071625 0.30071625]
  Mean final entropy: 0.000002
  Max final entropy: 0.000002
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [9.426259e-07 9.426259e-07 9.426259e-07 9.426259e-07 9.426259e-07
 9.426259e-07 9.426259e-07 9.426259e-07]
  Max Min Entropy: 9.426258884559502e-07
  Best actions: [[1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2], [5, 5, 5, 5, 5, 5, 5, 5]]

####################################


step: 100
seed: 816027
##############################
Training parameters:
  Number of trajectories: 8
  Number of episodes: 100
  Learning rate: 0.01
  Policy hidden dimensions: [32]

Using device: cuda

Episode (5/100) took 0.283 seconds.
  Mean final reward: -0.6804
  Mean return: -8.4536
  Mean final entropy: 0.0027
  Max final entropy: 0.0103
  Pseudo loss: 0.08213
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.750
Episode (10/100) took 0.271 seconds.
  Mean final reward: -0.8754
  Mean return: -7.1594
  Mean final entropy: 0.0031
  Max final entropy: 0.0082
  Pseudo loss: -1.64032
  Solved trajectories: 3 / 8
  Avg steps to disentangle: 1.625
Episode (15/100) took 0.268 seconds.
  Mean final reward: -0.8395
  Mean return: -3.9944
  Mean final entropy: 0.0027
  Max final entropy: 0.0066
  Pseudo loss: -0.49747
  Solved trajectories: 4 / 8
  Avg steps to disentangle: 1.625
Episode (20/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (25/100) took 0.269 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (30/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (35/100) took 0.273 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0001
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (40/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (45/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (50/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (55/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (60/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0001
  Max final entropy: 0.0005
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (65/100) took 0.268 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (70/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (75/100) took 0.274 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (80/100) took 0.266 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (85/100) took 0.272 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (90/100) took 0.267 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (95/100) took 0.270 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Episode (100/100) took 0.271 seconds.
  Mean final reward: 0.0000
  Mean return: -1.9793
  Mean final entropy: 0.0000
  Max final entropy: 0.0000
  Pseudo loss: 0.00000
  Solved trajectories: 8 / 8
  Avg steps to disentangle: 2.000
Testing with greedy policy:
  Start entropy: [0.63234496 0.63234496 0.63234496 0.63234496 0.63234496 0.63234496
 0.63234496 0.63234496]
  Mean final entropy: 0.000038
  Max final entropy: 0.000038
  Solved trajectories: 8 / 8
Exaushtive search:
  Minimum entropy: [6.9050833e-07 6.9050833e-07 6.9050833e-07 6.9050833e-07 6.9050833e-07
 6.9050833e-07 6.9050833e-07 6.9050833e-07]
  Max Min Entropy: 6.905083296260273e-07
  Best actions: [[3, 3, 3, 3, 3, 3, 3, 3], [5, 5, 5, 5, 5, 5, 5, 5], [1, 1, 1, 1, 1, 1, 1, 1]]

####################################

Training took 3072.136 seconds
Solved trajectories: 100.0 / 100
Max entropy at final step: 0.00097
